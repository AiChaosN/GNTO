{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645a12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848859e",
   "metadata": {},
   "source": [
    "# 1.数据预处理模块\n",
    "**DataPreprocess.py**   \n",
    "目前的数据主要为json格式.\n",
    "需要进过多部处理才可以作为输入模型的数据.\n",
    "\n",
    "1. 读取json文件\n",
    "2. 转为为NodePlan\n",
    "3. 转为为dataframe (添加两个字段: sql_id, parent_id)\n",
    "4. 统计各个特征的分布等.数据统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2023efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part17.csv', '../data/train_plan_part8.csv', '../data/train_plan_part6.csv', '../data/train_plan_part3.csv', '../data/train_plan_part19.csv', '../data/train_plan_part9.csv', '../data/train_plan_part11.csv', '../data/train_plan_part1.csv', '../data/train_plan_part0.csv', '../data/train_plan_part18.csv', '../data/train_plan_part10.csv', '../data/train_plan_part12.csv', '../data/train_plan_part16.csv', '../data/train_plan_part15.csv', '../data/train_plan_part2.csv', '../data/train_plan_part14.csv', '../data/train_plan_part5.csv', '../data/train_plan_part7.csv', '../data/train_plan_part13.csv', '../data/train_plan_part4.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  85000  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "1  85001  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "2  85002  {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "3  85003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  85004  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Parallel Aware\": false, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 11788.77, \"Total Cost\": 49094.94, \"Plan Rows\": 1028173, \"Plan Width\": 12, \"Actual Startup Time\": 41.924, \"Actual Total Time\": 200.35, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Recheck Cond\": \"(keyword_id > 17243)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 24037, \"Lossy Heap Blocks\": 0, \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 11531.73, \"Plan Rows\": 1028173, \"Plan Width\": 0, \"Actual Startup Time\": 39.572, \"Actual Total Time\": 39.572, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id > 17243)\"}]}, \"Planning Time\": 1.679, \"Triggers\": [], \"Execution Time\": 224.454}\n",
      "plans_dict:\n",
      " [{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 59038.23, 'Total Cost': 292179.55, 'Plan Rows': 958880, 'Plan Width': 136, 'Actual Startup Time': 730.101, 'Actual Total Time': 1214.107, 'Actual Rows': 934003, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 58038.23, 'Total Cost': 195291.55, 'Plan Rows': 399533, 'Plan Width': 136, 'Actual Startup Time': 714.231, 'Actual Total Time': 999.792, 'Actual Rows': 311334, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 96872.38, 'Plan Rows': 1832141, 'Plan Width': 42, 'Actual Startup Time': 0.839, 'Actual Total Time': 252.992, 'Actual Rows': 1441006, 'Actual Loops': 3, 'Index Cond': '(role_id = 10)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 185.267, 'Actual Total Time': 185.268, 'Actual Rows': 159056, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 2016, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 0.229, 'Actual Total Time': 139.643, 'Actual Rows': 159056, 'Actual Loops': 3, 'Filter': '((kind_id < 7) AND (production_year > 1999))', 'Rows Removed by Filter': 683715, 'Workers': []}]}]}]}, {'Node Type': 'Hash Join', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 109926.51, 'Total Cost': 255406.01, 'Plan Rows': 2072097, 'Plan Width': 106, 'Actual Startup Time': 625.826, 'Actual Total Time': 2162.527, 'Actual Rows': 462768, 'Actual Loops': 1, 'Inner Unique': True, 'Hash Cond': '(mk.movie_id = t.id)', 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 0.0, 'Total Cost': 81003.13, 'Plan Rows': 3360520, 'Plan Width': 12, 'Actual Startup Time': 0.021, 'Actual Total Time': 422.848, 'Actual Rows': 3346045, 'Actual Loops': 1, 'Filter': '(keyword_id < 15855)', 'Rows Removed by Filter': 1177885}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Startup Cost': 67602.3, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 622.011, 'Actual Total Time': 622.012, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 64, 'Original Hash Batches': 64, 'Peak Memory Usage': 3011, 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 322.091, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Filter': '(kind_id = 7)', 'Rows Removed by Filter': 985048}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 58428.86, 'Total Cost': 127528.39, 'Plan Rows': 262393, 'Plan Width': 134, 'Actual Startup Time': 352.379, 'Actual Total Time': 552.141, 'Actual Rows': 166070, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 57428.86, 'Total Cost': 100289.09, 'Plan Rows': 109330, 'Plan Width': 134, 'Actual Startup Time': 333.262, 'Actual Total Time': 420.655, 'Actual Rows': 55357, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(mc.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'company_type_id_movie_companies', 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 0.43, 'Total Cost': 29683.22, 'Plan Rows': 555600, 'Plan Width': 40, 'Actual Startup Time': 0.654, 'Actual Total Time': 69.425, 'Actual Rows': 444961, 'Actual Loops': 3, 'Index Cond': '(company_type_id > 1)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 181.576, 'Actual Total Time': 181.576, 'Actual Rows': 126448, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 1728, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 0.234, 'Actual Total Time': 140.725, 'Actual Rows': 126448, 'Actual Loops': 3, 'Filter': '((production_year < 1995) AND (kind_id = 7))', 'Rows Removed by Filter': 716322, 'Workers': []}]}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 53012.0, 'Total Cost': 77258.72, 'Plan Rows': 15739, 'Plan Width': 136, 'Actual Startup Time': 158.907, 'Actual Total Time': 255.435, 'Actual Rows': 7205, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 52012.0, 'Total Cost': 74684.82, 'Plan Rows': 6558, 'Plan Width': 136, 'Actual Startup Time': 143.201, 'Actual Total Time': 233.676, 'Actual Rows': 2402, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 21600.99, 'Plan Rows': 408484, 'Plan Width': 42, 'Actual Startup Time': 1.14, 'Actual Total Time': 46.233, 'Actual Rows': 299463, 'Actual Loops': 3, 'Index Cond': '(role_id = 5)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 141.336, 'Actual Total Time': 141.336, 'Actual Rows': 14129, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 1, 'Original Hash Batches': 1, 'Peak Memory Usage': 5536, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 0.19, 'Actual Total Time': 137.827, 'Actual Rows': 14129, 'Actual Loops': 3, 'Filter': '((kind_id > 4) AND (production_year = 2002))', 'Rows Removed by Filter': 828642, 'Workers': []}]}]}]}]\n",
      "execution_times:\n",
      " [224.454, 1238.107, 2174.023, 556.23, 255.674]\n",
      "{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0}\n",
      "{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}\n",
      "[(0, 0), (1, 1), (0, 1)]\n",
      "[(0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plan_id</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>Node Type</th>\n",
       "      <th>Parallel Aware</th>\n",
       "      <th>Relation Name</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Startup Cost</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Plan Rows</th>\n",
       "      <th>Plan Width</th>\n",
       "      <th>...</th>\n",
       "      <th>Peak Memory Usage</th>\n",
       "      <th>Filter</th>\n",
       "      <th>Rows Removed by Filter</th>\n",
       "      <th>Join Filter</th>\n",
       "      <th>Rows Removed by Join Filter</th>\n",
       "      <th>Merge Cond</th>\n",
       "      <th>Sort Key</th>\n",
       "      <th>Sort Method</th>\n",
       "      <th>Sort Space Used</th>\n",
       "      <th>Sort Space Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitmap Heap Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>mk</td>\n",
       "      <td>11788.77</td>\n",
       "      <td>49094.94</td>\n",
       "      <td>1028173</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitmap Index Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11531.73</td>\n",
       "      <td>1028173</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Gather</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59038.23</td>\n",
       "      <td>292179.55</td>\n",
       "      <td>958880</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hash Join</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58038.23</td>\n",
       "      <td>195291.55</td>\n",
       "      <td>399533</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Index Scan</td>\n",
       "      <td>True</td>\n",
       "      <td>cast_info</td>\n",
       "      <td>ci</td>\n",
       "      <td>0.44</td>\n",
       "      <td>96872.38</td>\n",
       "      <td>1832141</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   plan_id  node_idx          Node Type  Parallel Aware  Relation Name Alias  \\\n",
       "0        0         0   Bitmap Heap Scan           False  movie_keyword    mk   \n",
       "1        0         1  Bitmap Index Scan           False            NaN   NaN   \n",
       "2        1         0             Gather           False            NaN   NaN   \n",
       "3        1         1          Hash Join            True            NaN   NaN   \n",
       "4        1         2         Index Scan            True      cast_info    ci   \n",
       "\n",
       "   Startup Cost  Total Cost  Plan Rows  Plan Width  ...  Peak Memory Usage  \\\n",
       "0      11788.77    49094.94    1028173          12  ...                NaN   \n",
       "1          0.00    11531.73    1028173           0  ...                NaN   \n",
       "2      59038.23   292179.55     958880         136  ...                NaN   \n",
       "3      58038.23   195291.55     399533         136  ...                NaN   \n",
       "4          0.44    96872.38    1832141          42  ...                NaN   \n",
       "\n",
       "   Filter  Rows Removed by Filter  Join Filter Rows Removed by Join Filter  \\\n",
       "0     NaN                     NaN          NaN                         NaN   \n",
       "1     NaN                     NaN          NaN                         NaN   \n",
       "2     NaN                     NaN          NaN                         NaN   \n",
       "3     NaN                     NaN          NaN                         NaN   \n",
       "4     NaN                     NaN          NaN                         NaN   \n",
       "\n",
       "   Merge Cond  Sort Key  Sort Method Sort Space Used Sort Space Type  \n",
       "0         NaN       NaN          NaN             NaN             NaN  \n",
       "1         NaN       NaN          NaN             NaN             NaN  \n",
       "2         NaN       NaN          NaN             NaN             NaN  \n",
       "3         NaN       NaN          NaN             NaN             NaN  \n",
       "4         NaN       NaN          NaN             NaN             NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.数据处理\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "from models.DataPreprocessor import get_plans_dict, DataPreprocessor, plan_trees_to_graphs, graphs_to_df, df_to_graphs, DFStatisticsInfo\n",
    "\n",
    "json_path = \"../data/train_plan_*.csv\"\n",
    "plans_dict, execution_times = get_plans_dict(json_path)\n",
    "print(\"plans_dict:\\n\", plans_dict[0:5])\n",
    "print(\"execution_times:\\n\", execution_times[0:5])\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)\n",
    "\n",
    "edges_list, matrix_plans = plan_trees_to_graphs(plans_tree, add_self_loops=True, undirected=False)\n",
    "print(matrix_plans[0][0])\n",
    "print(matrix_plans[0][1])\n",
    "print(edges_list[0])\n",
    "print(edges_list[99])\n",
    "\n",
    "plans_df = graphs_to_df(matrix_plans)\n",
    "plans_df.to_csv(\"../data/process/01_plans_df.csv\", index=False)\n",
    "plans_df.head()\n",
    "\n",
    "# stats = DFStatisticsInfo(plans_df, sample_threshold=200, sample_k=10, strict_alias_check=True)\n",
    "# node_types = stats.get_node_type_set()\n",
    "# global_must = stats.global_must_keys()\n",
    "# global_all  = stats.global_all_keys()\n",
    "# per_key     = stats.per_key_values()\n",
    "# per_type    = stats.per_nodetype_key_stats()\n",
    "# issues      = stats.report_issues()\n",
    "# stats.pretty_print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6199d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   plan_id  node_idx          Node Type  Parallel Aware  Relation Name Alias  \\\n",
      "0        0         0   Bitmap Heap Scan           False  movie_keyword    mk   \n",
      "1        0         1  Bitmap Index Scan           False            NaN   NaN   \n",
      "2        1         0             Gather           False            NaN   NaN   \n",
      "3        1         1          Hash Join            True            NaN   NaN   \n",
      "4        1         2         Index Scan            True      cast_info    ci   \n",
      "\n",
      "   Startup Cost  Total Cost  Plan Rows  Plan Width  ...  Sort Key  \\\n",
      "0      11788.77    49094.94    1028173          12  ...       NaN   \n",
      "1          0.00    11531.73    1028173           0  ...       NaN   \n",
      "2      59038.23   292179.55     958880         136  ...       NaN   \n",
      "3      58038.23   195291.55     399533         136  ...       NaN   \n",
      "4          0.44    96872.38    1832141          42  ...       NaN   \n",
      "\n",
      "   Sort Method  Sort Space Used  Sort Space Type Filter_Split  \\\n",
      "0          NaN              NaN              NaN           []   \n",
      "1          NaN              NaN              NaN           []   \n",
      "2          NaN              NaN              NaN           []   \n",
      "3          NaN              NaN              NaN           []   \n",
      "4          NaN              NaN              NaN           []   \n",
      "\n",
      "           Index Cond_Split        Recheck Cond_Split  \\\n",
      "0                        []  [[keyword_id, >, 17243]]   \n",
      "1  [[keyword_id, >, 17243]]                        []   \n",
      "2                        []                        []   \n",
      "3                        []                        []   \n",
      "4        [[role_id, =, 10]]                        []   \n",
      "\n",
      "            Hash Cond_Split Join Filter_Split Merge Cond_Split  \n",
      "0                        []                []               []  \n",
      "1                        []                []               []  \n",
      "2                        []                []               []  \n",
      "3  [[ci.movie_id, =, t.id]]                []               []  \n",
      "4                        []                []               []  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                name   min      max  cardinality  num_unique_values  \\\n",
      "0               t.id     1  2528312      2528312            2528312   \n",
      "1          t.kind_id     1        7      2528312                  6   \n",
      "2  t.production_year  1880     2019      2528312                133   \n",
      "3              mc.id     1  2609129      2609129            2609129   \n",
      "4      mc.company_id     1   234997      2609129             234997   \n",
      "\n",
      "  table_name      column_name  \n",
      "0          t               id  \n",
      "1          t          kind_id  \n",
      "2          t  production_year  \n",
      "3         mc               id  \n",
      "4         mc       company_id  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': 24037.0, 'Lossy Heap Blocks': 0.0, 'Parent Relationship': nan, 'Index Name': nan, 'Index Cond': nan, 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Join Type': nan, 'Inner Unique': nan, 'Hash Cond': nan, 'Workers': nan, 'Scan Direction': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [['keyword_id', '>', 17243]], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Index Scan', 'Parallel Aware': False, 'Relation Name': nan, 'Alias': nan, 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Parent Relationship': 'Outer', 'Index Name': 'keyword_id_movie_keyword', 'Index Cond': '(keyword_id > 17243)', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Join Type': nan, 'Inner Unique': nan, 'Hash Cond': nan, 'Workers': nan, 'Scan Direction': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['keyword_id', '>', 17243]], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.DataPreprocessor import safe_cond_parse\n",
    "import pandas as pd\n",
    "\n",
    "NEED_PARSE_COND_SCAN = [\n",
    "    \"Filter\",\n",
    "    \"Index Cond\",\n",
    "    \"Recheck Cond\"\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_JOIN = [\n",
    "    \"Hash Cond\",\n",
    "    \"Join Filter\",\n",
    "    \"Merge Cond\",\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_COLS = NEED_PARSE_COND_SCAN + NEED_PARSE_COND_JOIN\n",
    "\n",
    "for col in NEED_PARSE_COND_COLS:\n",
    "    plans_df[f\"{col}_Split\"] = plans_df[col].apply(safe_cond_parse)\n",
    "\n",
    "\n",
    "print(plans_df.head())\n",
    "print(\"-\"*100)\n",
    "db_info = pd.read_csv(\"../data/column_min_max_vals.csv\")\n",
    "db_info[\"table_name\"], db_info[\"column_name\"] = db_info[\"name\"].str.split(\".\").str[0], db_info[\"name\"].str.split(\".\").str[1]\n",
    "print(db_info.head())\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "graphs = df_to_graphs(plans_df)\n",
    "print(graphs[0])\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52da685",
   "metadata": {},
   "source": [
    "# 2. 数据准备\n",
    "TrainAndEval.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d065e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTrainAndEval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 构建数据集\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m build_dataset(res, edges_list, execution_times, in_dim\u001b[38;5;241m=\u001b[39mf_in, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m数据集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models.TrainAndEval import build_dataset\n",
    "# 构建数据集\n",
    "dataset = build_dataset(res, edges_list, execution_times, in_dim=f_in, bidirectional=True)\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "for i in range(20):\n",
    "    print(dataset[i].x)\n",
    "    print(dataset[i].edge_index)\n",
    "    print(dataset[i].y)\n",
    "    # print(f\"样本: x.shape={dataset[i].x.shape}, edge_index.shape={dataset[i].edge_index.shape}, y={dataset[i].y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af54b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 100000\n",
      "[['Bitmap Heap Scan', False, 'movie_keyword', 'mk', 11788.77, 49094.94, 1028173, 12, 41.924, 200.35, 1029758, 1, '(keyword_id > 17243)', 0.0, 24037.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [['keyword_id', '>', 17243]], [], [], []], ['Bitmap Index Scan', False, nan, nan, 0.0, 11531.73, 1028173, 0, 39.572, 39.572, 1029758, 1, nan, nan, nan, nan, 'Outer', 'keyword_id_movie_keyword', '(keyword_id > 17243)', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [['keyword_id', '>', 17243]], [], [], [], []]]\n",
      "[(0, 0), (1, 1), (0, 1)]\n",
      "224.454\n",
      "[['Gather', False, nan, nan, 59038.23, 292179.55, 958880, 136, 730.101, 1214.107, 934003, 1, nan, nan, nan, nan, nan, nan, nan, 2.0, 2.0, False, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [], [], [], []], ['Hash Join', True, nan, nan, 58038.23, 195291.55, 399533, 136, 714.231, 999.792, 311334, 3, nan, nan, nan, nan, 'Outer', nan, nan, nan, nan, nan, 'Inner', True, '(ci.movie_id = t.id)', [], nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [], [['ci.movie_id', '=', 't.id']], [], []], ['Index Scan', True, 'cast_info', 'ci', 0.44, 96872.38, 1832141, 42, 0.839, 252.992, 1441006, 3, nan, 0.0, nan, nan, 'Outer', 'role_id_cast_info', '(role_id = 10)', nan, nan, nan, nan, nan, nan, [], 'Forward', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [['role_id', '=', 10]], [], [], [], []], ['Hash', True, nan, nan, 51800.15, 51800.15, 229731, 94, 185.267, 185.268, 159056, 3, nan, nan, nan, nan, 'Inner', nan, nan, nan, nan, nan, nan, nan, nan, [], nan, 32768.0, 32768.0, 32.0, 32.0, 2016.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [], [], [], []], ['Seq Scan', True, 'title', 't', 0.0, 51800.15, 229731, 94, 0.229, 139.643, 159056, 3, nan, nan, nan, nan, 'Outer', nan, nan, nan, nan, nan, nan, nan, nan, [], nan, nan, nan, nan, nan, nan, '((kind_id < 7) AND (production_year > 1999))', 683715.0, nan, nan, nan, nan, nan, nan, nan, [['kind_id', '<', 7], ['production_year', '>', 1999]], [], [], [], [], []]]\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (1, 2), (3, 3), (1, 3), (4, 4), (3, 4)]\n",
      "1238.107\n",
      "[['Hash Join', False, nan, nan, 109926.51, 255406.01, 2072097, 106, 625.826, 2162.527, 462768, 1, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 'Inner', True, '(mk.movie_id = t.id)', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [], [['mk.movie_id', '=', 't.id']], [], []], ['Seq Scan', False, 'movie_keyword', 'mk', 0.0, 81003.13, 3360520, 12, 0.021, 422.848, 3346045, 1, nan, nan, nan, nan, 'Outer', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, '(keyword_id < 15855)', 1177885.0, nan, nan, nan, nan, nan, nan, nan, [['keyword_id', '<', 15855]], [], [], [], [], []], ['Hash', False, nan, nan, 67602.3, 67602.3, 1558977, 94, 622.011, 622.012, 1543264, 1, nan, nan, nan, nan, 'Inner', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 32768.0, 32768.0, 64.0, 64.0, 3011.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, [], [], [], [], [], []], ['Seq Scan', False, 'title', 't', 0.0, 67602.3, 1558977, 94, 0.011, 322.091, 1543264, 1, nan, nan, nan, nan, 'Outer', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, '(kind_id = 7)', 985048.0, nan, nan, nan, nan, nan, nan, nan, [['kind_id', '=', 7]], [], [], [], [], []]]\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (0, 2), (3, 3), (2, 3)]\n",
      "2174.023\n",
      "训练集: 70000, 验证集: 15000, 测试集: 15000\n",
      "训练批次数: 2188, 验证批次数: 469\n"
     ]
    }
   ],
   "source": [
    "# 5. 数据准备\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# 构建数据集\n",
    "dataset = build_dataset(res, edges_list, execution_times, in_dim=f_in, bidirectional=True)\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "for i in range(3):\n",
    "    print(dataset[i].x)\n",
    "    print(dataset[i].edge_index)\n",
    "    print(dataset[i].y)\n",
    "    # print(f\"样本: x.shape={dataset[i].x.shape}, edge_index.shape={dataset[i].edge_index.shape}, y={dataset[i].y}\")\n",
    "    \n",
    "# 数据集划分\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    range(len(dataset)), test_size=0.3, random_state=42\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = [dataset[i] for i in train_indices]\n",
    "val_dataset = [dataset[i] for i in val_indices]\n",
    "test_dataset = [dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"训练集: {len(train_dataset)}, 验证集: {len(val_dataset)}, 测试集: {len(test_dataset)}\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}, 验证批次数: {len(val_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf30b9",
   "metadata": {},
   "source": [
    "# 3. 模型模块\n",
    "\n",
    "## NodeEncoder\n",
    "NodeEncoder.py\n",
    "主要包括了各种vectorical的编码方式.\n",
    "\n",
    "1. 转为为Matrix(Node, Edge)\n",
    "\n",
    "\n",
    "## TreeEncoder\n",
    "TreeEncoder.py\n",
    "目前包括两种模型一个是GAT,一个是传统GNN.\n",
    "1. 转为vector\n",
    "\n",
    "## PredictionHead\n",
    "PredictionHead.py\n",
    "目前进行最简单的FNN进行后续回归任务\n",
    "\n",
    "1. 预测\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostModel(\n",
      "  (nodecoder): NodeEncoder_Mini(\n",
      "    (proj): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (treeencoder): TreeEncoder_GATMini(\n",
      "    (gat1): GATConv(32, 64, heads=8)\n",
      "    (gat2): GATConv(512, 64, heads=1)\n",
      "  )\n",
      "  (predict_head): PredictionHead_FNNMini(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from models.NodeEncoder import *\n",
    "from models.TreeEncoder import *\n",
    "from models.PredictionHead import *\n",
    "from models.TrainAndEval import EarlyStopping\n",
    "\n",
    "class CostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        data 结构\n",
    "        - x: [N, F_num]\n",
    "        - edge_index: [2, E]\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        g = self.treeencoder(x, data.edge_index, data.batch)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "f_in, d_node, d_graph, out_dim = 16, 32, 64, 1\n",
    "nodecoder = NodeEncoder_Mini(\n",
    "    in_dim=f_in,\n",
    "    d_node=d_node\n",
    ")\n",
    "gatTreeEncoder = TreeEncoder_GATMini(\n",
    "    in_dim=d_node,\n",
    "    hidden_dim=64,\n",
    "    out_dim=d_graph\n",
    ")\n",
    "predict_head = PredictionHead_FNNMini(\n",
    "    in_dim=d_graph,\n",
    "    out_dim=out_dim\n",
    ")\n",
    "\n",
    "# 模型构建\n",
    "model = CostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "print(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023b51d",
   "metadata": {},
   "source": [
    "# 4 训练&评估\n",
    "## 训练\n",
    "主要模块为划分训练集,测试集,验证集.\n",
    "调用模型进行训练.\n",
    "## 评估\n",
    "主要为评估方式.目前为MSE以及Q-error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3725e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AiChaosN/miniconda3/envs/cs224/lib/python3.12/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'y', 'x'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     70\u001b[0m     model, train_loader, val_loader, optimizer, scheduler, \n\u001b[1;32m     71\u001b[0m     criterion, early_stopping, device, weight_path, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     72\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, criterion, early_stopping, device, weight_path, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[1;32m     23\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n",
      "File \u001b[0;32m~/Project/Phd/project/GNTO/models/TrainAndEval.py:90\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, batch\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m, in \u001b[0;36mCostModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Data \u001b[38;5;241m|\u001b[39m Batch):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    data 结构\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    - x: [N, F_num]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    - edge_index: [2, E]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodecoder(data\u001b[38;5;241m.\u001b[39mx)                                   \u001b[38;5;66;03m# [N, d_node]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtreeencoder(x, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)         \u001b[38;5;66;03m# [B, d_graph]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_head(g)                                     \u001b[38;5;66;03m# [B, out_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Project/Phd/project/GNTO/models/NodeEncoder.py:36\u001b[0m, in \u001b[0;36mNodeEncoder_Mini.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from models.TrainAndEval import train_epoch, validate_epoch\n",
    "import time\n",
    "\n",
    "#  8. 训练循环\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "                early_stopping, device, weight_path, num_epochs=100):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"开始训练...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 计算时间\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # 打印进度\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\n早停触发在第 {epoch+1} 轮\")\n",
    "            break\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            date = datetime.now().strftime(\"%m%d\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, f'../results/weight_{date}.pth')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"训练完成!\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 开始训练\n",
    "date = datetime.now().strftime(\"%m%d\")\n",
    "weight_path = f'../results/{date}.pth'\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, \n",
    "    criterion, early_stopping, device, weight_path, num_epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. 可视化训练过程和结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 预测 vs 真实值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(targets, predictions, alpha=0.5, s=20)\n",
    "    min_val = min(targets.min(), predictions.min())\n",
    "    max_val = max(targets.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.xlabel('True Execution Time')\n",
    "    plt.ylabel('Predicted Execution Time')\n",
    "    plt.title('Predicted vs True Execution Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    date = datetime.now().strftime(\"%m%d\")\n",
    "    plt.savefig(f'../results/training_results_{date}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "# 绘制结果\n",
    "plot_training_history(train_losses, val_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
