{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848859e",
   "metadata": {},
   "source": [
    "# 1.数据预处理模块\n",
    "**DataPreprocess.py**   \n",
    "目前的数据主要为json格式.\n",
    "需要进过多部处理才可以作为输入模型的数据.\n",
    "\n",
    "1. 读取json文件\n",
    "2. 转为为NodePlan\n",
    "3. 转为为dataframe (添加两个字段: sql_id, parent_id)\n",
    "4. 统计各个特征的分布等.数据统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part17.csv', '../data/train_plan_part8.csv', '../data/train_plan_part6.csv', '../data/train_plan_part3.csv', '../data/train_plan_part19.csv', '../data/train_plan_part9.csv', '../data/train_plan_part11.csv', '../data/train_plan_part1.csv', '../data/train_plan_part0.csv', '../data/train_plan_part18.csv', '../data/train_plan_part10.csv', '../data/train_plan_part12.csv', '../data/train_plan_part16.csv', '../data/train_plan_part15.csv', '../data/train_plan_part2.csv', '../data/train_plan_part14.csv', '../data/train_plan_part5.csv', '../data/train_plan_part7.csv', '../data/train_plan_part13.csv', '../data/train_plan_part4.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  85000  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "1  85001  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "2  85002  {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "3  85003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  85004  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Parallel Aware\": false, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 11788.77, \"Total Cost\": 49094.94, \"Plan Rows\": 1028173, \"Plan Width\": 12, \"Actual Startup Time\": 41.924, \"Actual Total Time\": 200.35, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Recheck Cond\": \"(keyword_id > 17243)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 24037, \"Lossy Heap Blocks\": 0, \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 11531.73, \"Plan Rows\": 1028173, \"Plan Width\": 0, \"Actual Startup Time\": 39.572, \"Actual Total Time\": 39.572, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id > 17243)\"}]}, \"Planning Time\": 1.679, \"Triggers\": [], \"Execution Time\": 224.454}\n",
      "plans_dict:\n",
      " [{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 59038.23, 'Total Cost': 292179.55, 'Plan Rows': 958880, 'Plan Width': 136, 'Actual Startup Time': 730.101, 'Actual Total Time': 1214.107, 'Actual Rows': 934003, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 58038.23, 'Total Cost': 195291.55, 'Plan Rows': 399533, 'Plan Width': 136, 'Actual Startup Time': 714.231, 'Actual Total Time': 999.792, 'Actual Rows': 311334, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 96872.38, 'Plan Rows': 1832141, 'Plan Width': 42, 'Actual Startup Time': 0.839, 'Actual Total Time': 252.992, 'Actual Rows': 1441006, 'Actual Loops': 3, 'Index Cond': '(role_id = 10)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 185.267, 'Actual Total Time': 185.268, 'Actual Rows': 159056, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 2016, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 0.229, 'Actual Total Time': 139.643, 'Actual Rows': 159056, 'Actual Loops': 3, 'Filter': '((kind_id < 7) AND (production_year > 1999))', 'Rows Removed by Filter': 683715, 'Workers': []}]}]}]}, {'Node Type': 'Hash Join', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 109926.51, 'Total Cost': 255406.01, 'Plan Rows': 2072097, 'Plan Width': 106, 'Actual Startup Time': 625.826, 'Actual Total Time': 2162.527, 'Actual Rows': 462768, 'Actual Loops': 1, 'Inner Unique': True, 'Hash Cond': '(mk.movie_id = t.id)', 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 0.0, 'Total Cost': 81003.13, 'Plan Rows': 3360520, 'Plan Width': 12, 'Actual Startup Time': 0.021, 'Actual Total Time': 422.848, 'Actual Rows': 3346045, 'Actual Loops': 1, 'Filter': '(keyword_id < 15855)', 'Rows Removed by Filter': 1177885}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Startup Cost': 67602.3, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 622.011, 'Actual Total Time': 622.012, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 64, 'Original Hash Batches': 64, 'Peak Memory Usage': 3011, 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 322.091, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Filter': '(kind_id = 7)', 'Rows Removed by Filter': 985048}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 58428.86, 'Total Cost': 127528.39, 'Plan Rows': 262393, 'Plan Width': 134, 'Actual Startup Time': 352.379, 'Actual Total Time': 552.141, 'Actual Rows': 166070, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 57428.86, 'Total Cost': 100289.09, 'Plan Rows': 109330, 'Plan Width': 134, 'Actual Startup Time': 333.262, 'Actual Total Time': 420.655, 'Actual Rows': 55357, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(mc.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'company_type_id_movie_companies', 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 0.43, 'Total Cost': 29683.22, 'Plan Rows': 555600, 'Plan Width': 40, 'Actual Startup Time': 0.654, 'Actual Total Time': 69.425, 'Actual Rows': 444961, 'Actual Loops': 3, 'Index Cond': '(company_type_id > 1)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 181.576, 'Actual Total Time': 181.576, 'Actual Rows': 126448, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 1728, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 0.234, 'Actual Total Time': 140.725, 'Actual Rows': 126448, 'Actual Loops': 3, 'Filter': '((production_year < 1995) AND (kind_id = 7))', 'Rows Removed by Filter': 716322, 'Workers': []}]}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 53012.0, 'Total Cost': 77258.72, 'Plan Rows': 15739, 'Plan Width': 136, 'Actual Startup Time': 158.907, 'Actual Total Time': 255.435, 'Actual Rows': 7205, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 52012.0, 'Total Cost': 74684.82, 'Plan Rows': 6558, 'Plan Width': 136, 'Actual Startup Time': 143.201, 'Actual Total Time': 233.676, 'Actual Rows': 2402, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 21600.99, 'Plan Rows': 408484, 'Plan Width': 42, 'Actual Startup Time': 1.14, 'Actual Total Time': 46.233, 'Actual Rows': 299463, 'Actual Loops': 3, 'Index Cond': '(role_id = 5)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 141.336, 'Actual Total Time': 141.336, 'Actual Rows': 14129, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 1, 'Original Hash Batches': 1, 'Peak Memory Usage': 5536, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 0.19, 'Actual Total Time': 137.827, 'Actual Rows': 14129, 'Actual Loops': 3, 'Filter': '((kind_id > 4) AND (production_year = 2002))', 'Rows Removed by Filter': 828642, 'Workers': []}]}]}]}]\n",
      "execution_times:\n",
      " [224.454, 1238.107, 2174.023, 556.23, 255.674]\n",
      "{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0}\n",
      "{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}\n",
      "[(0, 0), (1, 1), (0, 1)]\n",
      "[(0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plan_id</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>Node Type</th>\n",
       "      <th>Parallel Aware</th>\n",
       "      <th>Relation Name</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Startup Cost</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Plan Rows</th>\n",
       "      <th>Plan Width</th>\n",
       "      <th>...</th>\n",
       "      <th>Peak Memory Usage</th>\n",
       "      <th>Filter</th>\n",
       "      <th>Rows Removed by Filter</th>\n",
       "      <th>Join Filter</th>\n",
       "      <th>Rows Removed by Join Filter</th>\n",
       "      <th>Merge Cond</th>\n",
       "      <th>Sort Key</th>\n",
       "      <th>Sort Method</th>\n",
       "      <th>Sort Space Used</th>\n",
       "      <th>Sort Space Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitmap Heap Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>mk</td>\n",
       "      <td>11788.77</td>\n",
       "      <td>49094.94</td>\n",
       "      <td>1028173</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitmap Index Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11531.73</td>\n",
       "      <td>1028173</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Gather</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59038.23</td>\n",
       "      <td>292179.55</td>\n",
       "      <td>958880</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hash Join</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58038.23</td>\n",
       "      <td>195291.55</td>\n",
       "      <td>399533</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Index Scan</td>\n",
       "      <td>True</td>\n",
       "      <td>cast_info</td>\n",
       "      <td>ci</td>\n",
       "      <td>0.44</td>\n",
       "      <td>96872.38</td>\n",
       "      <td>1832141</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   plan_id  node_idx          Node Type  Parallel Aware  Relation Name Alias  \\\n",
       "0        0         0   Bitmap Heap Scan           False  movie_keyword    mk   \n",
       "1        0         1  Bitmap Index Scan           False            NaN   NaN   \n",
       "2        1         0             Gather           False            NaN   NaN   \n",
       "3        1         1          Hash Join            True            NaN   NaN   \n",
       "4        1         2         Index Scan            True      cast_info    ci   \n",
       "\n",
       "   Startup Cost  Total Cost  Plan Rows  Plan Width  ...  Peak Memory Usage  \\\n",
       "0      11788.77    49094.94    1028173          12  ...                NaN   \n",
       "1          0.00    11531.73    1028173           0  ...                NaN   \n",
       "2      59038.23   292179.55     958880         136  ...                NaN   \n",
       "3      58038.23   195291.55     399533         136  ...                NaN   \n",
       "4          0.44    96872.38    1832141          42  ...                NaN   \n",
       "\n",
       "   Filter  Rows Removed by Filter  Join Filter Rows Removed by Join Filter  \\\n",
       "0     NaN                     NaN          NaN                         NaN   \n",
       "1     NaN                     NaN          NaN                         NaN   \n",
       "2     NaN                     NaN          NaN                         NaN   \n",
       "3     NaN                     NaN          NaN                         NaN   \n",
       "4     NaN                     NaN          NaN                         NaN   \n",
       "\n",
       "   Merge Cond  Sort Key  Sort Method Sort Space Used Sort Space Type  \n",
       "0         NaN       NaN          NaN             NaN             NaN  \n",
       "1         NaN       NaN          NaN             NaN             NaN  \n",
       "2         NaN       NaN          NaN             NaN             NaN  \n",
       "3         NaN       NaN          NaN             NaN             NaN  \n",
       "4         NaN       NaN          NaN             NaN             NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.数据处理\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "from models.DataPreprocessor import get_plans_dict, DataPreprocessor, plan_trees_to_graphs, graphs_to_df, df_to_graphs, DFStatisticsInfo\n",
    "\n",
    "json_path = \"../data/train_plan_*.csv\"\n",
    "plans_dict, execution_times = get_plans_dict(json_path)\n",
    "print(\"plans_dict:\\n\", plans_dict[0:5])\n",
    "print(\"execution_times:\\n\", execution_times[0:5])\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)\n",
    "\n",
    "edges_list, matrix_plans = plan_trees_to_graphs(plans_tree, add_self_loops=True, undirected=False)\n",
    "print(matrix_plans[0][0])\n",
    "print(matrix_plans[0][1])\n",
    "print(edges_list[0])\n",
    "print(edges_list[99])\n",
    "\n",
    "plans_df = graphs_to_df(matrix_plans)\n",
    "plans_df.to_csv(\"../data/process/01_plans_df.csv\", index=False)\n",
    "plans_df.head()\n",
    "\n",
    "# stats = DFStatisticsInfo(plans_df, sample_threshold=200, sample_k=10, strict_alias_check=True)\n",
    "# node_types = stats.get_node_type_set()\n",
    "# global_must = stats.global_must_keys()\n",
    "# global_all  = stats.global_all_keys()\n",
    "# per_key     = stats.per_key_values()\n",
    "# per_type    = stats.per_nodetype_key_stats()\n",
    "# issues      = stats.report_issues()\n",
    "# stats.pretty_print_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224576b2",
   "metadata": {},
   "source": [
    "# 2. 模型模块\n",
    "\n",
    "## NodeEncoder\n",
    "NodeEncoder.py\n",
    "主要包括了各种vectorical的编码方式.\n",
    "\n",
    "1. 转为为Matrix(Node, Edge)\n",
    "\n",
    "\n",
    "## TreeEncoder\n",
    "TreeEncoder.py\n",
    "目前包括两种模型一个是GAT,一个是传统GNN.\n",
    "1. 转为vector\n",
    "\n",
    "## PredictionHead\n",
    "PredictionHead.py\n",
    "目前进行最简单的FNN进行后续回归任务\n",
    "\n",
    "1. 预测\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03e4312",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TreeEncoder_GATMini.__init__() got an unexpected keyword argument 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m f_num, d_node, d_graph, out_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m nodecoder \u001b[38;5;241m=\u001b[39m NodeEncoder_Mini(\n\u001b[1;32m     36\u001b[0m     in_dim\u001b[38;5;241m=\u001b[39mf_num,\n\u001b[1;32m     37\u001b[0m     d_node\u001b[38;5;241m=\u001b[39md_node\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m gatTreeEncoder \u001b[38;5;241m=\u001b[39m TreeEncoder_GATMini(\n\u001b[1;32m     40\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39md_node,\n\u001b[1;32m     41\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     42\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39md_graph,\n\u001b[1;32m     43\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     44\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     45\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     46\u001b[0m     pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m predict_head \u001b[38;5;241m=\u001b[39m PredictionHead_FNNMini(d_graph, out_dim)\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m PlanCostModel(nodecoder, gatTreeEncoder, predict_head)\n",
      "\u001b[0;31mTypeError\u001b[0m: TreeEncoder_GATMini.__init__() got an unexpected keyword argument 'input_dim'"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class PlanCostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        期望 data 里至少有:\n",
    "        - x: [N, F_num] (numerical features)\n",
    "        - x_cat: [N, F_cat] (categorical features)\n",
    "        - edge_index: [2, E]\n",
    "        - batch: [N]  指示每个节点属于哪张图\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        g = self.treeencoder(x, data.edge_index, data.batch)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "from models.NodeEncoder import *\n",
    "from models.TreeEncoder import *\n",
    "from models.PredictionHead import *\n",
    "\n",
    "f_num, d_node, d_graph, out_dim = 16, 32, 64, 1\n",
    "nodecoder = NodeEncoder_Mini(\n",
    "    in_dim=f_num,\n",
    "    d_node=d_node\n",
    ")\n",
    "gatTreeEncoder = TreeEncoder_GATMini(\n",
    "    input_dim=d_node,\n",
    "    hidden_dim=64,\n",
    "    output_dim=d_graph,\n",
    "    num_layers=3,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    pooling=\"mean\"\n",
    ")\n",
    "predict_head = PredictionHead_FNNMini(d_graph, out_dim)\n",
    "model = PlanCostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "\n",
    "print(type(edges_list))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52da685",
   "metadata": {},
   "source": [
    "# 3. 训练模块\n",
    "TrainAndEval.py\n",
    "\n",
    "## 训练\n",
    "主要模块为划分训练集,测试集,验证集.\n",
    "调用模型进行训练.\n",
    "## 评估\n",
    "主要为评估方式.目前为MSE以及Q-error.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d065e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 早停机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "# 训练&评估函数\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.y)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    q_errors_all = []  # 收集整份验证集的 q-error\n",
    "    eps = 1e-8\n",
    "    \n",
    "    Q50_list = []\n",
    "    Q95_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.y)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            \n",
    "            # 防 0 防负；Q-Error 定义是正数比例\n",
    "            p = torch.clamp(pred, min=eps)\n",
    "            t = torch.clamp(batch.y,    min=eps)\n",
    "            q_error = torch.maximum(p / t, t / p)              # [B]\n",
    "            q_errors_all.append(q_error.cpu().numpy())\n",
    "\n",
    "    if q_errors_all:\n",
    "        q_all = np.concatenate(q_errors_all, axis=0)\n",
    "        Q50 = float(np.quantile(q_all, 0.5))\n",
    "        Q95 = float(np.quantile(q_all, 0.95))\n",
    "    else:\n",
    "        Q50 = float(\"nan\")\n",
    "        Q95 = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, num_batches)\n",
    "    print(f\"val_loss: {avg_loss:.6f} | Q50: {Q50:.6f} | Q95: {Q95:.6f}\")\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds_all, targs_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch).view(-1).float()   # [B]，先拍平\n",
    "            y    = batch.y.view(-1).float()        # [B]\n",
    "            preds_all.append(pred.cpu())\n",
    "            targs_all.append(y.cpu())\n",
    "\n",
    "    preds = torch.cat(preds_all)   # [N]\n",
    "    targs = torch.cat(targs_all)   # [N]\n",
    "\n",
    "    # MSE（Torch实现）\n",
    "    mse = torch.mean((preds - targs) ** 2).item()\n",
    "\n",
    "    # Q-Error（Torch实现）\n",
    "    eps = 1e-8\n",
    "    p = torch.clamp(preds, min=eps)\n",
    "    t = torch.clamp(targs, min=eps)\n",
    "    q = torch.maximum(p / t, t / p)             # [N]\n",
    "    Q50 = torch.quantile(q, 0.5).item()\n",
    "    Q95 = torch.quantile(q, 0.95).item()\n",
    "\n",
    "    # 如果你需要返回 numpy\n",
    "    predictions = preds.numpy()\n",
    "    targets = targs.numpy()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"测试集评估结果:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"MSE:  {mse:.6f}\")\n",
    "    print(f\"Q50: {Q50:.6f}, Q95: {Q95:.6f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return predictions, targets, {'mse': mse, 'Q50': Q50, 'Q95': Q95}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. 可视化训练过程和结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 预测 vs 真实值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(targets, predictions, alpha=0.5, s=20)\n",
    "    min_val = min(targets.min(), predictions.min())\n",
    "    max_val = max(targets.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.xlabel('True Execution Time')\n",
    "    plt.ylabel('Predicted Execution Time')\n",
    "    plt.title('Predicted vs True Execution Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    date = datetime.now().strftime(\"%m%d\")\n",
    "    plt.savefig(f'../results/training_results_{date}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "# 绘制结果\n",
    "plot_training_history(train_losses, val_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
