{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9760e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part17.csv', '../data/train_plan_part8.csv', '../data/train_plan_part6.csv', '../data/train_plan_part3.csv', '../data/train_plan_part19.csv', '../data/train_plan_part9.csv', '../data/train_plan_part11.csv', '../data/train_plan_part1.csv', '../data/train_plan_part0.csv', '../data/train_plan_part18.csv', '../data/train_plan_part10.csv', '../data/train_plan_part12.csv', '../data/train_plan_part16.csv', '../data/train_plan_part15.csv', '../data/train_plan_part2.csv', '../data/train_plan_part14.csv', '../data/train_plan_part5.csv', '../data/train_plan_part7.csv', '../data/train_plan_part13.csv', '../data/train_plan_part4.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  85000  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "1  85001  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "2  85002  {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "3  85003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  85004  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Parallel Aware\": false, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 11788.77, \"Total Cost\": 49094.94, \"Plan Rows\": 1028173, \"Plan Width\": 12, \"Actual Startup Time\": 41.924, \"Actual Total Time\": 200.35, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Recheck Cond\": \"(keyword_id > 17243)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 24037, \"Lossy Heap Blocks\": 0, \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 11531.73, \"Plan Rows\": 1028173, \"Plan Width\": 0, \"Actual Startup Time\": 39.572, \"Actual Total Time\": 39.572, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id > 17243)\"}]}, \"Planning Time\": 1.679, \"Triggers\": [], \"Execution Time\": 224.454}\n",
      "plans_dict:\n",
      " {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}]}\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 匹配所有 train_plan_0*.csv\n",
    "files = glob.glob(\"../data/train_plan_*.csv\")\n",
    "print(\"找到的文件:\", files)\n",
    "\n",
    "# 读入并合并\n",
    "df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "print(\"总数据行数:\", len(df))\n",
    "print(\"df:\\n\", df.head())\n",
    "\n",
    "#获取json字符串\n",
    "plans_json = df['json']\n",
    "print(\"plans_json:\\n\", plans_json.iloc[0])\n",
    "\n",
    "#字符串转json\n",
    "plans_dict = []\n",
    "ExecutionTimes = []\n",
    "idx = 0\n",
    "for json_str in plans_json:\n",
    "    idx += 1\n",
    "    plan_dict = json.loads(json_str)\n",
    "    plans_dict.append(plan_dict['Plan'])\n",
    "    try:\n",
    "        ExecutionTimes.append(plan_dict['Execution Time'])\n",
    "    except:\n",
    "        print(f\"idx: {idx} 不存在Execution Time\")\n",
    "        print(plan_dict)\n",
    "print(\"plans_dict:\\n\", plans_dict[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1841cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "# json -> PlanNode\n",
    "from models.DataPreprocessor import PlanNode, DataPreprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd10e109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PlanNode -> edges_list, extra_info_list\n",
    "def tree_to_graph(root):\n",
    "    edges_list, extra_info_list = [], []\n",
    "\n",
    "    def dfs(node, parent_idx):\n",
    "        idx = len(extra_info_list)\n",
    "        extra_info_list.append(node.extra_info)\n",
    "        edges_list.append((idx, idx))\n",
    "        if parent_idx is not None:\n",
    "            edges_list.append((parent_idx, idx))\n",
    "        for ch in node.children:\n",
    "            dfs(ch, idx)\n",
    "\n",
    "    dfs(root, None)\n",
    "    return edges_list, extra_info_list\n",
    "\n",
    "edges_list, matrix_plans = [], []\n",
    "for i in plans_tree:\n",
    "    edges_matrix, extra_info_matrix = tree_to_graph(i)\n",
    "    # if len(edges_matrix) == 0:\n",
    "    #     print(i)\n",
    "    #     assert False\n",
    "    edges_list.append(edges_matrix)\n",
    "    matrix_plans.append(extra_info_matrix)\n",
    "\n",
    "type(matrix_plans[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc7b712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node Types] 13: ['Bitmap Heap Scan', 'Bitmap Index Scan', 'BitmapAnd', 'Gather', 'Gather Merge', 'Hash', 'Hash Join', 'Index Scan', 'Materialize', 'Merge Join', 'Nested Loop', 'Seq Scan', 'Sort']\n",
      "\n",
      "[Global MUST keys] 10: ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "\n",
      "[Global ALL keys] 41: ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Filter', 'Hash Batches', 'Hash Buckets', 'Hash Cond', 'Index Cond', 'Index Name', 'Inner Unique', 'Join Filter', 'Join Type', 'Lossy Heap Blocks', 'Merge Cond', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Rows Removed by Join Filter', 'Scan Direction', 'Single Copy', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers', 'Workers Launched', 'Workers Planned']\n",
      "\n",
      "[Per-key unique values] (sample if large)\n",
      "  - Actual Loops: 12749  sample: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  - Actual Rows: 139247  sample: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Actual Startup Time: 209032  sample: [0.25, 1.875, 0.625, 0.375, 0.75, 5.5, 4.5, 4.625, 8.5, 4.875]\n",
      "  - Actual Total Time: 320912  sample: [0.375, 0.125, 0.625, 0.5, 0.875, 0.25, 0.75, 4.5, 8.375, 1.625]\n",
      "  - Alias: 6  sample: ['ci', 'mi_idx', 't', 'mi', 'mk', 'mc']\n",
      "  - Exact Heap Blocks: 10880  sample: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  - Filter: 34772  sample: ['(company_id < 84218)', '(company_id < 123936)', '(keyword_id < 3154)', '(company_id > 426)', '(person_id > 174039)', '(keyword_id < 70195)', '(person_id < 3268430)', '(person_id > 3132838)', '(person_id > 3559681)', '(person_id < 2333733)']\n",
      "  - Hash Batches: 11  sample: [32, 1, 64, 128, 4, 256, 1024, 512, 8, 2, 16]\n",
      "  - Hash Buckets: 9  sample: [32768, 65536, 8192, 131072, 16384, 262144, 1024, 4096, 2048]\n",
      "  - Hash Cond: 21  sample: ['(mi_idx.movie_id = mc.movie_id)', '(mi_idx.movie_id = t.id)', '(mc.movie_id = t.id)', '(ci.movie_id = mi_idx.movie_id)', '(mk.movie_id = mi_idx.movie_id)', '(mk.movie_id = ci.movie_id)', '(mk.movie_id = t.id)', '(mi.movie_id = mk.movie_id)', '(ci.movie_id = t.id)', '(mi_idx.movie_id = ci.movie_id)', '(mi.movie_id = t.id)', '(mi.movie_id = mc.movie_id)', '(t.id = mi_idx.movie_id)', '(mc.movie_id = ci.movie_id)', '(mk.movie_id = mc.movie_id)', '(t.id = ci.movie_id)', '(mi.movie_id = ci.movie_id)', '(ci.movie_id = mc.movie_id)', '(mi.movie_id = mi_idx.movie_id)', '(t.id = mc.movie_id)', '(mc.movie_id = mi_idx.movie_id)']\n",
      "  - Index Cond: 23100  sample: ['(keyword_id < 3154)', '(company_id = 8753)', '(person_id = 1215617)', '(keyword_id = 2653)', '(person_id = 2435219)', '(person_id = 461148)', '(person_id = 1164236)', '(keyword_id = 43102)', '(person_id > 3998209)', '(person_id < 496343)']\n",
      "  - Index Name: 14  sample: ['movie_id_movie_info_idx', 'movie_id_movie_info', 'company_id_movie_companies', 'keyword_id_movie_keyword', 'movie_id_movie_companies', 'movie_id_cast_info', 'role_id_cast_info', 'title_pkey', 'info_type_id_movie_info', 'movie_id_movie_keyword', 'company_type_id_movie_companies', 'person_id_cast_info', 'info_type_id_movie_info_idx', 'kind_id_title']\n",
      "  - Inner Unique: 2  sample: [False, True]\n",
      "  - Join Filter: 9  sample: ['(ci.movie_id = t.id)', '(t.id = ci.movie_id)', '(t.id = mi.movie_id)', '(t.id = mi_idx.movie_id)', '(mi.movie_id = t.id)', '(mi_idx.movie_id = t.id)', '(mc.movie_id = t.id)', '(t.id = mk.movie_id)', '(t.id = mc.movie_id)']\n",
      "  - Join Type: 1  sample: ['Inner']\n",
      "  - Lossy Heap Blocks: 1  sample: [0]\n",
      "  - Merge Cond: 8  sample: ['(t.id = mi_idx.movie_id)', '(t.id = ci.movie_id)', '(t.id = mi.movie_id)', '(mi_idx.movie_id = ci.movie_id)', '(mc.movie_id = t.id)', '(t.id = mk.movie_id)', '(t.id = mc.movie_id)', '(mk.movie_id = mi.movie_id)']\n",
      "  - Node Type: 13  sample: ['Bitmap Heap Scan', 'Bitmap Index Scan', 'Seq Scan', 'Hash Join', 'Sort', 'Gather Merge', 'Nested Loop', 'Merge Join', 'BitmapAnd', 'Index Scan', 'Hash', 'Materialize', 'Gather']\n",
      "  - Original Hash Batches: 11  sample: [32, 1, 64, 128, 4, 512, 256, 1024, 8, 2, 16]\n",
      "  - Original Hash Buckets: 9  sample: [32768, 65536, 16384, 131072, 8192, 1024, 4096, 262144, 2048]\n",
      "  - Parallel Aware: 2  sample: [False, True]\n",
      "  - Parent Relationship: 3  sample: ['Member', 'Inner', 'Outer']\n",
      "  - Peak Memory Usage: 2603  sample: [8192, 10, 13, 17, 21, 23, 27, 35, 48, 8256]\n",
      "  - Plan Rows: 141195  sample: [1048576, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Plan Width: 32  sample: [0, 131, 134, 136, 12, 146, 148, 25, 159, 161, 37, 40, 168, 42, 176, 180, 52, 54, 193, 65, 67, 74, 208, 210, 82, 86, 94, 99, 106, 114, 116, 119]\n",
      "  - Recheck Cond: 13632  sample: ['(keyword_id = 3946)', '(keyword_id > 10827)', '(keyword_id < 3154)', '(keyword_id > 16270)', '(keyword_id = 1281)', '(keyword_id < 1192)', '(keyword_id = 2653)', '(keyword_id = 7870)', '(keyword_id > 63779)', '(keyword_id = 6092)']\n",
      "  - Relation Name: 6  sample: ['cast_info', 'movie_info_idx', 'title', 'movie_companies', 'movie_info', 'movie_keyword']\n",
      "  - Rows Removed by Filter: 34719  sample: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Rows Removed by Index Recheck: 1  sample: [0]\n",
      "  - Rows Removed by Join Filter: 1  sample: [0]\n",
      "  - Scan Direction: 1  sample: ['Forward']\n",
      "  - Single Copy: 1  sample: [False]\n",
      "  - Sort Key: 3  sample: ['[\"ci.movie_id\"]', '[\"mk.movie_id\"]', '[\"mi.movie_id\"]']\n",
      "  - Sort Method: 3  sample: ['external sort', 'external merge', 'quicksort']\n",
      "  - Sort Space Type: 2  sample: ['Memory', 'Disk']\n",
      "  - Sort Space Used: 5  sample: [1888, 2464, 8912, 1465, 4026]\n",
      "  - Startup Cost: 73298  sample: [0.0, 131073.71, 262145.65, 3.25, 131075.88, 3.5, 262153.76, 131084.36, 524301.42, 131087.93]\n",
      "  - Total Cost: 149033  sample: [0.5, 1.75, 2.5, 3.75, 1.5, 0.75, 6.0, 2.0, 8.25, 2.25]\n",
      "  - Workers: 6  sample: ['[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4078, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 3950, \"Worker Number\": 1}]', '[]', '[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4026, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4026, \"Worker Number\": 1}]', '[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 1465, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 1465, \"Worker Number\": 1}]', '[{\"Sort Method\": \"external merge\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 8912, \"Worker Number\": 0}, {\"Sort Method\": \"external merge\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 8912, \"Worker Number\": 1}]', '[{\"Sort Method\": \"external sort\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 1888, \"Worker Number\": 0}, {\"Sort Method\": \"external sort\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 1888, \"Worker Number\": 1}]']\n",
      "  - Workers Launched: 2  sample: [1, 2]\n",
      "  - Workers Planned: 2  sample: [1, 2]\n",
      "\n",
      "[Per NodeType key stats]\n",
      "## Bitmap Heap Scan\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Lossy Heap Blocks', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Index Recheck', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(6): ['Alias', 'Exact Heap Blocks', 'Lossy Heap Blocks', 'Recheck Cond', 'Relation Name', 'Rows Removed by Index Recheck']\n",
      "   max(20): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Filter', 'Lossy Heap Blocks', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Filter', 'Parent Relationship', 'Rows Removed by Filter', 'Workers']\n",
      "## Bitmap Index Scan\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Index Cond', 'Index Name', 'Parent Relationship']\n",
      "   max(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## BitmapAnd\n",
      "   must_all(11): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(1): ['Parent Relationship']\n",
      "   max(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Gather\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Single Copy', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   must_only_type(3): ['Single Copy', 'Workers Launched', 'Workers Planned']\n",
      "   max(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Single Copy', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   optional(1): ['Parent Relationship']\n",
      "## Gather Merge\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   must_only_type(3): ['Parent Relationship', 'Workers Launched', 'Workers Planned']\n",
      "   max(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   optional(0): []\n",
      "## Hash\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Batches', 'Hash Buckets', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(6): ['Hash Batches', 'Hash Buckets', 'Original Hash Batches', 'Original Hash Buckets', 'Parent Relationship', 'Peak Memory Usage']\n",
      "   max(17): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Batches', 'Hash Buckets', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Hash Join\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Cond', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Hash Cond', 'Inner Unique', 'Join Type']\n",
      "   max(15): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Cond', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(2): ['Parent Relationship', 'Workers']\n",
      "## Index Scan\n",
      "   must_all(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Index Name', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Relation Name', 'Scan Direction', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(4): ['Alias', 'Index Name', 'Relation Name', 'Scan Direction']\n",
      "   max(20): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Filter', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Scan Direction', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(6): ['Filter', 'Index Cond', 'Parent Relationship', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Workers']\n",
      "## Materialize\n",
      "   must_all(11): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(1): ['Parent Relationship']\n",
      "   max(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Merge Join\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Merge Cond', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Inner Unique', 'Join Type', 'Merge Cond']\n",
      "   max(15): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Merge Cond', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(2): ['Parent Relationship', 'Workers']\n",
      "## Nested Loop\n",
      "   must_all(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(2): ['Inner Unique', 'Join Type']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Filter', 'Join Type', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Rows Removed by Join Filter', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Join Filter', 'Parent Relationship', 'Rows Removed by Join Filter', 'Workers']\n",
      "## Seq Scan\n",
      "   must_all(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Relation Name', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(2): ['Alias', 'Relation Name']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Filter', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Relation Name', 'Rows Removed by Filter', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Filter', 'Parent Relationship', 'Rows Removed by Filter', 'Workers']\n",
      "## Sort\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   must_only_type(6): ['Parent Relationship', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Workers']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(0): []\n"
     ]
    }
   ],
   "source": [
    "from models.Utils import StatisticsInfo\n",
    "\n",
    "statisticsInfo = StatisticsInfo(matrix_plans, sample_threshold=100, sample_k=10).build()\n",
    "statisticsInfo.pretty_print_report()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbe03226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1028173, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1028173, 0]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NodeVectorizer\n",
    "import re, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "from models.Utils import process_join_cond_field, process_index_cond_field, load_column_stats\n",
    "\n",
    "# -------- 词表 --------\n",
    "class Vocab:\n",
    "    def __init__(self): self.idx = {\"<pad>\":0, \"<unk>\":1}\n",
    "    def add(self, s):\n",
    "        if s not in self.idx: self.idx[s] = len(self.idx)\n",
    "    def get(self, s): return self.idx.get(s, 1)\n",
    "    @property\n",
    "    def size(self): return len(self.idx)\n",
    "\n",
    "NodeTypeVocab = ['Bitmap Heap Scan', 'Bitmap Index Scan', 'BitmapAnd', 'Gather', 'Gather Merge', 'Hash', 'Hash Join', 'Index Scan', 'Materialize', 'Merge Join', 'Nested Loop', 'Seq Scan', 'Sort']\n",
    "\n",
    "\n",
    "def NodeVectorizer(matrix_plans : List[List[dict]]) -> List[List[List[List]]]:\n",
    "    res = []\n",
    "    for mp in matrix_plans:\n",
    "        plan_matrix = []\n",
    "        for node in mp:\n",
    "            node_vector = [0] * 16\n",
    "            \n",
    "            # [0]-[12] node_type one-hot\n",
    "            try:\n",
    "                node_type_idx = NodeTypeVocab.index(node[\"Node Type\"])\n",
    "                node_vector[node_type_idx] = 1\n",
    "            except:\n",
    "                print(node)\n",
    "                assert False\n",
    "            \n",
    "            # [7]-[8] plan_width & plan_rows\n",
    "            node_vector[len(NodeTypeVocab)] = node[\"Plan Width\"]\n",
    "            node_vector[len(NodeTypeVocab)+1] = node[\"Plan Rows\"]\n",
    "\n",
    "            # [9]-[14] other_keys\n",
    "            # TODO\n",
    "\n",
    "            # [15]-[24] join\n",
    "            # TODO\n",
    "\n",
    "            # [25]-[34] scan\n",
    "            # TODO\n",
    "\n",
    "            # [35]-[44] sort\n",
    "            # TODO\n",
    "\n",
    "            # [45]-[54] group\n",
    "            # TODO\n",
    "\n",
    "            plan_matrix.append(node_vector)\n",
    "        res.append(plan_matrix)\n",
    "    return res\n",
    "\n",
    "res = NodeVectorizer(matrix_plans)\n",
    "res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e52b3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class NodeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: data.x 形状 [N, F_in]\n",
    "    输出: node_embs [N, d_node]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, d_node: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, d_node),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_node),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "# ---- 组合总模型 ----\n",
    "class PlanCostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        期望 data 里至少有:\n",
    "        - x: [N, F_in]\n",
    "        - edge_index: [2, E]\n",
    "        - batch: [N]  指示每个节点属于哪张图\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        g = self.treeencoder(x, data.edge_index)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "\n",
    "from models.TreeEncoder import GATTreeEncoder\n",
    "from models.PredictionHead import PredictionHead\n",
    "# ---- 使用示例 ----\n",
    "# 假设你的节点原始特征维度 F_in=64，节点隐层 d_node=128，图级维度 d_graph=256\n",
    "F_in, d_node, d_graph = 16, 32, 64\n",
    "nodecoder = NodeEncoder(F_in, d_node)\n",
    "gatTreeEncoder = GATTreeEncoder(\n",
    "    input_dim=d_node,      # 一定用实际特征维度\n",
    "    hidden_dim=64,\n",
    "    output_dim=d_graph,\n",
    "    num_layers=3,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    pooling=\"mean\"\n",
    ")\n",
    "predict_head = PredictionHead(d_graph, out_dim=1)\n",
    "\n",
    "model = PlanCostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8000bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "PlanCostModel(\n",
      "  (nodecoder): NodeEncoder(\n",
      "    (proj): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (treeencoder): GATTreeEncoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GATConv(32, 64, heads=4)\n",
      "      (1-2): 2 x GATConv(256, 64, heads=4)\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (output_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (predict_head): PredictionHead(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(type(ExecutionTimes))\n",
    "print(type(res))\n",
    "print(type(edges_list))\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64953496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def coerce_x_to_tensor(x_plan, in_dim: int):\n",
    "    \"\"\"\n",
    "    x_plan: 很深的 list（最终行向量长度= in_dim）\n",
    "    变成 [N, in_dim] 的 float32 Tensor\n",
    "    \"\"\"\n",
    "    x = torch.tensor(x_plan, dtype=torch.float32)\n",
    "    assert x.numel() % in_dim == 0, f\"最后一维应为 {in_dim}，拿到形状 {tuple(x.shape)}\"\n",
    "    x = x.view(-1, in_dim)   # 拉平成 [N, in_dim]\n",
    "    return x\n",
    "\n",
    "def coerce_edge_index(ei_like):\n",
    "    \"\"\"\n",
    "    ei_like: list/ndarray/tensor, 形状 [2,E] 或 [E,2]\n",
    "    返回规范 [2,E] 的 long Tensor\n",
    "    \"\"\"\n",
    "    ei = torch.as_tensor(ei_like, dtype=torch.long)\n",
    "    if ei.ndim != 2:\n",
    "        raise ValueError(f\"edge_index 需要二维，拿到 {tuple(ei.shape)}\")\n",
    "    if ei.shape[0] != 2 and ei.shape[1] == 2:\n",
    "        ei = ei.t().contiguous()\n",
    "    elif ei.shape[0] != 2 and ei.shape[1] != 2:\n",
    "        raise ValueError(f\"edge_index 需为 [2,E] 或 [E,2]，拿到 {tuple(ei.shape)}\")\n",
    "    return ei.contiguous()\n",
    "\n",
    "def build_dataset(res, edges_list, execution_times, in_dim=16, bidirectional=False):\n",
    "    assert len(res) == len(edges_list) == len(execution_times), \"长度必须一致\"\n",
    "    data_list = []\n",
    "    for i, (x_plan, ei_like, y) in enumerate(zip(res, edges_list, execution_times)):\n",
    "        x = coerce_x_to_tensor(x_plan, in_dim)      # [N, in_dim]\n",
    "        edge_index = coerce_edge_index(ei_like)     # [2,E]\n",
    "        N = x.size(0)\n",
    "\n",
    "        # 边索引有效性检查\n",
    "        if edge_index.numel() > 0:\n",
    "            if int(edge_index.min()) < 0 or int(edge_index.max()) >= N:\n",
    "                raise ValueError(f\"plan[{i}] 的 edge_index 越界：节点数 N={N}，但 edge_index.max={int(edge_index.max())}\")\n",
    "\n",
    "        # 可选：做成双向图（若你的 edges 只有父->子）\n",
    "        if bidirectional:\n",
    "            edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "        y = torch.tensor([float(y)], dtype=torch.float32)  # 图级回归标签\n",
    "        data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc0d3024",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)                \u001b[38;5;66;03m# .x / .edge_index / .batch / .y\u001b[39;00m\n\u001b[1;32m     20\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(batch)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)         \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m y    \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview_as(pred)            \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred, y)\n\u001b[1;32m     24\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设你已有 model = PlanCostModel(nodecoder, treeencoder, predict_head)\n",
    "in_dim = 16\n",
    "dataset = build_dataset(res, edges_list, ExecutionTimes, in_dim=in_dim, bidirectional=False)\n",
    "\n",
    "loader  = DataLoader(dataset, batch_size=32, shuffle=True)  # PyG 的 DataLoader 会自动做 Batch\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model   = model.to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    mse_sum, n = 0.0, 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)                # .x / .edge_index / .batch / .y\n",
    "        pred = model(batch).squeeze(-1)         # [B]\n",
    "        y    = batch.y.view_as(pred)            # [B]\n",
    "        loss = F.mse_loss(pred, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        mse_sum += loss.item() * y.numel()\n",
    "        n += y.numel()\n",
    "    print(f\"[epoch {ep}] RMSE={math.sqrt(mse_sum/max(1,n)):.4f}\")\n",
    "\n",
    "# 简评估（用训练集演示）\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, gts = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        p = model(batch).squeeze(-1)\n",
    "        preds.append(p.cpu()); gts.append(batch.y.view_as(p).cpu())\n",
    "    preds = torch.cat(preds); gts = torch.cat(gts)\n",
    "    rmse = torch.sqrt(F.mse_loss(preds, gts)).item()\n",
    "    mae  = torch.mean(torch.abs(preds - gts)).item()\n",
    "    print(f\"[eval] RMSE={rmse:.4f}  MAE={mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3289a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
