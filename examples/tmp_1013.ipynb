{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645a12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848859e",
   "metadata": {},
   "source": [
    "# 1.数据预处理模块\n",
    "**DataPreprocess.py**   \n",
    "目前的数据主要为json格式.\n",
    "需要进过多部处理才可以作为输入模型的数据.\n",
    "\n",
    "1. 读取json文件\n",
    "2. 转为为NodePlan\n",
    "3. 转为为dataframe (添加两个字段: sql_id, parent_id)\n",
    "4. 统计各个特征的分布等.数据统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2023efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part17.csv', '../data/train_plan_part8.csv', '../data/train_plan_part6.csv', '../data/train_plan_part3.csv', '../data/train_plan_part19.csv', '../data/train_plan_part9.csv', '../data/train_plan_part11.csv', '../data/train_plan_part1.csv', '../data/train_plan_part0.csv', '../data/train_plan_part18.csv', '../data/train_plan_part10.csv', '../data/train_plan_part12.csv', '../data/train_plan_part16.csv', '../data/train_plan_part15.csv', '../data/train_plan_part2.csv', '../data/train_plan_part14.csv', '../data/train_plan_part5.csv', '../data/train_plan_part7.csv', '../data/train_plan_part13.csv', '../data/train_plan_part4.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  85000  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "1  85001  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "2  85002  {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "3  85003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  85004  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Parallel Aware\": false, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 11788.77, \"Total Cost\": 49094.94, \"Plan Rows\": 1028173, \"Plan Width\": 12, \"Actual Startup Time\": 41.924, \"Actual Total Time\": 200.35, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Recheck Cond\": \"(keyword_id > 17243)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 24037, \"Lossy Heap Blocks\": 0, \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 11531.73, \"Plan Rows\": 1028173, \"Plan Width\": 0, \"Actual Startup Time\": 39.572, \"Actual Total Time\": 39.572, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id > 17243)\"}]}, \"Planning Time\": 1.679, \"Triggers\": [], \"Execution Time\": 224.454}\n",
      "plans_dict:\n",
      " [{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 59038.23, 'Total Cost': 292179.55, 'Plan Rows': 958880, 'Plan Width': 136, 'Actual Startup Time': 730.101, 'Actual Total Time': 1214.107, 'Actual Rows': 934003, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 58038.23, 'Total Cost': 195291.55, 'Plan Rows': 399533, 'Plan Width': 136, 'Actual Startup Time': 714.231, 'Actual Total Time': 999.792, 'Actual Rows': 311334, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 96872.38, 'Plan Rows': 1832141, 'Plan Width': 42, 'Actual Startup Time': 0.839, 'Actual Total Time': 252.992, 'Actual Rows': 1441006, 'Actual Loops': 3, 'Index Cond': '(role_id = 10)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 185.267, 'Actual Total Time': 185.268, 'Actual Rows': 159056, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 2016, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 229731, 'Plan Width': 94, 'Actual Startup Time': 0.229, 'Actual Total Time': 139.643, 'Actual Rows': 159056, 'Actual Loops': 3, 'Filter': '((kind_id < 7) AND (production_year > 1999))', 'Rows Removed by Filter': 683715, 'Workers': []}]}]}]}, {'Node Type': 'Hash Join', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 109926.51, 'Total Cost': 255406.01, 'Plan Rows': 2072097, 'Plan Width': 106, 'Actual Startup Time': 625.826, 'Actual Total Time': 2162.527, 'Actual Rows': 462768, 'Actual Loops': 1, 'Inner Unique': True, 'Hash Cond': '(mk.movie_id = t.id)', 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 0.0, 'Total Cost': 81003.13, 'Plan Rows': 3360520, 'Plan Width': 12, 'Actual Startup Time': 0.021, 'Actual Total Time': 422.848, 'Actual Rows': 3346045, 'Actual Loops': 1, 'Filter': '(keyword_id < 15855)', 'Rows Removed by Filter': 1177885}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Startup Cost': 67602.3, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 622.011, 'Actual Total Time': 622.012, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 64, 'Original Hash Batches': 64, 'Peak Memory Usage': 3011, 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 67602.3, 'Plan Rows': 1558977, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 322.091, 'Actual Rows': 1543264, 'Actual Loops': 1, 'Filter': '(kind_id = 7)', 'Rows Removed by Filter': 985048}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 58428.86, 'Total Cost': 127528.39, 'Plan Rows': 262393, 'Plan Width': 134, 'Actual Startup Time': 352.379, 'Actual Total Time': 552.141, 'Actual Rows': 166070, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 57428.86, 'Total Cost': 100289.09, 'Plan Rows': 109330, 'Plan Width': 134, 'Actual Startup Time': 333.262, 'Actual Total Time': 420.655, 'Actual Rows': 55357, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(mc.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'company_type_id_movie_companies', 'Relation Name': 'movie_companies', 'Alias': 'mc', 'Startup Cost': 0.43, 'Total Cost': 29683.22, 'Plan Rows': 555600, 'Plan Width': 40, 'Actual Startup Time': 0.654, 'Actual Total Time': 69.425, 'Actual Rows': 444961, 'Actual Loops': 3, 'Index Cond': '(company_type_id > 1)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 181.576, 'Actual Total Time': 181.576, 'Actual Rows': 126448, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 32, 'Original Hash Batches': 32, 'Peak Memory Usage': 1728, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 207302, 'Plan Width': 94, 'Actual Startup Time': 0.234, 'Actual Total Time': 140.725, 'Actual Rows': 126448, 'Actual Loops': 3, 'Filter': '((production_year < 1995) AND (kind_id = 7))', 'Rows Removed by Filter': 716322, 'Workers': []}]}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 53012.0, 'Total Cost': 77258.72, 'Plan Rows': 15739, 'Plan Width': 136, 'Actual Startup Time': 158.907, 'Actual Total Time': 255.435, 'Actual Rows': 7205, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 52012.0, 'Total Cost': 74684.82, 'Plan Rows': 6558, 'Plan Width': 136, 'Actual Startup Time': 143.201, 'Actual Total Time': 233.676, 'Actual Rows': 2402, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Scan Direction': 'Forward', 'Index Name': 'role_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 21600.99, 'Plan Rows': 408484, 'Plan Width': 42, 'Actual Startup Time': 1.14, 'Actual Total Time': 46.233, 'Actual Rows': 299463, 'Actual Loops': 3, 'Index Cond': '(role_id = 5)', 'Rows Removed by Index Recheck': 0, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 141.336, 'Actual Total Time': 141.336, 'Actual Rows': 14129, 'Actual Loops': 3, 'Hash Buckets': 65536, 'Original Hash Buckets': 65536, 'Hash Batches': 1, 'Original Hash Batches': 1, 'Peak Memory Usage': 5536, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 16913, 'Plan Width': 94, 'Actual Startup Time': 0.19, 'Actual Total Time': 137.827, 'Actual Rows': 14129, 'Actual Loops': 3, 'Filter': '((kind_id > 4) AND (production_year = 2002))', 'Rows Removed by Filter': 828642, 'Workers': []}]}]}]}]\n",
      "execution_times:\n",
      " [224.454, 1238.107, 2174.023, 556.23, 255.674]\n",
      "{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0}\n",
      "{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}\n",
      "[(0, 0), (1, 1), (0, 1)]\n",
      "[(0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plan_id</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>Node Type</th>\n",
       "      <th>Parallel Aware</th>\n",
       "      <th>Relation Name</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Startup Cost</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Plan Rows</th>\n",
       "      <th>Plan Width</th>\n",
       "      <th>...</th>\n",
       "      <th>Peak Memory Usage</th>\n",
       "      <th>Filter</th>\n",
       "      <th>Rows Removed by Filter</th>\n",
       "      <th>Join Filter</th>\n",
       "      <th>Rows Removed by Join Filter</th>\n",
       "      <th>Merge Cond</th>\n",
       "      <th>Sort Key</th>\n",
       "      <th>Sort Method</th>\n",
       "      <th>Sort Space Used</th>\n",
       "      <th>Sort Space Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitmap Heap Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>movie_keyword</td>\n",
       "      <td>mk</td>\n",
       "      <td>11788.77</td>\n",
       "      <td>49094.94</td>\n",
       "      <td>1028173</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitmap Index Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11531.73</td>\n",
       "      <td>1028173</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Gather</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59038.23</td>\n",
       "      <td>292179.55</td>\n",
       "      <td>958880</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hash Join</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58038.23</td>\n",
       "      <td>195291.55</td>\n",
       "      <td>399533</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Index Scan</td>\n",
       "      <td>True</td>\n",
       "      <td>cast_info</td>\n",
       "      <td>ci</td>\n",
       "      <td>0.44</td>\n",
       "      <td>96872.38</td>\n",
       "      <td>1832141</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   plan_id  node_idx          Node Type  Parallel Aware  Relation Name Alias  \\\n",
       "0        0         0   Bitmap Heap Scan           False  movie_keyword    mk   \n",
       "1        0         1  Bitmap Index Scan           False            NaN   NaN   \n",
       "2        1         0             Gather           False            NaN   NaN   \n",
       "3        1         1          Hash Join            True            NaN   NaN   \n",
       "4        1         2         Index Scan            True      cast_info    ci   \n",
       "\n",
       "   Startup Cost  Total Cost  Plan Rows  Plan Width  ...  Peak Memory Usage  \\\n",
       "0      11788.77    49094.94    1028173          12  ...                NaN   \n",
       "1          0.00    11531.73    1028173           0  ...                NaN   \n",
       "2      59038.23   292179.55     958880         136  ...                NaN   \n",
       "3      58038.23   195291.55     399533         136  ...                NaN   \n",
       "4          0.44    96872.38    1832141          42  ...                NaN   \n",
       "\n",
       "   Filter  Rows Removed by Filter  Join Filter Rows Removed by Join Filter  \\\n",
       "0     NaN                     NaN          NaN                         NaN   \n",
       "1     NaN                     NaN          NaN                         NaN   \n",
       "2     NaN                     NaN          NaN                         NaN   \n",
       "3     NaN                     NaN          NaN                         NaN   \n",
       "4     NaN                     NaN          NaN                         NaN   \n",
       "\n",
       "   Merge Cond  Sort Key  Sort Method Sort Space Used Sort Space Type  \n",
       "0         NaN       NaN          NaN             NaN             NaN  \n",
       "1         NaN       NaN          NaN             NaN             NaN  \n",
       "2         NaN       NaN          NaN             NaN             NaN  \n",
       "3         NaN       NaN          NaN             NaN             NaN  \n",
       "4         NaN       NaN          NaN             NaN             NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.数据处理\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "from models.DataPreprocessor import get_plans_dict, DataPreprocessor, plan_trees_to_graphs, graphs_to_df, df_to_graphs, DFStatisticsInfo\n",
    "\n",
    "json_path = \"../data/train_plan_*.csv\"\n",
    "plans_dict, execution_times = get_plans_dict(json_path)\n",
    "print(\"plans_dict:\\n\", plans_dict[0:5])\n",
    "print(\"execution_times:\\n\", execution_times[0:5])\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)\n",
    "\n",
    "edges_list, matrix_plans = plan_trees_to_graphs(plans_tree, add_self_loops=True, undirected=False)\n",
    "print(matrix_plans[0][0])\n",
    "print(matrix_plans[0][1])\n",
    "print(edges_list[0])\n",
    "print(edges_list[99])\n",
    "\n",
    "plans_df = graphs_to_df(matrix_plans)\n",
    "plans_df.to_csv(\"../data/process/01_plans_df.csv\", index=False)\n",
    "plans_df.head()\n",
    "\n",
    "# stats = DFStatisticsInfo(plans_df, sample_threshold=200, sample_k=10, strict_alias_check=True)\n",
    "# node_types = stats.get_node_type_set()\n",
    "# global_must = stats.global_must_keys()\n",
    "# global_all  = stats.global_all_keys()\n",
    "# per_key     = stats.per_key_values()\n",
    "# per_type    = stats.per_nodetype_key_stats()\n",
    "# issues      = stats.report_issues()\n",
    "# stats.pretty_print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6199d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n",
      "                name   min      max  cardinality  num_unique_values  \\\n",
      "0               t.id     1  2528312      2528312            2528312   \n",
      "1          t.kind_id     1        7      2528312                  6   \n",
      "2  t.production_year  1880     2019      2528312                133   \n",
      "3              mc.id     1  2609129      2609129            2609129   \n",
      "4      mc.company_id     1   234997      2609129             234997   \n",
      "\n",
      "  table_name      column_name  \n",
      "0          t               id  \n",
      "1          t          kind_id  \n",
      "2          t  production_year  \n",
      "3         mc               id  \n",
      "4         mc       company_id  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3316710/2169505961.py:47: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tmp = df[split_cols].applymap(to_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并predicatge_list\n",
      "处理谓词列表完成\n",
      "处理node_type_id完成\n",
      "处理plan_rows完成\n",
      "处理plan_width完成\n",
      "   plan_id  node_idx          Node Type  Parallel Aware  Relation Name Alias  \\\n",
      "0        0         0   Bitmap Heap Scan           False  movie_keyword    mk   \n",
      "1        0         1  Bitmap Index Scan           False            NaN   NaN   \n",
      "2        1         0             Gather           False            NaN   NaN   \n",
      "3        1         1          Hash Join            True            NaN   NaN   \n",
      "4        1         2         Index Scan            True      cast_info    ci   \n",
      "\n",
      "   Startup Cost  Total Cost  Plan Rows  Plan Width  ...  \\\n",
      "0      11788.77    49094.94    1028173          12  ...   \n",
      "1          0.00    11531.73    1028173           0  ...   \n",
      "2      59038.23   292179.55     958880         136  ...   \n",
      "3      58038.23   195291.55     399533         136  ...   \n",
      "4          0.44    96872.38    1832141          42  ...   \n",
      "\n",
      "           Index Cond_Split        Recheck Cond_Split  \\\n",
      "0                        []  [[keyword_id, >, 17243]]   \n",
      "1  [[keyword_id, >, 17243]]                        []   \n",
      "2                        []                        []   \n",
      "3                        []                        []   \n",
      "4        [[role_id, =, 10]]                        []   \n",
      "\n",
      "            Hash Cond_Split  Join Filter_Split Merge Cond_Split  \\\n",
      "0                        []                 []               []   \n",
      "1                        []                 []               []   \n",
      "2                        []                 []               []   \n",
      "3  [[ci.movie_id, =, t.id]]                 []               []   \n",
      "4                        []                 []               []   \n",
      "\n",
      "            predicatge_list               predicate_list_processed  \\\n",
      "0  [[keyword_id, >, 17243]]  [(15, 1, 0.12850956629325702, False)]   \n",
      "1  [[keyword_id, >, 17243]]  [(15, 1, 0.12850956629325702, False)]   \n",
      "2                        []                                     []   \n",
      "3  [[ci.movie_id, =, t.id]]                     [(1, 0, 17, True)]   \n",
      "4        [[role_id, =, 10]]                   [(3, 0, 0.9, False)]   \n",
      "\n",
      "   node_type_id  plan_rows plan_width  \n",
      "0             0  13.843295   2.564949  \n",
      "1             1  13.843295   0.000000  \n",
      "2             2  13.773522   4.919981  \n",
      "3             3  12.898054   4.919981  \n",
      "4             4  14.420997   3.761200  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': 24037.0, 'Lossy Heap Blocks': 0.0, 'Parent Relationship': nan, 'Index Name': nan, 'Index Cond': nan, 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Join Type': nan, 'Inner Unique': nan, 'Hash Cond': nan, 'Workers': nan, 'Scan Direction': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [['keyword_id', '>', 17243]], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': [], 'predicatge_list': [['keyword_id', '>', 17243]], 'predicate_list_processed': [(15, 1, 0.12850956629325702, False)], 'node_type_id': 0, 'plan_rows': 13.843295097351074, 'plan_width': 2.5649492740631104}, {'Node Type': 'Bitmap Index Scan', 'Parallel Aware': False, 'Relation Name': nan, 'Alias': nan, 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Parent Relationship': 'Outer', 'Index Name': 'keyword_id_movie_keyword', 'Index Cond': '(keyword_id > 17243)', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Join Type': nan, 'Inner Unique': nan, 'Hash Cond': nan, 'Workers': nan, 'Scan Direction': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['keyword_id', '>', 17243]], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': [], 'predicatge_list': [['keyword_id', '>', 17243]], 'predicate_list_processed': [(15, 1, 0.12850956629325702, False)], 'node_type_id': 1, 'plan_rows': 13.843295097351074, 'plan_width': 0.0}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from this import d\n",
    "from models.DataPreprocessor import safe_cond_parse, process_predicate_list\n",
    "import pandas as pd\n",
    "\n",
    "# schema信息\n",
    "db_info = pd.read_csv(\"../data/column_min_max_vals.csv\")\n",
    "db_info[\"table_name\"], db_info[\"column_name\"] = db_info[\"name\"].str.split(\".\").str[0], db_info[\"name\"].str.split(\".\").str[1]\n",
    "print(db_info.head())\n",
    "print(\"-\"*100)\n",
    "\n",
    "# 需要解析的条件列表\n",
    "NEED_PARSE_COND_SCAN = [\n",
    "    \"Filter\",\n",
    "    \"Index Cond\",\n",
    "    \"Recheck Cond\"\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_JOIN = [\n",
    "    \"Hash Cond\",\n",
    "    \"Join Filter\",\n",
    "    \"Merge Cond\",\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_COLS = NEED_PARSE_COND_SCAN + NEED_PARSE_COND_JOIN\n",
    "\n",
    "for col in NEED_PARSE_COND_COLS:\n",
    "    plans_df[f\"{col}_Split\"] = plans_df[col].apply(safe_cond_parse)\n",
    "\n",
    "# 将需要解析的条件列表统一放到predicatge_list\n",
    "def merge_split_predicates(df: pd.DataFrame,\n",
    "                           suffix: str = \"_Split\",\n",
    "                           new_col: str = \"predicatge_list\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    split_cols = [c for c in df.columns if c.endswith(suffix)]\n",
    "    if not split_cols:\n",
    "        df[new_col] = [[] for _ in range(len(df))]\n",
    "        return df\n",
    "\n",
    "    # 保证每个单元格都是 list（NaN/None/标量 → []）\n",
    "    def to_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, tuple):\n",
    "            return list(x)\n",
    "        return []\n",
    "\n",
    "    tmp = df[split_cols].applymap(to_list)\n",
    "\n",
    "    # 行内拼接多个 *_Split 列（展平）\n",
    "    df[new_col] = tmp.apply(lambda row: [item for sub in row for item in sub], axis=1)\n",
    "    return df\n",
    "plans_df = merge_split_predicates(plans_df)\n",
    "print(\"合并predicatge_list\")\n",
    "# 将predicatge_list [lhs, op, rhs] 里面的col变为int， op变为int， value变为int\n",
    "# 确保空值是空列表，方便按行处理\n",
    "plans_df[\"predicatge_list\"] = plans_df[\"predicatge_list\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "\n",
    "# 逐行调用，得到“处理后的谓词列表”\n",
    "# plans_df[\"predicatge_list_processed\"] = plans_df[\"predicatge_list\"].apply(\n",
    "#     lambda lst: process_predicate_list(lst, db_info, plans_df[\"Alias\"])\n",
    "# )\n",
    "plans_df[\"predicate_list_processed\"] = plans_df.apply(\n",
    "    lambda row: process_predicate_list(\n",
    "        predicate_list=row[\"predicatge_list\"],\n",
    "        db_info=db_info,\n",
    "        default_alias=row.get(\"Alias\"),\n",
    "        index_name=row.get(\"Index Name\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "print(\"处理谓词列表完成\")\n",
    "\n",
    "# 1：将nodetype做一个新列名叫node_type_id，\n",
    "# 2：将Plan Rows 和Plan Width也做一个新列plan_rows, plan_width都是做完log1p的\n",
    "import numpy as np\n",
    "node_types = plans_df[\"Node Type\"].unique()\n",
    "node_types_dict = {node_type: i for i, node_type in enumerate(node_types)}\n",
    "plans_df[\"node_type_id\"] = plans_df[\"Node Type\"].map(node_types_dict)\n",
    "print(\"处理node_type_id完成\")\n",
    "plans_df[\"plan_rows\"]  = np.log1p(pd.to_numeric(plans_df[\"Plan Rows\"],  errors=\"coerce\").fillna(0.0)).astype(\"float32\")\n",
    "print(\"处理plan_rows完成\")\n",
    "plans_df[\"plan_width\"] = np.log1p(pd.to_numeric(plans_df[\"Plan Width\"], errors=\"coerce\").fillna(0.0)).astype(\"float32\")\n",
    "print(\"处理plan_width完成\")\n",
    "\n",
    "print(plans_df.head())\n",
    "print(\"-\"*100)\n",
    "graphs = df_to_graphs(plans_df)\n",
    "print(graphs[0])\n",
    "print(\"-\"*100)\n",
    "\n",
    "plans_df.to_csv(\"../data/process/02_plans_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4004d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate_list_processed 列的最长长度为: 3\n"
     ]
    }
   ],
   "source": [
    "# 列的最长长度\n",
    "max_len = max(len(x) for x in plans_df[\"predicate_list_processed\"])\n",
    "print(f\"predicate_list_processed 列的最长长度为: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52da685",
   "metadata": {},
   "source": [
    "# 2. 数据准备\n",
    "TrainAndEval.py\n",
    "\n",
    "## 该模块主要内容\n",
    "1. 将数据划分为训练集,验证集,测试集\n",
    "2. 构建数据集\n",
    "3. 创建数据加载器\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6af54b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 100000\n",
      "tensor([[ 0.0000, 13.8433,  2.5649, 15.0000,  1.0000,  0.1285,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0000, 13.8433,  0.0000, 15.0000,  1.0000,  0.1285,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 1]])\n",
      "tensor([224.4540])\n",
      "tensor([[ 2.0000, 13.7735,  4.9200,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 3.0000, 12.8981,  4.9200,  1.0000,  0.0000, 17.0000,  1.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 4.0000, 14.4210,  3.7612,  3.0000,  0.0000,  0.9000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.0000, 12.3447,  4.5539,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.0000, 12.3447,  4.5539, 18.0000,  3.0000,  1.0000,  0.0000, 19.0000,\n",
      "          1.0000,  0.8561,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0, 1, 0, 2, 1, 3, 1, 4, 3],\n",
      "        [0, 1, 1, 2, 2, 3, 3, 4, 4]])\n",
      "tensor([1238.1071])\n",
      "tensor([[ 3.0000, 14.5441,  4.6728, 16.0000,  0.0000, 17.0000,  1.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.0000, 15.0276,  2.5649, 15.0000,  3.0000,  0.1182,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.0000, 14.2595,  4.5539,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.0000, 14.2595,  4.5539, 18.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0, 1, 0, 2, 0, 3, 2],\n",
      "        [0, 1, 1, 2, 2, 3, 3]])\n",
      "tensor([2174.0229])\n",
      "训练集: 70000, 验证集: 15000, 测试集: 15000\n",
      "训练批次数: 2188, 验证批次数: 469\n"
     ]
    }
   ],
   "source": [
    "# 5. 数据准备\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.TrainAndEval import build_dataset\n",
    "\n",
    "# 构建数据集\n",
    "dataset = build_dataset(graphs, edges_list, execution_times, in_dim=3+3*4, max_len=3)\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "for i in range(3):\n",
    "    print(dataset[i].x)\n",
    "    print(dataset[i].edge_index)\n",
    "    print(dataset[i].y)\n",
    "    # print(f\"样本: x.shape={dataset[i].x.shape}, edge_index.shape={dataset[i].edge_index.shape}, y={dataset[i].y}\")\n",
    "    \n",
    "# 数据集划分\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    range(len(dataset)), test_size=0.3, random_state=42\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = [dataset[i] for i in train_indices]\n",
    "val_dataset = [dataset[i] for i in val_indices]\n",
    "test_dataset = [dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"训练集: {len(train_dataset)}, 验证集: {len(val_dataset)}, 测试集: {len(test_dataset)}\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}, 验证批次数: {len(val_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36b71469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[143, 15], edge_index=[2, 254], y=[32], batch=[143], ptr=[33])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf30b9",
   "metadata": {},
   "source": [
    "# 3. 模型模块\n",
    "\n",
    "## NodeEncoder\n",
    "NodeEncoder.py\n",
    "主要包括了各种vectorical的编码方式.\n",
    "\n",
    "1. 转为为Matrix(Node, Edge)\n",
    "\n",
    "\n",
    "## TreeEncoder\n",
    "TreeEncoder.py\n",
    "目前包括两种模型一个是GAT,一个是传统GNN.\n",
    "1. 转为vector\n",
    "\n",
    "## PredictionHead\n",
    "PredictionHead.py\n",
    "目前进行最简单的FNN进行后续回归任务\n",
    "\n",
    "1. 预测\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c03e4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostModel(\n",
      "  (nodecoder): NodeEncoder_V0(\n",
      "    (type_emb): Embedding(13, 16)\n",
      "    (stats_mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (pred_enc): PredicateEncoder1(\n",
      "      (col_emb): Embedding(32, 8, padding_idx=0)\n",
      "      (op_emb): Embedding(6, 3, padding_idx=0)\n",
      "      (atom_mlp): Sequential(\n",
      "        (0): Linear(in_features=21, out_features=32, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (fuse): Sequential(\n",
      "      (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (treeencoder): TreeEncoder_GATMini(\n",
      "    (gat1): GATConv(32, 64, heads=8)\n",
      "    (gat2): GATConv(512, 64, heads=1)\n",
      "  )\n",
      "  (predict_head): PredictionHead_FNNMini(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from models.NodeEncoder import *\n",
    "from models.TreeEncoder import *\n",
    "from models.PredictionHead import *\n",
    "from models.TrainAndEval import EarlyStopping\n",
    "\n",
    "class CostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        data 结构\n",
    "        - x: [N, F_num]\n",
    "        - edge_index: [2, E]\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        print(x.shape)\n",
    "        print(data.edge_index.shape)\n",
    "        g = self.treeencoder(x, data.edge_index)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "f_in, d_node, d_graph, out_dim = len(dataset[0].x), 32, 64, 1\n",
    "nodecoder = NodeEncoder_V0(\n",
    "    num_node_types=13,\n",
    "    num_cols=32,\n",
    "    num_ops=6,\n",
    "    out_dim=d_node\n",
    ")\n",
    "gatTreeEncoder = TreeEncoder_GATMini(\n",
    "    in_dim=d_node,\n",
    "    hidden_dim=64,\n",
    "    out_dim=d_graph\n",
    ")\n",
    "predict_head = PredictionHead_FNNMini(\n",
    "    in_dim=d_graph,\n",
    "    out_dim=out_dim\n",
    ")\n",
    "\n",
    "# 模型构建\n",
    "model = CostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "print(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023b51d",
   "metadata": {},
   "source": [
    "# 4 训练&评估\n",
    "## 训练\n",
    "主要模块为划分训练集,测试集,验证集.\n",
    "调用模型进行训练.\n",
    "## 评估\n",
    "主要为评估方式.目前为MSE以及Q-error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac3725e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "------------------------------------------------------------\n",
      "a\n",
      "tensor([[ 2.0000, 11.1505,  5.0752,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.0000, 10.2750,  5.0752,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 3.0000, 10.8288,  4.7875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 6.0000, 15.1239,  4.3175,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.0000, 14.5933,  4.5539,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.0000, 14.5933,  4.5539,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 70\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     71\u001b[0m     model, train_loader, val_loader, optimizer, scheduler, \n\u001b[1;32m     72\u001b[0m     criterion, early_stopping, device, weight_path, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n",
      "Cell \u001b[0;32mIn[58], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, criterion, early_stopping, device, weight_path, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 验证\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/Phd/project/GNTO/models/TrainAndEval.py:120\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36mCostModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mdata 结构\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m- x: [N, F_num]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m- edge_index: [2, E]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodecoder(data\u001b[38;5;241m.\u001b[39mx)                                   \u001b[38;5;66;03m# [N, d_node]\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtreeencoder(x, data\u001b[38;5;241m.\u001b[39medge_index)         \u001b[38;5;66;03m# [B, d_graph]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from models.TrainAndEval import train_epoch, validate_epoch\n",
    "import time\n",
    "\n",
    "#  8. 训练循环\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "                early_stopping, device, weight_path, num_epochs=100):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"开始训练...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 训练\n",
    "        print(\"a\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        print(\"b\")\n",
    "        # 验证\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 计算时间\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # 打印进度\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\n早停触发在第 {epoch+1} 轮\")\n",
    "            break\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            date = datetime.now().strftime(\"%m%d\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, f'../results/weight_{date}.pth')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"训练完成!\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 开始训练\n",
    "date = datetime.now().strftime(\"%m%d\")\n",
    "weight_path = f'../results/{date}.pth'\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, \n",
    "    criterion, early_stopping, device, weight_path, num_epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc95b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 绘制结果\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m plot_training_history(train_losses, val_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. 可视化训练过程和结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 预测 vs 真实值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(targets, predictions, alpha=0.5, s=20)\n",
    "    min_val = min(targets.min(), predictions.min())\n",
    "    max_val = max(targets.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.xlabel('True Execution Time')\n",
    "    plt.ylabel('Predicted Execution Time')\n",
    "    plt.title('Predicted vs True Execution Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    date = datetime.now().strftime(\"%m%d\")\n",
    "    plt.savefig(f'../results/training_results_{date}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "# 绘制结果\n",
    "plot_training_history(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1f73a",
   "metadata": {},
   "source": [
    "$$\n",
    "x = [\n",
    "\\underbrace{x_1, ..., x_6,\\ }_{\\text{categorical}}\n",
    "\\underbrace{x_7, x_8,\\ }_{\\text{numeric}}\n",
    "\\underbrace{x_9, ..., x_{16},\\ }_{\\text{categorical}}\n",
    "\\underbrace{x_{17}, ..., x_{32}}_{\\text{conditional}}\n",
    "]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{x} = [\n",
    "\\underbrace{\\tilde{x}_1, ..., \\tilde{x}_6,\\ }_{\\text{NodeType}}\n",
    "\\underbrace{\\tilde{x}_7, \\tilde{x}_8,\\ }_{\\text{Size}}\n",
    "\\underbrace{\\tilde{x}_9, ..., \\tilde{x}_{16},\\ }_{\\text{Table}}\n",
    "\\underbrace{\\tilde{x}_{17}, ..., \\tilde{x}_{32}}_{\\text{Predicate}}\n",
    "]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9987c28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
