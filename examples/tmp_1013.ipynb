{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645a12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848859e",
   "metadata": {},
   "source": [
    "# 1.数据预处理模块\n",
    "**DataPreprocess.py**   \n",
    "目前的数据主要为json格式.\n",
    "需要进过多部处理才可以作为输入模型的数据.\n",
    "\n",
    "1. 读取json文件\n",
    "2. 转为为NodePlan\n",
    "3. 转为为dataframe (添加两个字段: sql_id, parent_id)\n",
    "4. 统计各个特征的分布等.数据统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2023efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part4.csv', '../data/train_plan_part14.csv', '../data/train_plan_part9.csv', '../data/train_plan_part16.csv', '../data/train_plan_part6.csv', '../data/train_plan_part0.csv', '../data/train_plan_part17.csv', '../data/train_plan_part11.csv', '../data/train_plan_part3.csv', '../data/train_plan_part13.csv', '../data/train_plan_part8.csv', '../data/train_plan_part12.csv', '../data/train_plan_part10.csv', '../data/train_plan_part1.csv', '../data/train_plan_part7.csv', '../data/train_plan_part19.csv', '../data/train_plan_part15.csv', '../data/train_plan_part18.csv', '../data/train_plan_part2.csv', '../data/train_plan_part5.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  20000  {\"Plan\": {\"Node Type\": \"Nested Loop\", \"Paralle...\n",
      "1  20001  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "2  20002  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "3  20003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  20004  {\"Plan\": {\"Node Type\": \"Merge Join\", \"Parallel...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Nested Loop\", \"Parallel Aware\": false, \"Join Type\": \"Inner\", \"Startup Cost\": 1173.46, \"Total Cost\": 90035.89, \"Plan Rows\": 214713, \"Plan Width\": 148, \"Actual Startup Time\": 1.621, \"Actual Total Time\": 2274.459, \"Actual Rows\": 419818, \"Actual Loops\": 1, \"Inner Unique\": false, \"Plans\": [{\"Node Type\": \"Gather\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Startup Cost\": 1173.02, \"Total Cost\": 60095.54, \"Plan Rows\": 14988, \"Plan Width\": 106, \"Actual Startup Time\": 1.611, \"Actual Total Time\": 8.006, \"Actual Rows\": 14373, \"Actual Loops\": 1, \"Workers Planned\": 2, \"Workers Launched\": 2, \"Single Copy\": false, \"Plans\": [{\"Node Type\": \"Nested Loop\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Join Type\": \"Inner\", \"Startup Cost\": 173.02, \"Total Cost\": 57596.74, \"Plan Rows\": 6245, \"Plan Width\": 106, \"Actual Startup Time\": 0.739, \"Actual Total Time\": 70.797, \"Actual Rows\": 4791, \"Actual Loops\": 3, \"Inner Unique\": true, \"Workers\": [], \"Plans\": [{\"Node Type\": \"Bitmap Heap Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": true, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 172.59, \"Total Cost\": 22567.1, \"Plan Rows\": 6245, \"Plan Width\": 12, \"Actual Startup Time\": 0.468, \"Actual Total Time\": 16.425, \"Actual Rows\": 4791, \"Actual Loops\": 3, \"Recheck Cond\": \"(keyword_id < 3)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 14, \"Lossy Heap Blocks\": 0, \"Workers\": [], \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 168.84, \"Plan Rows\": 14988, \"Plan Width\": 0, \"Actual Startup Time\": 0.818, \"Actual Total Time\": 0.818, \"Actual Rows\": 14373, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id < 3)\", \"Workers\": []}]}, {\"Node Type\": \"Index Scan\", \"Parent Relationship\": \"Inner\", \"Parallel Aware\": false, \"Scan Direction\": \"Forward\", \"Index Name\": \"title_pkey\", \"Relation Name\": \"title\", \"Alias\": \"t\", \"Startup Cost\": 0.43, \"Total Cost\": 5.61, \"Plan Rows\": 1, \"Plan Width\": 94, \"Actual Startup Time\": 0.011, \"Actual Total Time\": 0.011, \"Actual Rows\": 1, \"Actual Loops\": 14373, \"Index Cond\": \"(id = mk.movie_id)\", \"Rows Removed by Index Recheck\": 0, \"Workers\": []}]}]}, {\"Node Type\": \"Index Scan\", \"Parent Relationship\": \"Inner\", \"Parallel Aware\": false, \"Scan Direction\": \"Forward\", \"Index Name\": \"movie_id_cast_info\", \"Relation Name\": \"cast_info\", \"Alias\": \"ci\", \"Startup Cost\": 0.44, \"Total Cost\": 1.6, \"Plan Rows\": 40, \"Plan Width\": 42, \"Actual Startup Time\": 0.008, \"Actual Total Time\": 0.151, \"Actual Rows\": 29, \"Actual Loops\": 14373, \"Index Cond\": \"(movie_id = t.id)\", \"Rows Removed by Index Recheck\": 0}]}, \"Planning Time\": 3.12, \"Triggers\": [], \"Execution Time\": 2290.531}\n",
      "plans_dict:\n",
      " [{'Node Type': 'Nested Loop', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 1173.46, 'Total Cost': 90035.89, 'Plan Rows': 214713, 'Plan Width': 148, 'Actual Startup Time': 1.621, 'Actual Total Time': 2274.459, 'Actual Rows': 419818, 'Actual Loops': 1, 'Inner Unique': False, 'Plans': [{'Node Type': 'Gather', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Startup Cost': 1173.02, 'Total Cost': 60095.54, 'Plan Rows': 14988, 'Plan Width': 106, 'Actual Startup Time': 1.611, 'Actual Total Time': 8.006, 'Actual Rows': 14373, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Nested Loop', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 173.02, 'Total Cost': 57596.74, 'Plan Rows': 6245, 'Plan Width': 106, 'Actual Startup Time': 0.739, 'Actual Total Time': 70.797, 'Actual Rows': 4791, 'Actual Loops': 3, 'Inner Unique': True, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Heap Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 172.59, 'Total Cost': 22567.1, 'Plan Rows': 6245, 'Plan Width': 12, 'Actual Startup Time': 0.468, 'Actual Total Time': 16.425, 'Actual Rows': 4791, 'Actual Loops': 3, 'Recheck Cond': '(keyword_id < 3)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 14, 'Lossy Heap Blocks': 0, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 168.84, 'Plan Rows': 14988, 'Plan Width': 0, 'Actual Startup Time': 0.818, 'Actual Total Time': 0.818, 'Actual Rows': 14373, 'Actual Loops': 1, 'Index Cond': '(keyword_id < 3)', 'Workers': []}]}, {'Node Type': 'Index Scan', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'title_pkey', 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.43, 'Total Cost': 5.61, 'Plan Rows': 1, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 0.011, 'Actual Rows': 1, 'Actual Loops': 14373, 'Index Cond': '(id = mk.movie_id)', 'Rows Removed by Index Recheck': 0, 'Workers': []}]}]}, {'Node Type': 'Index Scan', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'movie_id_cast_info', 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.44, 'Total Cost': 1.6, 'Plan Rows': 40, 'Plan Width': 42, 'Actual Startup Time': 0.008, 'Actual Total Time': 0.151, 'Actual Rows': 29, 'Actual Loops': 14373, 'Index Cond': '(movie_id = t.id)', 'Rows Removed by Index Recheck': 0}]}, {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 172.59, 'Total Cost': 22676.39, 'Plan Rows': 14988, 'Plan Width': 12, 'Actual Startup Time': 1.113, 'Actual Total Time': 27.646, 'Actual Rows': 14373, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id < 3)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 5753, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 168.84, 'Plan Rows': 14988, 'Plan Width': 0, 'Actual Startup Time': 0.625, 'Actual Total Time': 0.625, 'Actual Rows': 14373, 'Actual Loops': 1, 'Index Cond': '(keyword_id < 3)'}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 38746.77, 'Total Cost': 56686.84, 'Plan Rows': 3645, 'Plan Width': 131, 'Actual Startup Time': 84.347, 'Actual Total Time': 133.102, 'Actual Rows': 2446, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Nested Loop', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 37746.77, 'Total Cost': 55322.34, 'Plan Rows': 1519, 'Plan Width': 131, 'Actual Startup Time': 64.344, 'Actual Total Time': 106.394, 'Actual Rows': 815, 'Actual Loops': 3, 'Inner Unique': False, 'Join Filter': '(t.id = mk.movie_id)', 'Rows Removed by Join Filter': 0, 'Workers': [], 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 37746.34, 'Total Cost': 53371.5, 'Plan Rows': 890, 'Plan Width': 119, 'Actual Startup Time': 63.594, 'Actual Total Time': 100.854, 'Actual Rows': 446, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(mi_idx.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Startup Cost': 0.0, 'Total Cost': 15122.68, 'Plan Rows': 191422, 'Plan Width': 25, 'Actual Startup Time': 0.459, 'Actual Total Time': 47.176, 'Actual Rows': 153308, 'Actual Loops': 3, 'Filter': '(info_type_id < 100)', 'Rows Removed by Filter': 306703, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 37685.08, 'Total Cost': 37685.08, 'Plan Rows': 4901, 'Plan Width': 94, 'Actual Startup Time': 37.723, 'Actual Total Time': 37.724, 'Actual Rows': 2409, 'Actual Loops': 3, 'Hash Buckets': 16384, 'Original Hash Buckets': 16384, 'Hash Batches': 1, 'Original Hash Batches': 1, 'Peak Memory Usage': 992, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Heap Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 1074.48, 'Total Cost': 37685.08, 'Plan Rows': 4901, 'Plan Width': 94, 'Actual Startup Time': 1.614, 'Actual Total Time': 37.151, 'Actual Rows': 2409, 'Actual Loops': 3, 'Recheck Cond': '(kind_id = 3)', 'Rows Removed by Index Recheck': 0, 'Filter': '(production_year < 1966)', 'Rows Removed by Filter': 31103, 'Exact Heap Blocks': 8391, 'Lossy Heap Blocks': 0, 'Workers': [], 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'kind_id_title', 'Startup Cost': 0.0, 'Total Cost': 1071.54, 'Plan Rows': 98015, 'Plan Width': 0, 'Actual Startup Time': 3.252, 'Actual Total Time': 3.252, 'Actual Rows': 100537, 'Actual Loops': 1, 'Index Cond': '(kind_id = 3)', 'Workers': []}]}]}]}, {'Node Type': 'Index Scan', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'movie_id_movie_keyword', 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 0.43, 'Total Cost': 1.63, 'Plan Rows': 45, 'Plan Width': 12, 'Actual Startup Time': 0.011, 'Actual Total Time': 0.012, 'Actual Rows': 2, 'Actual Loops': 1339, 'Index Cond': '(movie_id = mi_idx.movie_id)', 'Rows Removed by Index Recheck': 0, 'Filter': '(keyword_id > 137)', 'Rows Removed by Filter': 0, 'Workers': []}]}]}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Startup Cost': 57115.97, 'Total Cost': 1314433.34, 'Plan Rows': 5464817, 'Plan Width': 136, 'Actual Startup Time': 3070.175, 'Actual Total Time': 5175.483, 'Actual Rows': 3198888, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False, 'Plans': [{'Node Type': 'Hash Join', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 56115.97, 'Total Cost': 766951.64, 'Plan Rows': 2277007, 'Plan Width': 136, 'Actual Startup Time': 3054.063, 'Actual Total Time': 4664.523, 'Actual Rows': 1066296, 'Actual Loops': 3, 'Inner Unique': True, 'Hash Cond': '(ci.movie_id = t.id)', 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'cast_info', 'Alias': 'ci', 'Startup Cost': 0.0, 'Total Cost': 403604.7, 'Plan Rows': 15091770, 'Plan Width': 42, 'Actual Startup Time': 0.318, 'Actual Total Time': 1033.507, 'Actual Rows': 12081448, 'Actual Loops': 3, 'Workers': []}, {'Node Type': 'Hash', 'Parent Relationship': 'Inner', 'Parallel Aware': True, 'Startup Cost': 51800.15, 'Total Cost': 51800.15, 'Plan Rows': 158946, 'Plan Width': 94, 'Actual Startup Time': 182.493, 'Actual Total Time': 182.494, 'Actual Rows': 89350, 'Actual Loops': 3, 'Hash Buckets': 32768, 'Original Hash Buckets': 32768, 'Hash Batches': 16, 'Original Hash Batches': 16, 'Peak Memory Usage': 2336, 'Workers': [], 'Plans': [{'Node Type': 'Seq Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': True, 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.0, 'Total Cost': 51800.15, 'Plan Rows': 158946, 'Plan Width': 94, 'Actual Startup Time': 0.152, 'Actual Total Time': 127.778, 'Actual Rows': 89350, 'Actual Loops': 3, 'Filter': '((kind_id > 4) AND (production_year < 1987))', 'Rows Removed by Filter': 753421, 'Workers': []}]}]}]}, {'Node Type': 'Merge Join', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 3.26, 'Total Cost': 168707.46, 'Plan Rows': 885515, 'Plan Width': 119, 'Actual Startup Time': 0.047, 'Actual Total Time': 1630.919, 'Actual Rows': 913298, 'Actual Loops': 1, 'Inner Unique': False, 'Merge Cond': '(t.id = mi_idx.movie_id)', 'Plans': [{'Node Type': 'Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'title_pkey', 'Relation Name': 'title', 'Alias': 't', 'Startup Cost': 0.43, 'Total Cost': 108393.83, 'Plan Rows': 2431932, 'Plan Width': 94, 'Actual Startup Time': 0.025, 'Actual Total Time': 1109.292, 'Actual Rows': 2425943, 'Actual Loops': 1, 'Filter': '(production_year > 1910)', 'Rows Removed by Filter': 99851}, {'Node Type': 'Index Scan', 'Parent Relationship': 'Inner', 'Parallel Aware': False, 'Scan Direction': 'Forward', 'Index Name': 'movie_id_movie_info_idx', 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Startup Cost': 0.43, 'Total Cost': 43188.14, 'Plan Rows': 920621, 'Plan Width': 25, 'Actual Startup Time': 0.012, 'Actual Total Time': 234.623, 'Actual Rows': 920110, 'Actual Loops': 1, 'Filter': '(info_type_id > 99)', 'Rows Removed by Filter': 459925}]}]\n",
      "execution_times:\n",
      " [2290.531, 28.036, 133.249, 5255.643, 1652.545]\n",
      "{'Node Type': 'Nested Loop', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 1173.46, 'Total Cost': 90035.89, 'Plan Rows': 214713, 'Plan Width': 148, 'Actual Startup Time': 1.621, 'Actual Total Time': 2274.459, 'Actual Rows': 419818, 'Actual Loops': 1, 'Inner Unique': False}\n",
      "{'Node Type': 'Gather', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Startup Cost': 1173.02, 'Total Cost': 60095.54, 'Plan Rows': 14988, 'Plan Width': 106, 'Actual Startup Time': 1.611, 'Actual Total Time': 8.006, 'Actual Rows': 14373, 'Actual Loops': 1, 'Workers Planned': 2, 'Workers Launched': 2, 'Single Copy': False}\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (1, 2), (3, 3), (2, 3), (4, 4), (3, 4), (5, 5), (2, 5), (6, 6), (0, 6)]\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (1, 2), (3, 3), (1, 3), (4, 4), (3, 4), (5, 5), (4, 5), (6, 6), (4, 6), (7, 7), (6, 7)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plan_id</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>Node Type</th>\n",
       "      <th>Parallel Aware</th>\n",
       "      <th>Join Type</th>\n",
       "      <th>Startup Cost</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Plan Rows</th>\n",
       "      <th>Plan Width</th>\n",
       "      <th>Actual Startup Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Hash Buckets</th>\n",
       "      <th>Original Hash Buckets</th>\n",
       "      <th>Hash Batches</th>\n",
       "      <th>Original Hash Batches</th>\n",
       "      <th>Peak Memory Usage</th>\n",
       "      <th>Merge Cond</th>\n",
       "      <th>Sort Key</th>\n",
       "      <th>Sort Method</th>\n",
       "      <th>Sort Space Used</th>\n",
       "      <th>Sort Space Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nested Loop</td>\n",
       "      <td>False</td>\n",
       "      <td>Inner</td>\n",
       "      <td>1173.46</td>\n",
       "      <td>90035.89</td>\n",
       "      <td>214713</td>\n",
       "      <td>148</td>\n",
       "      <td>1.621</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gather</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1173.02</td>\n",
       "      <td>60095.54</td>\n",
       "      <td>14988</td>\n",
       "      <td>106</td>\n",
       "      <td>1.611</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Nested Loop</td>\n",
       "      <td>False</td>\n",
       "      <td>Inner</td>\n",
       "      <td>173.02</td>\n",
       "      <td>57596.74</td>\n",
       "      <td>6245</td>\n",
       "      <td>106</td>\n",
       "      <td>0.739</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bitmap Heap Scan</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>172.59</td>\n",
       "      <td>22567.10</td>\n",
       "      <td>6245</td>\n",
       "      <td>12</td>\n",
       "      <td>0.468</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Bitmap Index Scan</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>168.84</td>\n",
       "      <td>14988</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   plan_id  node_idx          Node Type  Parallel Aware Join Type  \\\n",
       "0        0         0        Nested Loop           False     Inner   \n",
       "1        0         1             Gather           False       NaN   \n",
       "2        0         2        Nested Loop           False     Inner   \n",
       "3        0         3   Bitmap Heap Scan            True       NaN   \n",
       "4        0         4  Bitmap Index Scan           False       NaN   \n",
       "\n",
       "   Startup Cost  Total Cost  Plan Rows  Plan Width  Actual Startup Time  ...  \\\n",
       "0       1173.46    90035.89     214713         148                1.621  ...   \n",
       "1       1173.02    60095.54      14988         106                1.611  ...   \n",
       "2        173.02    57596.74       6245         106                0.739  ...   \n",
       "3        172.59    22567.10       6245          12                0.468  ...   \n",
       "4          0.00      168.84      14988           0                0.818  ...   \n",
       "\n",
       "   Hash Buckets  Original Hash Buckets  Hash Batches Original Hash Batches  \\\n",
       "0           NaN                    NaN           NaN                   NaN   \n",
       "1           NaN                    NaN           NaN                   NaN   \n",
       "2           NaN                    NaN           NaN                   NaN   \n",
       "3           NaN                    NaN           NaN                   NaN   \n",
       "4           NaN                    NaN           NaN                   NaN   \n",
       "\n",
       "  Peak Memory Usage  Merge Cond  Sort Key Sort Method Sort Space Used  \\\n",
       "0               NaN         NaN       NaN         NaN             NaN   \n",
       "1               NaN         NaN       NaN         NaN             NaN   \n",
       "2               NaN         NaN       NaN         NaN             NaN   \n",
       "3               NaN         NaN       NaN         NaN             NaN   \n",
       "4               NaN         NaN       NaN         NaN             NaN   \n",
       "\n",
       "  Sort Space Type  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.数据处理\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "from models.DataPreprocessor import get_plans_dict, DataPreprocessor, plan_trees_to_graphs, graphs_to_df, df_to_graphs, DFStatisticsInfo\n",
    "\n",
    "json_path = \"../data/train_plan_*.csv\"\n",
    "plans_dict, execution_times = get_plans_dict(json_path)\n",
    "print(\"plans_dict:\\n\", plans_dict[0:5])\n",
    "print(\"execution_times:\\n\", execution_times[0:5])\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)\n",
    "\n",
    "edges_list, matrix_plans = plan_trees_to_graphs(plans_tree, add_self_loops=True, undirected=False)\n",
    "print(matrix_plans[0][0])\n",
    "print(matrix_plans[0][1])\n",
    "print(edges_list[0])\n",
    "print(edges_list[99])\n",
    "\n",
    "plans_df = graphs_to_df(matrix_plans)\n",
    "plans_df.to_csv(\"../data/process/01_plans_df.csv\", index=False)\n",
    "plans_df.head()\n",
    "\n",
    "# stats = DFStatisticsInfo(plans_df, sample_threshold=200, sample_k=10, strict_alias_check=True)\n",
    "# node_types = stats.get_node_type_set()\n",
    "# global_must = stats.global_must_keys()\n",
    "# global_all  = stats.global_all_keys()\n",
    "# per_key     = stats.per_key_values()\n",
    "# per_type    = stats.per_nodetype_key_stats()\n",
    "# issues      = stats.report_issues()\n",
    "# stats.pretty_print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6199d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name   min      max  cardinality  num_unique_values  \\\n",
      "0               t.id     1  2528312      2528312            2528312   \n",
      "1          t.kind_id     1        7      2528312                  6   \n",
      "2  t.production_year  1880     2019      2528312                133   \n",
      "3              mc.id     1  2609129      2609129            2609129   \n",
      "4      mc.company_id     1   234997      2609129             234997   \n",
      "\n",
      "  table_name      column_name  \n",
      "0          t               id  \n",
      "1          t          kind_id  \n",
      "2          t  production_year  \n",
      "3         mc               id  \n",
      "4         mc       company_id  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10392/40245514.py:47: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tmp = df[split_cols].applymap(to_list)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'无法解析 lhs 列名（需要别名）：mi_idx.movie_id，行别名=nan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 62\u001b[0m\n\u001b[1;32m     56\u001b[0m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicatge_list\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicatge_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m v: v \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 逐行调用，得到“处理后的谓词列表”\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# plans_df[\"predicatge_list_processed\"] = plans_df[\"predicatge_list\"].apply(\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     lambda lst: process_predicate_list(lst, db_info, plans_df[\"Alias\"])\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicate_list_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mplans_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_predicate_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicatge_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdb_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 1：将nodetype做一个新列名叫node_type_id，\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 2：将Plan Rows 和Plan Width也做一个新列plan_rows, plan_width都是做完log1p的\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/pandas/core/frame.py:10381\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10367\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10369\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10371\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10379\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10380\u001b[0m )\n\u001b[0;32m> 10381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 63\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     56\u001b[0m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicatge_list\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicatge_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m v: v \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 逐行调用，得到“处理后的谓词列表”\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# plans_df[\"predicatge_list_processed\"] = plans_df[\"predicatge_list\"].apply(\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     lambda lst: process_predicate_list(lst, db_info, plans_df[\"Alias\"])\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     62\u001b[0m plans_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicate_list_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m plans_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mprocess_predicate_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicatge_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdb_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     68\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 1：将nodetype做一个新列名叫node_type_id，\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 2：将Plan Rows 和Plan Width也做一个新列plan_rows, plan_width都是做完log1p的\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/Other/GNTO/models/DataPreprocessor.py:592\u001b[0m, in \u001b[0;36mprocess_predicate_list\u001b[0;34m(predicate_list, db_info, default_alias, op_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m lhs_id \u001b[38;5;241m=\u001b[39m _resolve_col(lhs, default_alias, full2id, col_to_fulls)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m无法解析 lhs 列名（需要别名）：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlhs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m，行别名=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefault_alias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# 尝试把 rhs 当数字\u001b[39;00m\n\u001b[1;32m    595\u001b[0m rhs_num \u001b[38;5;241m=\u001b[39m _to_num(rhs)\n",
      "\u001b[0;31mKeyError\u001b[0m: '无法解析 lhs 列名（需要别名）：mi_idx.movie_id，行别名=nan'"
     ]
    }
   ],
   "source": [
    "from this import d\n",
    "from models.DataPreprocessor import safe_cond_parse, process_predicate_list\n",
    "import pandas as pd\n",
    "\n",
    "# schema信息\n",
    "db_info = pd.read_csv(\"../data/column_min_max_no_idx.csv\")\n",
    "db_info[\"table_name\"], db_info[\"column_name\"] = db_info[\"name\"].str.split(\".\").str[0], db_info[\"name\"].str.split(\".\").str[1]\n",
    "print(db_info.head())\n",
    "print(\"-\"*100)\n",
    "\n",
    "# 需要解析的条件列表\n",
    "NEED_PARSE_COND_SCAN = [\n",
    "    \"Filter\",\n",
    "    \"Index Cond\",\n",
    "    \"Recheck Cond\"\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_JOIN = [\n",
    "    \"Hash Cond\",\n",
    "    \"Join Filter\",\n",
    "    \"Merge Cond\",\n",
    "]\n",
    "\n",
    "NEED_PARSE_COND_COLS = NEED_PARSE_COND_SCAN + NEED_PARSE_COND_JOIN\n",
    "\n",
    "for col in NEED_PARSE_COND_COLS:\n",
    "    plans_df[f\"{col}_Split\"] = plans_df[col].apply(safe_cond_parse)\n",
    "\n",
    "# 将需要解析的条件列表统一放到predicatge_list\n",
    "def merge_split_predicates(df: pd.DataFrame,\n",
    "                           suffix: str = \"_Split\",\n",
    "                           new_col: str = \"predicatge_list\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    split_cols = [c for c in df.columns if c.endswith(suffix)]\n",
    "    if not split_cols:\n",
    "        df[new_col] = [[] for _ in range(len(df))]\n",
    "        return df\n",
    "\n",
    "    # 保证每个单元格都是 list（NaN/None/标量 → []）\n",
    "    def to_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, tuple):\n",
    "            return list(x)\n",
    "        return []\n",
    "\n",
    "    tmp = df[split_cols].applymap(to_list)\n",
    "\n",
    "    # 行内拼接多个 *_Split 列（展平）\n",
    "    df[new_col] = tmp.apply(lambda row: [item for sub in row for item in sub], axis=1)\n",
    "    return df\n",
    "plans_df = merge_split_predicates(plans_df)\n",
    "\n",
    "# 将predicatge_list [lhs, op, rhs] 里面的col变为int， op变为int， value变为int\n",
    "# 确保空值是空列表，方便按行处理\n",
    "plans_df[\"predicatge_list\"] = plans_df[\"predicatge_list\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "\n",
    "# 逐行调用，得到“处理后的谓词列表”\n",
    "# plans_df[\"predicatge_list_processed\"] = plans_df[\"predicatge_list\"].apply(\n",
    "#     lambda lst: process_predicate_list(lst, db_info, plans_df[\"Alias\"])\n",
    "# )\n",
    "plans_df[\"predicate_list_processed\"] = plans_df.apply(\n",
    "    lambda row: process_predicate_list(\n",
    "        predicate_list=row[\"predicatge_list\"],\n",
    "        db_info=db_info,\n",
    "        default_alias=row.get(\"Alias\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 1：将nodetype做一个新列名叫node_type_id，\n",
    "# 2：将Plan Rows 和Plan Width也做一个新列plan_rows, plan_width都是做完log1p的\n",
    "import numpy as np\n",
    "node_types = plans_df[\"Node Type\"].unique()\n",
    "node_types_dict = {node_type: i for i, node_type in enumerate(node_types)}\n",
    "plans_df[\"node_type_id\"] = plans_df[\"Node Type\"].map(node_types_dict)\n",
    "plans_df[\"plan_rows\"]  = np.log1p(pd.to_numeric(plans_df[\"Plan Rows\"],  errors=\"coerce\").fillna(0.0)).astype(\"float32\")\n",
    "plans_df[\"plan_width\"] = np.log1p(pd.to_numeric(plans_df[\"Plan Width\"], errors=\"coerce\").fillna(0.0)).astype(\"float32\")\n",
    "\n",
    "print(plans_df.head())\n",
    "print(\"-\"*100)\n",
    "graphs = df_to_graphs(plans_df)\n",
    "print(graphs[0])\n",
    "print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece64fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 0 行存在需要别名但缺失的裸列：\n"
     ]
    }
   ],
   "source": [
    "# 缺失判断\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 为了打印完整列\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# 统计 db_info 里每个裸列名出现的表数，方便判断是否“多表同名”\n",
    "db_info[\"__bare__\"] = db_info[\"name\"].astype(str).str.split(\".\").str[-1]\n",
    "bare_counts = db_info[\"__bare__\"].value_counts().to_dict()\n",
    "\n",
    "def row_has_ambiguous_bare_predicates(row, suffix=\"_Split\"):\n",
    "    \"\"\"该行是否存在需要 alias 的裸列名（且该裸列在多表出现）\"\"\"\n",
    "    # 收集该行所有 *_Split 的谓词\n",
    "    preds = []\n",
    "    for c in row.index:\n",
    "        if str(c).endswith(suffix):\n",
    "            v = row[c]\n",
    "            if isinstance(v, list):\n",
    "                preds.extend(v)\n",
    "    # 检查每个三元组 [lhs, op, rhs]\n",
    "    ambiguous = []\n",
    "    for trip in preds:\n",
    "        if not isinstance(trip, (list, tuple)) or len(trip) < 3:\n",
    "            continue\n",
    "        lhs, op, rhs = str(trip[0]), str(trip[1]), trip[2]\n",
    "        # 裸列名且在多表重复（需要 alias）\n",
    "        if \".\" not in lhs and bare_counts.get(lhs, 0) > 1 and not pd.notna(row.get(\"Alias\")):\n",
    "            ambiguous.append((\"lhs\", trip))\n",
    "        # rhs 如果是字符串且无点，也可能是列名\n",
    "        if isinstance(rhs, str) and \".\" not in rhs and bare_counts.get(rhs, 0) > 1 and not pd.notna(row.get(\"Alias\")):\n",
    "            ambiguous.append((\"rhs\", trip))\n",
    "    return ambiguous\n",
    "\n",
    "# 找到所有有歧义的行并打印\n",
    "bad_rows = []\n",
    "for idx, row in plans_df.iterrows():\n",
    "    amb = row_has_ambiguous_bare_predicates(row)\n",
    "    if amb:\n",
    "        bad_rows.append((idx, amb))\n",
    "\n",
    "print(f\"发现 {len(bad_rows)} 行存在需要别名但缺失的裸列：\")\n",
    "for idx, amb in bad_rows[:10]:\n",
    "    print(f\"\\n--- 行索引: {idx}  Alias={plans_df.at[idx, 'Alias']!r}\")\n",
    "    print(\"问题谓词：\", amb)                 # 列出触发问题的三元组\n",
    "    print(\"整行数据：\")\n",
    "    print(plans_df.loc[idx])               # 打印该行完整数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52da685",
   "metadata": {},
   "source": [
    "# 2. 数据准备\n",
    "TrainAndEval.py\n",
    "\n",
    "## 该模块主要内容\n",
    "1. 将数据划分为训练集,验证集,测试集\n",
    "2. 构建数据集\n",
    "3. 创建数据加载器\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af54b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 100000\n",
      "[{'Node Type': 'Nested Loop', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 1173.46, 'Total Cost': 90035.89, 'Plan Rows': 214713, 'Plan Width': 148, 'Actual Startup Time': 1.621, 'Actual Total Time': 2274.459, 'Actual Rows': 419818, 'Actual Loops': 1, 'Inner Unique': False, 'Parent Relationship': nan, 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': nan, 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Gather', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 1173.02, 'Total Cost': 60095.54, 'Plan Rows': 14988, 'Plan Width': 106, 'Actual Startup Time': 1.611, 'Actual Total Time': 8.006, 'Actual Rows': 14373, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': 2.0, 'Workers Launched': 2.0, 'Single Copy': False, 'Workers': nan, 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Nested Loop', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 173.02, 'Total Cost': 57596.74, 'Plan Rows': 6245, 'Plan Width': 106, 'Actual Startup Time': 0.739, 'Actual Total Time': 70.797, 'Actual Rows': 4791, 'Actual Loops': 3, 'Inner Unique': True, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': True, 'Join Type': nan, 'Startup Cost': 172.59, 'Total Cost': 22567.1, 'Plan Rows': 6245, 'Plan Width': 12, 'Actual Startup Time': 0.468, 'Actual Total Time': 16.425, 'Actual Rows': 4791, 'Actual Loops': 3, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Recheck Cond': '(keyword_id < 3)', 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': 14.0, 'Lossy Heap Blocks': 0.0, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [['keyword_id', '<', 3]], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.0, 'Total Cost': 168.84, 'Plan Rows': 14988, 'Plan Width': 0, 'Actual Startup Time': 0.818, 'Actual Total Time': 0.818, 'Actual Rows': 14373, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'keyword_id_movie_keyword', 'Index Cond': '(keyword_id < 3)', 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['keyword_id', '<', 3]], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.43, 'Total Cost': 5.61, 'Plan Rows': 1, 'Plan Width': 94, 'Actual Startup Time': 0.011, 'Actual Total Time': 0.011, 'Actual Rows': 1, 'Actual Loops': 14373, 'Inner Unique': nan, 'Parent Relationship': 'Inner', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': 'title', 'Alias': 't', 'Recheck Cond': nan, 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'title_pkey', 'Index Cond': '(id = mk.movie_id)', 'Scan Direction': 'Forward', 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['id', '=', 'mk.movie_id']], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.44, 'Total Cost': 1.6, 'Plan Rows': 40, 'Plan Width': 42, 'Actual Startup Time': 0.008, 'Actual Total Time': 0.151, 'Actual Rows': 29, 'Actual Loops': 14373, 'Inner Unique': nan, 'Parent Relationship': 'Inner', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': nan, 'Relation Name': 'cast_info', 'Alias': 'ci', 'Recheck Cond': nan, 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'movie_id_cast_info', 'Index Cond': '(movie_id = t.id)', 'Scan Direction': 'Forward', 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['movie_id', '=', 't.id']], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}]\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (1, 2), (3, 3), (2, 3), (4, 4), (3, 4), (5, 5), (2, 5), (6, 6), (0, 6)]\n",
      "2290.531\n",
      "[{'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 172.59, 'Total Cost': 22676.39, 'Plan Rows': 14988, 'Plan Width': 12, 'Actual Startup Time': 1.113, 'Actual Total Time': 27.646, 'Actual Rows': 14373, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': nan, 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': nan, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Recheck Cond': '(keyword_id < 3)', 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': 5753.0, 'Lossy Heap Blocks': 0.0, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [['keyword_id', '<', 3]], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.0, 'Total Cost': 168.84, 'Plan Rows': 14988, 'Plan Width': 0, 'Actual Startup Time': 0.625, 'Actual Total Time': 0.625, 'Actual Rows': 14373, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': nan, 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'keyword_id_movie_keyword', 'Index Cond': '(keyword_id < 3)', 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['keyword_id', '<', 3]], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}]\n",
      "[(0, 0), (1, 1), (0, 1)]\n",
      "28.036\n",
      "[{'Node Type': 'Gather', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 38746.77, 'Total Cost': 56686.84, 'Plan Rows': 3645, 'Plan Width': 131, 'Actual Startup Time': 84.347, 'Actual Total Time': 133.102, 'Actual Rows': 2446, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': nan, 'Workers Planned': 2.0, 'Workers Launched': 2.0, 'Single Copy': False, 'Workers': nan, 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Nested Loop', 'Parallel Aware': False, 'Join Type': 'Inner', 'Startup Cost': 37746.77, 'Total Cost': 55322.34, 'Plan Rows': 1519, 'Plan Width': 131, 'Actual Startup Time': 64.344, 'Actual Total Time': 106.394, 'Actual Rows': 815, 'Actual Loops': 3, 'Inner Unique': False, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': '(t.id = mk.movie_id)', 'Rows Removed by Join Filter': 0.0, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [['t.id', '=', 'mk.movie_id']], 'Merge Cond_Split': []}, {'Node Type': 'Hash Join', 'Parallel Aware': True, 'Join Type': 'Inner', 'Startup Cost': 37746.34, 'Total Cost': 53371.5, 'Plan Rows': 890, 'Plan Width': 119, 'Actual Startup Time': 63.594, 'Actual Total Time': 100.854, 'Actual Rows': 446, 'Actual Loops': 3, 'Inner Unique': True, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': '(mi_idx.movie_id = t.id)', 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [['mi_idx.movie_id', '=', 't.id']], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Seq Scan', 'Parallel Aware': True, 'Join Type': nan, 'Startup Cost': 0.0, 'Total Cost': 15122.68, 'Plan Rows': 191422, 'Plan Width': 25, 'Actual Startup Time': 0.459, 'Actual Total Time': 47.176, 'Actual Rows': 153308, 'Actual Loops': 3, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': 'movie_info_idx', 'Alias': 'mi_idx', 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': '(info_type_id < 100)', 'Rows Removed by Filter': 306703.0, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [['info_type_id', '<', 100]], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Hash', 'Parallel Aware': True, 'Join Type': nan, 'Startup Cost': 37685.08, 'Total Cost': 37685.08, 'Plan Rows': 4901, 'Plan Width': 94, 'Actual Startup Time': 37.723, 'Actual Total Time': 37.724, 'Actual Rows': 2409, 'Actual Loops': 3, 'Inner Unique': nan, 'Parent Relationship': 'Inner', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': 16384.0, 'Original Hash Buckets': 16384.0, 'Hash Batches': 1.0, 'Original Hash Batches': 1.0, 'Peak Memory Usage': 992.0, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': True, 'Join Type': nan, 'Startup Cost': 1074.48, 'Total Cost': 37685.08, 'Plan Rows': 4901, 'Plan Width': 94, 'Actual Startup Time': 1.614, 'Actual Total Time': 37.151, 'Actual Rows': 2409, 'Actual Loops': 3, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': 'title', 'Alias': 't', 'Recheck Cond': '(kind_id = 3)', 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': 8391.0, 'Lossy Heap Blocks': 0.0, 'Index Name': nan, 'Index Cond': nan, 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': '(production_year < 1966)', 'Rows Removed by Filter': 31103.0, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [['production_year', '<', 1966]], 'Index Cond_Split': [], 'Recheck Cond_Split': [['kind_id', '=', 3]], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Bitmap Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.0, 'Total Cost': 1071.54, 'Plan Rows': 98015, 'Plan Width': 0, 'Actual Startup Time': 3.252, 'Actual Total Time': 3.252, 'Actual Rows': 100537, 'Actual Loops': 1, 'Inner Unique': nan, 'Parent Relationship': 'Outer', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': nan, 'Alias': nan, 'Recheck Cond': nan, 'Rows Removed by Index Recheck': nan, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'kind_id_title', 'Index Cond': '(kind_id = 3)', 'Scan Direction': nan, 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': nan, 'Rows Removed by Filter': nan, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [], 'Index Cond_Split': [['kind_id', '=', 3]], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}, {'Node Type': 'Index Scan', 'Parallel Aware': False, 'Join Type': nan, 'Startup Cost': 0.43, 'Total Cost': 1.63, 'Plan Rows': 45, 'Plan Width': 12, 'Actual Startup Time': 0.011, 'Actual Total Time': 0.012, 'Actual Rows': 2, 'Actual Loops': 1339, 'Inner Unique': nan, 'Parent Relationship': 'Inner', 'Workers Planned': nan, 'Workers Launched': nan, 'Single Copy': nan, 'Workers': [], 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Recheck Cond': nan, 'Rows Removed by Index Recheck': 0.0, 'Exact Heap Blocks': nan, 'Lossy Heap Blocks': nan, 'Index Name': 'movie_id_movie_keyword', 'Index Cond': '(movie_id = mi_idx.movie_id)', 'Scan Direction': 'Forward', 'Join Filter': nan, 'Rows Removed by Join Filter': nan, 'Hash Cond': nan, 'Filter': '(keyword_id > 137)', 'Rows Removed by Filter': 0.0, 'Hash Buckets': nan, 'Original Hash Buckets': nan, 'Hash Batches': nan, 'Original Hash Batches': nan, 'Peak Memory Usage': nan, 'Merge Cond': nan, 'Sort Key': nan, 'Sort Method': nan, 'Sort Space Used': nan, 'Sort Space Type': nan, 'Filter_Split': [['keyword_id', '>', 137]], 'Index Cond_Split': [['movie_id', '=', 'mi_idx.movie_id']], 'Recheck Cond_Split': [], 'Hash Cond_Split': [], 'Join Filter_Split': [], 'Merge Cond_Split': []}]\n",
      "[(0, 0), (1, 1), (0, 1), (2, 2), (1, 2), (3, 3), (2, 3), (4, 4), (2, 4), (5, 5), (4, 5), (6, 6), (5, 6), (7, 7), (1, 7)]\n",
      "133.249\n",
      "训练集: 70000, 验证集: 15000, 测试集: 15000\n",
      "训练批次数: 2188, 验证批次数: 469\n"
     ]
    }
   ],
   "source": [
    "# 5. 数据准备\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.TrainAndEval import build_dataset\n",
    "\n",
    "# 构建数据集\n",
    "dataset = build_dataset(graphs, edges_list, execution_times)\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "for i in range(3):\n",
    "    print(dataset[i].x)\n",
    "    print(dataset[i].edge_index)\n",
    "    print(dataset[i].y)\n",
    "    # print(f\"样本: x.shape={dataset[i].x.shape}, edge_index.shape={dataset[i].edge_index.shape}, y={dataset[i].y}\")\n",
    "    \n",
    "# 数据集划分\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    range(len(dataset)), test_size=0.3, random_state=42\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = [dataset[i] for i in train_indices]\n",
    "val_dataset = [dataset[i] for i in val_indices]\n",
    "test_dataset = [dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"训练集: {len(train_dataset)}, 验证集: {len(val_dataset)}, 测试集: {len(test_dataset)}\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}, 验证批次数: {len(val_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf30b9",
   "metadata": {},
   "source": [
    "# 3. 模型模块\n",
    "\n",
    "## NodeEncoder\n",
    "NodeEncoder.py\n",
    "主要包括了各种vectorical的编码方式.\n",
    "\n",
    "1. 转为为Matrix(Node, Edge)\n",
    "\n",
    "\n",
    "## TreeEncoder\n",
    "TreeEncoder.py\n",
    "目前包括两种模型一个是GAT,一个是传统GNN.\n",
    "1. 转为vector\n",
    "\n",
    "## PredictionHead\n",
    "PredictionHead.py\n",
    "目前进行最简单的FNN进行后续回归任务\n",
    "\n",
    "1. 预测\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CostModel(\n",
      "  (nodecoder): NodeEncoder_V0(\n",
      "    (node_type_emb): Embedding(13, 16)\n",
      "    (predicate_encoder): PredicateEncoder(\n",
      "      (op_emb): Embedding(6, 3)\n",
      "      (col_emb): Embedding(16, 8)\n",
      "    )\n",
      "  )\n",
      "  (treeencoder): TreeEncoder_GATMini(\n",
      "    (gat1): GATConv(32, 64, heads=8)\n",
      "    (gat2): GATConv(512, 64, heads=1)\n",
      "  )\n",
      "  (predict_head): PredictionHead_FNNMini(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from models.NodeEncoder import *\n",
    "from models.TreeEncoder import *\n",
    "from models.PredictionHead import *\n",
    "from models.TrainAndEval import EarlyStopping\n",
    "\n",
    "class CostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        data 结构\n",
    "        - x: [N, F_num]\n",
    "        - edge_index: [2, E]\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        g = self.treeencoder(x, data.edge_index, data.batch)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "f_in, d_node, d_graph, out_dim = len(dataset[0].x), 32, 64, 1\n",
    "nodecoder = NodeEncoder_V0(\n",
    "    in_dim=f_in,\n",
    "    out_dim=d_node\n",
    ")\n",
    "gatTreeEncoder = TreeEncoder_GATMini(\n",
    "    in_dim=d_node,\n",
    "    hidden_dim=64,\n",
    "    out_dim=d_graph\n",
    ")\n",
    "predict_head = PredictionHead_FNNMini(\n",
    "    in_dim=d_graph,\n",
    "    out_dim=out_dim\n",
    ")\n",
    "\n",
    "# 模型构建\n",
    "model = CostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "print(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023b51d",
   "metadata": {},
   "source": [
    "# 4 训练&评估\n",
    "## 训练\n",
    "主要模块为划分训练集,测试集,验证集.\n",
    "调用模型进行训练.\n",
    "## 评估\n",
    "主要为评估方式.目前为MSE以及Q-error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3725e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaosn/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'x', 'y'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, criterion, early_stopping, device, weight_path, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[1;32m     23\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n",
      "File \u001b[0;32m~/Project/Other/GNTO/models/TrainAndEval.py:89\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, batch\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mCostModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Data \u001b[38;5;241m|\u001b[39m Batch):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    data 结构\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    - x: [N, F_num]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    - edge_index: [2, E]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                                   \u001b[38;5;66;03m# [N, d_node]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtreeencoder(x, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)         \u001b[38;5;66;03m# [B, d_graph]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_head(g)                                     \u001b[38;5;66;03m# [B, out_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Project/Other/GNTO/models/NodeEncoder.py:54\u001b[0m, in \u001b[0;36mNodeEncoder_Mini.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Tools/miniconda3/envs/ai4db/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from models.TrainAndEval import train_epoch, validate_epoch\n",
    "import time\n",
    "\n",
    "#  8. 训练循环\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "                early_stopping, device, weight_path, num_epochs=100):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"开始训练...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 计算时间\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # 打印进度\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\n早停触发在第 {epoch+1} 轮\")\n",
    "            break\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            date = datetime.now().strftime(\"%m%d\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, f'../results/weight_{date}.pth')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"训练完成!\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 开始训练\n",
    "date = datetime.now().strftime(\"%m%d\")\n",
    "weight_path = f'../results/{date}.pth'\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, \n",
    "    criterion, early_stopping, device, weight_path, num_epochs=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc95b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 绘制结果\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m plot_training_history(train_losses, val_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. 可视化训练过程和结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 预测 vs 真实值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(targets, predictions, alpha=0.5, s=20)\n",
    "    min_val = min(targets.min(), predictions.min())\n",
    "    max_val = max(targets.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.xlabel('True Execution Time')\n",
    "    plt.ylabel('Predicted Execution Time')\n",
    "    plt.title('Predicted vs True Execution Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    date = datetime.now().strftime(\"%m%d\")\n",
    "    plt.savefig(f'../results/training_results_{date}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "# 绘制结果\n",
    "plot_training_history(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1f73a",
   "metadata": {},
   "source": [
    "$$\n",
    "x = [\n",
    "\\underbrace{x_1, ..., x_6,\\ }_{\\text{categorical}}\n",
    "\\underbrace{x_7, x_8,\\ }_{\\text{numeric}}\n",
    "\\underbrace{x_9, ..., x_{16},\\ }_{\\text{categorical}}\n",
    "\\underbrace{x_{17}, ..., x_{32}}_{\\text{conditional}}\n",
    "]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{x} = [\n",
    "\\underbrace{\\tilde{x}_1, ..., \\tilde{x}_6,\\ }_{\\text{NodeType}}\n",
    "\\underbrace{\\tilde{x}_7, \\tilde{x}_8,\\ }_{\\text{Size}}\n",
    "\\underbrace{\\tilde{x}_9, ..., \\tilde{x}_{16},\\ }_{\\text{Table}}\n",
    "\\underbrace{\\tilde{x}_{17}, ..., \\tilde{x}_{32}}_{\\text{Predicate}}\n",
    "]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9987c28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
