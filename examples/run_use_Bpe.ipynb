{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4074c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的文件: ['../data/train_plan_part17.csv', '../data/train_plan_part8.csv', '../data/train_plan_part6.csv', '../data/train_plan_part3.csv', '../data/train_plan_part19.csv', '../data/train_plan_part9.csv', '../data/train_plan_part11.csv', '../data/train_plan_part1.csv', '../data/train_plan_part0.csv', '../data/train_plan_part18.csv', '../data/train_plan_part10.csv', '../data/train_plan_part12.csv', '../data/train_plan_part16.csv', '../data/train_plan_part15.csv', '../data/train_plan_part2.csv', '../data/train_plan_part14.csv', '../data/train_plan_part5.csv', '../data/train_plan_part7.csv', '../data/train_plan_part13.csv', '../data/train_plan_part4.csv']\n",
      "总数据行数: 100000\n",
      "df:\n",
      "       id                                               json\n",
      "0  85000  {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Pa...\n",
      "1  85001  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "2  85002  {\"Plan\": {\"Node Type\": \"Hash Join\", \"Parallel ...\n",
      "3  85003  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "4  85004  {\"Plan\": {\"Node Type\": \"Gather\", \"Parallel Awa...\n",
      "plans_json:\n",
      " {\"Plan\": {\"Node Type\": \"Bitmap Heap Scan\", \"Parallel Aware\": false, \"Relation Name\": \"movie_keyword\", \"Alias\": \"mk\", \"Startup Cost\": 11788.77, \"Total Cost\": 49094.94, \"Plan Rows\": 1028173, \"Plan Width\": 12, \"Actual Startup Time\": 41.924, \"Actual Total Time\": 200.35, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Recheck Cond\": \"(keyword_id > 17243)\", \"Rows Removed by Index Recheck\": 0, \"Exact Heap Blocks\": 24037, \"Lossy Heap Blocks\": 0, \"Plans\": [{\"Node Type\": \"Bitmap Index Scan\", \"Parent Relationship\": \"Outer\", \"Parallel Aware\": false, \"Index Name\": \"keyword_id_movie_keyword\", \"Startup Cost\": 0.0, \"Total Cost\": 11531.73, \"Plan Rows\": 1028173, \"Plan Width\": 0, \"Actual Startup Time\": 39.572, \"Actual Total Time\": 39.572, \"Actual Rows\": 1029758, \"Actual Loops\": 1, \"Index Cond\": \"(keyword_id > 17243)\"}]}, \"Planning Time\": 1.679, \"Triggers\": [], \"Execution Time\": 224.454}\n",
      "plans_dict:\n",
      " {'Node Type': 'Bitmap Heap Scan', 'Parallel Aware': False, 'Relation Name': 'movie_keyword', 'Alias': 'mk', 'Startup Cost': 11788.77, 'Total Cost': 49094.94, 'Plan Rows': 1028173, 'Plan Width': 12, 'Actual Startup Time': 41.924, 'Actual Total Time': 200.35, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Recheck Cond': '(keyword_id > 17243)', 'Rows Removed by Index Recheck': 0, 'Exact Heap Blocks': 24037, 'Lossy Heap Blocks': 0, 'Plans': [{'Node Type': 'Bitmap Index Scan', 'Parent Relationship': 'Outer', 'Parallel Aware': False, 'Index Name': 'keyword_id_movie_keyword', 'Startup Cost': 0.0, 'Total Cost': 11531.73, 'Plan Rows': 1028173, 'Plan Width': 0, 'Actual Startup Time': 39.572, 'Actual Total Time': 39.572, 'Actual Rows': 1029758, 'Actual Loops': 1, 'Index Cond': '(keyword_id > 17243)'}]}\n",
      "[Node Types] 13: ['Bitmap Heap Scan', 'Bitmap Index Scan', 'BitmapAnd', 'Gather', 'Gather Merge', 'Hash', 'Hash Join', 'Index Scan', 'Materialize', 'Merge Join', 'Nested Loop', 'Seq Scan', 'Sort']\n",
      "\n",
      "[Global MUST keys] 10: ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "\n",
      "[Global ALL keys] 41: ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Filter', 'Hash Batches', 'Hash Buckets', 'Hash Cond', 'Index Cond', 'Index Name', 'Inner Unique', 'Join Filter', 'Join Type', 'Lossy Heap Blocks', 'Merge Cond', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Rows Removed by Join Filter', 'Scan Direction', 'Single Copy', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers', 'Workers Launched', 'Workers Planned']\n",
      "\n",
      "[Per-key unique values] (sample if large)\n",
      "  - Actual Loops: 12749  sample: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  - Actual Rows: 139247  sample: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Actual Startup Time: 209032  sample: [0.25, 1.875, 0.625, 0.375, 0.75, 5.5, 4.5, 4.625, 8.5, 4.875]\n",
      "  - Actual Total Time: 320912  sample: [0.375, 0.125, 0.625, 0.5, 0.875, 0.25, 0.75, 4.5, 8.375, 1.625]\n",
      "  - Alias: 6  sample: ['t', 'mc', 'mk', 'mi_idx', 'ci', 'mi']\n",
      "  - Exact Heap Blocks: 10880  sample: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  - Filter: 34772  sample: ['(keyword_id < 19543)', '(keyword_id > 266)', '(company_id < 1404)', '(person_id > 3389811)', '(keyword_id > 3709)', '(keyword_id < 53039)', '(person_id > 855852)', '(person_id > 1920747)', '(company_id < 16690)', '(person_id < 2735518)']\n",
      "  - Hash Batches: 11  sample: [32, 1, 64, 128, 4, 256, 1024, 512, 8, 2, 16]\n",
      "  - Hash Buckets: 9  sample: [32768, 65536, 8192, 131072, 16384, 262144, 1024, 4096, 2048]\n",
      "  - Hash Cond: 21  sample: ['(mk.movie_id = ci.movie_id)', '(mi_idx.movie_id = ci.movie_id)', '(t.id = ci.movie_id)', '(mi.movie_id = mc.movie_id)', '(mk.movie_id = mc.movie_id)', '(mc.movie_id = mi_idx.movie_id)', '(mc.movie_id = t.id)', '(mc.movie_id = ci.movie_id)', '(ci.movie_id = mc.movie_id)', '(mi.movie_id = ci.movie_id)', '(mi.movie_id = mi_idx.movie_id)', '(mi.movie_id = mk.movie_id)', '(mi.movie_id = t.id)', '(t.id = mc.movie_id)', '(t.id = mi_idx.movie_id)', '(ci.movie_id = mi_idx.movie_id)', '(mi_idx.movie_id = t.id)', '(mk.movie_id = mi_idx.movie_id)', '(ci.movie_id = t.id)', '(mi_idx.movie_id = mc.movie_id)', '(mk.movie_id = t.id)']\n",
      "  - Index Cond: 23100  sample: ['(person_id = 3417979)', '(person_id = 2377323)', '(keyword_id > 94952)', '(person_id = 2707956)', '(keyword_id < 4437)', '(company_id = 669)', '(person_id > 3927051)', '(company_id = 4521)', '(keyword_id = 16270)', '(keyword_id = 12625)']\n",
      "  - Index Name: 14  sample: ['movie_id_movie_info_idx', 'info_type_id_movie_info', 'kind_id_title', 'movie_id_movie_companies', 'movie_id_cast_info', 'company_id_movie_companies', 'person_id_cast_info', 'movie_id_movie_keyword', 'role_id_cast_info', 'movie_id_movie_info', 'company_type_id_movie_companies', 'info_type_id_movie_info_idx', 'keyword_id_movie_keyword', 'title_pkey']\n",
      "  - Inner Unique: 2  sample: [False, True]\n",
      "  - Join Filter: 9  sample: ['(t.id = mc.movie_id)', '(t.id = mk.movie_id)', '(t.id = mi_idx.movie_id)', '(mc.movie_id = t.id)', '(mi_idx.movie_id = t.id)', '(ci.movie_id = t.id)', '(t.id = ci.movie_id)', '(t.id = mi.movie_id)', '(mi.movie_id = t.id)']\n",
      "  - Join Type: 1  sample: ['Inner']\n",
      "  - Lossy Heap Blocks: 1  sample: [0]\n",
      "  - Merge Cond: 8  sample: ['(t.id = mc.movie_id)', '(t.id = mk.movie_id)', '(t.id = mi_idx.movie_id)', '(mc.movie_id = t.id)', '(mi_idx.movie_id = ci.movie_id)', '(t.id = ci.movie_id)', '(t.id = mi.movie_id)', '(mk.movie_id = mi.movie_id)']\n",
      "  - Node Type: 13  sample: ['Sort', 'Seq Scan', 'Index Scan', 'BitmapAnd', 'Gather Merge', 'Merge Join', 'Bitmap Index Scan', 'Materialize', 'Hash', 'Bitmap Heap Scan', 'Gather', 'Nested Loop', 'Hash Join']\n",
      "  - Original Hash Batches: 11  sample: [32, 1, 64, 128, 4, 512, 256, 1024, 8, 2, 16]\n",
      "  - Original Hash Buckets: 9  sample: [32768, 65536, 16384, 131072, 8192, 1024, 4096, 262144, 2048]\n",
      "  - Parallel Aware: 2  sample: [False, True]\n",
      "  - Parent Relationship: 3  sample: ['Inner', 'Outer', 'Member']\n",
      "  - Peak Memory Usage: 2603  sample: [8192, 10, 13, 17, 21, 23, 27, 35, 48, 8256]\n",
      "  - Plan Rows: 141195  sample: [1048576, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Plan Width: 32  sample: [0, 131, 134, 136, 12, 146, 148, 25, 159, 161, 37, 40, 168, 42, 176, 180, 52, 54, 193, 65, 67, 74, 208, 210, 82, 86, 94, 99, 106, 114, 116, 119]\n",
      "  - Recheck Cond: 13632  sample: ['(keyword_id > 31525)', '(keyword_id = 868)', '(keyword_id = 8342)', '(keyword_id > 94952)', '(keyword_id > 9673)', '(company_id < 1214)', '(keyword_id = 620)', '(company_id > 49060)', '(person_id > 3821382)', '(keyword_id = 3866)']\n",
      "  - Relation Name: 6  sample: ['cast_info', 'movie_info_idx', 'title', 'movie_keyword', 'movie_info', 'movie_companies']\n",
      "  - Rows Removed by Filter: 34719  sample: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Rows Removed by Index Recheck: 1  sample: [0]\n",
      "  - Rows Removed by Join Filter: 1  sample: [0]\n",
      "  - Scan Direction: 1  sample: ['Forward']\n",
      "  - Single Copy: 1  sample: [False]\n",
      "  - Sort Key: 3  sample: ['[\"mk.movie_id\"]', '[\"ci.movie_id\"]', '[\"mi.movie_id\"]']\n",
      "  - Sort Method: 3  sample: ['quicksort', 'external merge', 'external sort']\n",
      "  - Sort Space Type: 2  sample: ['Disk', 'Memory']\n",
      "  - Sort Space Used: 5  sample: [1888, 2464, 8912, 1465, 4026]\n",
      "  - Startup Cost: 73298  sample: [0.0, 131073.71, 262145.65, 3.25, 131075.88, 3.5, 262153.76, 131084.36, 524301.42, 131087.93]\n",
      "  - Total Cost: 149033  sample: [0.5, 1.75, 2.5, 3.75, 1.5, 0.75, 6.0, 2.0, 8.25, 2.25]\n",
      "  - Workers: 6  sample: ['[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 1465, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 1465, \"Worker Number\": 1}]', '[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4026, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4026, \"Worker Number\": 1}]', '[]', '[{\"Sort Method\": \"external sort\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 1888, \"Worker Number\": 0}, {\"Sort Method\": \"external sort\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 1888, \"Worker Number\": 1}]', '[{\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 4078, \"Worker Number\": 0}, {\"Sort Method\": \"quicksort\", \"Sort Space Type\": \"Memory\", \"Sort Space Used\": 3950, \"Worker Number\": 1}]', '[{\"Sort Method\": \"external merge\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 8912, \"Worker Number\": 0}, {\"Sort Method\": \"external merge\", \"Sort Space Type\": \"Disk\", \"Sort Space Used\": 8912, \"Worker Number\": 1}]']\n",
      "  - Workers Launched: 2  sample: [1, 2]\n",
      "  - Workers Planned: 2  sample: [1, 2]\n",
      "\n",
      "[Per NodeType key stats]\n",
      "## Bitmap Heap Scan\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Lossy Heap Blocks', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Index Recheck', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(6): ['Alias', 'Exact Heap Blocks', 'Lossy Heap Blocks', 'Recheck Cond', 'Relation Name', 'Rows Removed by Index Recheck']\n",
      "   max(20): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Exact Heap Blocks', 'Filter', 'Lossy Heap Blocks', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Recheck Cond', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Filter', 'Parent Relationship', 'Rows Removed by Filter', 'Workers']\n",
      "## Bitmap Index Scan\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Index Cond', 'Index Name', 'Parent Relationship']\n",
      "   max(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## BitmapAnd\n",
      "   must_all(11): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(1): ['Parent Relationship']\n",
      "   max(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Gather\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Single Copy', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   must_only_type(3): ['Single Copy', 'Workers Launched', 'Workers Planned']\n",
      "   max(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Single Copy', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   optional(1): ['Parent Relationship']\n",
      "## Gather Merge\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   must_only_type(3): ['Parent Relationship', 'Workers Launched', 'Workers Planned']\n",
      "   max(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers Launched', 'Workers Planned']\n",
      "   optional(0): []\n",
      "## Hash\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Batches', 'Hash Buckets', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(6): ['Hash Batches', 'Hash Buckets', 'Original Hash Batches', 'Original Hash Buckets', 'Parent Relationship', 'Peak Memory Usage']\n",
      "   max(17): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Batches', 'Hash Buckets', 'Node Type', 'Original Hash Batches', 'Original Hash Buckets', 'Parallel Aware', 'Parent Relationship', 'Peak Memory Usage', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Hash Join\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Cond', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Hash Cond', 'Inner Unique', 'Join Type']\n",
      "   max(15): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Hash Cond', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(2): ['Parent Relationship', 'Workers']\n",
      "## Index Scan\n",
      "   must_all(14): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Index Name', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Relation Name', 'Scan Direction', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(4): ['Alias', 'Index Name', 'Relation Name', 'Scan Direction']\n",
      "   max(20): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Filter', 'Index Cond', 'Index Name', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Relation Name', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Scan Direction', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(6): ['Filter', 'Index Cond', 'Parent Relationship', 'Rows Removed by Filter', 'Rows Removed by Index Recheck', 'Workers']\n",
      "## Materialize\n",
      "   must_all(11): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(1): ['Parent Relationship']\n",
      "   max(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(1): ['Workers']\n",
      "## Merge Join\n",
      "   must_all(13): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Merge Cond', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(3): ['Inner Unique', 'Join Type', 'Merge Cond']\n",
      "   max(15): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Merge Cond', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(2): ['Parent Relationship', 'Workers']\n",
      "## Nested Loop\n",
      "   must_all(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Type', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(2): ['Inner Unique', 'Join Type']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Inner Unique', 'Join Filter', 'Join Type', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Rows Removed by Join Filter', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Join Filter', 'Parent Relationship', 'Rows Removed by Join Filter', 'Workers']\n",
      "## Seq Scan\n",
      "   must_all(12): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Node Type', 'Parallel Aware', 'Plan Rows', 'Plan Width', 'Relation Name', 'Startup Cost', 'Total Cost']\n",
      "   must_only_type(2): ['Alias', 'Relation Name']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Alias', 'Filter', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Relation Name', 'Rows Removed by Filter', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(4): ['Filter', 'Parent Relationship', 'Rows Removed by Filter', 'Workers']\n",
      "## Sort\n",
      "   must_all(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   must_only_type(6): ['Parent Relationship', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Workers']\n",
      "   max(16): ['Actual Loops', 'Actual Rows', 'Actual Startup Time', 'Actual Total Time', 'Node Type', 'Parallel Aware', 'Parent Relationship', 'Plan Rows', 'Plan Width', 'Sort Key', 'Sort Method', 'Sort Space Type', 'Sort Space Used', 'Startup Cost', 'Total Cost', 'Workers']\n",
      "   optional(0): []\n",
      "=== 使用BPE增强的节点向量化 ===\n",
      "训练BPE分词器...\n",
      "  relation_name: 词汇表大小 66\n",
      "  index_name: 词汇表大小 111\n",
      "  join_condition: 词汇表大小 43\n",
      "  filter_condition: 词汇表大小 500\n",
      "BPE向量化完成:\n",
      "  特征维度: 47\n",
      "  - 节点类型: 13\n",
      "  - 数值特征: 2\n",
      "  - BPE文本特征: 32\n",
      "  样本形状: 2x47\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "PlanCostModel(\n",
      "  (nodecoder): NodeEncoder(\n",
      "    (proj): Sequential(\n",
      "      (0): Linear(in_features=47, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (treeencoder): GATTreeEncoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GATConv(32, 64, heads=4)\n",
      "      (1-2): 2 x GATConv(256, 64, heads=4)\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (output_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (predict_head): PredictionHead(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1.读取数据\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 匹配所有 train_plan_0*.csv\n",
    "files = glob.glob(\"../data/train_plan_*.csv\")\n",
    "print(\"找到的文件:\", files)\n",
    "\n",
    "# 读入并合并\n",
    "df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "print(\"总数据行数:\", len(df))\n",
    "print(\"df:\\n\", df.head())\n",
    "\n",
    "#获取json字符串\n",
    "plans_json = df['json']\n",
    "print(\"plans_json:\\n\", plans_json.iloc[0])\n",
    "\n",
    "#字符串转json\n",
    "plans_dict = []\n",
    "ExecutionTimes = []\n",
    "idx = 0\n",
    "for json_str in plans_json:\n",
    "    idx += 1\n",
    "    plan_dict = json.loads(json_str)\n",
    "    plans_dict.append(plan_dict['Plan'])\n",
    "    try:\n",
    "        ExecutionTimes.append(plan_dict['Execution Time'])\n",
    "    except:\n",
    "        print(f\"idx: {idx} 不存在Execution Time\")\n",
    "        print(plan_dict)\n",
    "print(\"plans_dict:\\n\", plans_dict[0])\n",
    "\n",
    "# 2.数据格式转换 json -> PlanNode\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # 确保当前目录加入路径\n",
    "\n",
    "# json -> PlanNode\n",
    "from models.DataPreprocessor import PlanNode, DataPreprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "plans_tree = preprocessor.preprocess_all(plans_dict)\n",
    "\n",
    "# 3.数据格式转换 planNode -> graph 格式\n",
    "# PlanNode -> edges_list, extra_info_list\n",
    "def tree_to_graph(root):\n",
    "    edges_list, extra_info_list = [], []\n",
    "\n",
    "    def dfs(node, parent_idx):\n",
    "        idx = len(extra_info_list)\n",
    "        extra_info_list.append(node.extra_info)\n",
    "        edges_list.append((idx, idx))\n",
    "        if parent_idx is not None:\n",
    "            edges_list.append((parent_idx, idx))\n",
    "        for ch in node.children:\n",
    "            dfs(ch, idx)\n",
    "\n",
    "    dfs(root, None)\n",
    "    return edges_list, extra_info_list\n",
    "\n",
    "edges_list, matrix_plans = [], []\n",
    "for i in plans_tree:\n",
    "    edges_matrix, extra_info_matrix = tree_to_graph(i)\n",
    "    # if len(edges_matrix) == 0:\n",
    "    #     print(i)\n",
    "    #     assert False\n",
    "    edges_list.append(edges_matrix)\n",
    "    matrix_plans.append(extra_info_matrix)\n",
    "\n",
    "type(matrix_plans[0][0])\n",
    "\n",
    "# 统计信息\n",
    "\n",
    "from models.Utils import StatisticsInfo\n",
    "\n",
    "statisticsInfo = StatisticsInfo(matrix_plans, sample_threshold=100, sample_k=10).build()\n",
    "statisticsInfo.pretty_print_report()\n",
    "\n",
    "# NodeVectorizer\n",
    "import re, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "from models.Utils import process_join_cond_field, process_index_cond_field, load_column_stats\n",
    "\n",
    "# -------- 词表 --------\n",
    "class Vocab:\n",
    "    def __init__(self): self.idx = {\"<pad>\":0, \"<unk>\":1}\n",
    "    def add(self, s):\n",
    "        if s not in self.idx: self.idx[s] = len(self.idx)\n",
    "    def get(self, s): return self.idx.get(s, 1)\n",
    "    @property\n",
    "    def size(self): return len(self.idx)\n",
    "\n",
    "NodeTypeVocab = ['Bitmap Heap Scan', 'Bitmap Index Scan', 'BitmapAnd', 'Gather', 'Gather Merge', 'Hash', 'Hash Join', 'Index Scan', 'Materialize', 'Merge Join', 'Nested Loop', 'Seq Scan', 'Sort']\n",
    "\n",
    "\n",
    "# ===== BPE-Enhanced Node Vectorizer =====\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"简化版BPE分词器，用于处理SQL查询计划中的字符串特征\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = Counter()\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        \n",
    "    def _get_word_tokens(self, word: str) -> List[str]:\n",
    "        \"\"\"将单词拆分为字符级tokens\"\"\"\n",
    "        return list(word) + ['</w>']\n",
    "    \n",
    "    def _get_pairs(self, word_tokens: List[str]) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"获取相邻字符对\"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word_tokens[0]\n",
    "        for char in word_tokens[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"训练BPE模型\"\"\"\n",
    "        # 1. 预处理文本，收集词频\n",
    "        for text in texts:\n",
    "            if text is None:\n",
    "                continue\n",
    "            # 清理文本，处理SQL相关字符\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', str(text).lower())\n",
    "            for word in words:\n",
    "                self.word_freqs[word] += 1\n",
    "        \n",
    "        # 2. 初始化词汇表（字符级）\n",
    "        vocab = set()\n",
    "        for word in self.word_freqs.keys():\n",
    "            vocab.update(self._get_word_tokens(word))\n",
    "        \n",
    "        # 为每个字符分配ID\n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "        self.vocab['<UNK>'] = len(self.vocab)\n",
    "        self.vocab['<PAD>'] = len(self.vocab)\n",
    "        \n",
    "        # 3. 创建单词的token表示\n",
    "        word_tokens = {}\n",
    "        for word in self.word_freqs.keys():\n",
    "            word_tokens[word] = self._get_word_tokens(word)\n",
    "        \n",
    "        # 4. BPE合并过程\n",
    "        for _ in range(self.vocab_size - len(self.vocab)):\n",
    "            # 统计所有相邻字符对的频率\n",
    "            pairs_freq = Counter()\n",
    "            for word, freq in self.word_freqs.items():\n",
    "                pairs = self._get_pairs(word_tokens[word])\n",
    "                for pair in pairs:\n",
    "                    pairs_freq[pair] += freq\n",
    "            \n",
    "            if not pairs_freq:\n",
    "                break\n",
    "                \n",
    "            # 找到最频繁的字符对\n",
    "            best_pair = pairs_freq.most_common(1)[0][0]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # 合并这个字符对\n",
    "            new_token = ''.join(best_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            \n",
    "            # 更新所有单词的token表示\n",
    "            for word in word_tokens:\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens[word]):\n",
    "                    if (i < len(word_tokens[word]) - 1 and \n",
    "                        word_tokens[word][i] == best_pair[0] and \n",
    "                        word_tokens[word][i + 1] == best_pair[1]):\n",
    "                        new_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(word_tokens[word][i])\n",
    "                        i += 1\n",
    "                word_tokens[word] = new_tokens\n",
    "    \n",
    "    def encode(self, text: str, max_length: int = 32) -> List[int]:\n",
    "        \"\"\"将文本编码为token ID序列\"\"\"\n",
    "        if text is None:\n",
    "            return [self.vocab.get('<PAD>', 0)] * max_length\n",
    "            \n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', str(text).lower())\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = self._get_word_tokens(word)\n",
    "            \n",
    "            # 应用学习到的合并规则\n",
    "            for merge in self.merges:\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if (i < len(word_tokens) - 1 and \n",
    "                        word_tokens[i] == merge[0] and \n",
    "                        word_tokens[i + 1] == merge[1]):\n",
    "                        new_tokens.append(''.join(merge))\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_tokens\n",
    "            \n",
    "            # 转换为ID\n",
    "            for token in word_tokens:\n",
    "                token_id = self.vocab.get(token, self.vocab.get('<UNK>', 0))\n",
    "                tokens.append(token_id)\n",
    "        \n",
    "        # 截断或填充到指定长度\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        else:\n",
    "            pad_id = self.vocab.get('<PAD>', 0)\n",
    "            tokens.extend([pad_id] * (max_length - len(tokens)))\n",
    "            \n",
    "        return tokens\n",
    "\n",
    "class BPENodeVectorizer:\n",
    "    \"\"\"使用BPE增强的节点向量化器\"\"\"\n",
    "    \n",
    "    def __init__(self, node_types: List[str], use_bpe: bool = True, bpe_vocab_size: int = 500):\n",
    "        self.node_types = node_types\n",
    "        self.use_bpe = use_bpe\n",
    "        self.bpe_vocab_size = bpe_vocab_size\n",
    "        \n",
    "        # 数值特征维度\n",
    "        self.node_type_dim = len(node_types)  # one-hot编码\n",
    "        self.numeric_dim = 2  # Plan Width, Plan Rows\n",
    "        self.bpe_dim = 32 if use_bpe else 0  # BPE特征维度\n",
    "        \n",
    "        self.total_dim = self.node_type_dim + self.numeric_dim + self.bpe_dim\n",
    "        \n",
    "        if self.use_bpe:\n",
    "            self.bpe_tokenizers = {\n",
    "                'relation_name': BPETokenizer(bpe_vocab_size),\n",
    "                'index_name': BPETokenizer(bpe_vocab_size),\n",
    "                'join_condition': BPETokenizer(bpe_vocab_size),\n",
    "                'filter_condition': BPETokenizer(bpe_vocab_size),\n",
    "            }\n",
    "    \n",
    "    def _extract_text_features(self, matrix_plans: List[List[dict]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"提取所有文本特征用于BPE训练\"\"\"\n",
    "        text_features = {\n",
    "            'relation_name': [],\n",
    "            'index_name': [],\n",
    "            'join_condition': [],\n",
    "            'filter_condition': []\n",
    "        }\n",
    "        \n",
    "        for mp in matrix_plans:\n",
    "            for node in mp:\n",
    "                # 关系名\n",
    "                relation_name = node.get('Relation Name') or node.get('Alias', '')\n",
    "                text_features['relation_name'].append(str(relation_name))\n",
    "                \n",
    "                # 索引名\n",
    "                index_name = node.get('Index Name', '')\n",
    "                text_features['index_name'].append(str(index_name))\n",
    "                \n",
    "                # Join条件\n",
    "                join_cond = ''\n",
    "                if 'Hash Cond' in node:\n",
    "                    join_cond = str(node['Hash Cond'])\n",
    "                elif 'Merge Cond' in node:\n",
    "                    join_cond = str(node['Merge Cond'])\n",
    "                elif 'Join Filter' in node:\n",
    "                    join_cond = str(node['Join Filter'])\n",
    "                text_features['join_condition'].append(join_cond)\n",
    "                \n",
    "                # 过滤条件\n",
    "                filter_cond = ''\n",
    "                if 'Filter' in node:\n",
    "                    filter_cond = str(node['Filter'])\n",
    "                elif 'Index Cond' in node:\n",
    "                    filter_cond = str(node['Index Cond'])\n",
    "                text_features['filter_condition'].append(filter_cond)\n",
    "        \n",
    "        return text_features\n",
    "    \n",
    "    def fit(self, matrix_plans: List[List[dict]]):\n",
    "        \"\"\"训练BPE模型\"\"\"\n",
    "        if not self.use_bpe:\n",
    "            return\n",
    "            \n",
    "        print(\"训练BPE分词器...\")\n",
    "        text_features = self._extract_text_features(matrix_plans)\n",
    "        \n",
    "        for feature_name, tokenizer in self.bpe_tokenizers.items():\n",
    "            texts = text_features[feature_name]\n",
    "            # 过滤空文本\n",
    "            texts = [t for t in texts if t and str(t).strip() and str(t) != 'None']\n",
    "            if texts:\n",
    "                tokenizer.train(texts)\n",
    "                print(f\"  {feature_name}: 词汇表大小 {len(tokenizer.vocab)}\")\n",
    "    \n",
    "    def transform(self, matrix_plans: List[List[dict]]) -> List[List[List[float]]]:\n",
    "        \"\"\"将计划转换为向量表示\"\"\"\n",
    "        res = []\n",
    "        \n",
    "        for mp in matrix_plans:\n",
    "            plan_matrix = []\n",
    "            for node in mp:\n",
    "                node_vector = [0.0] * self.total_dim\n",
    "                offset = 0\n",
    "                \n",
    "                # 1. Node Type one-hot编码 [0:node_type_dim]\n",
    "                try:\n",
    "                    node_type_idx = self.node_types.index(node[\"Node Type\"])\n",
    "                    node_vector[node_type_idx] = 1.0\n",
    "                except ValueError:\n",
    "                    print(f\"未知节点类型: {node['Node Type']}\")\n",
    "                offset += self.node_type_dim\n",
    "                \n",
    "                # 2. 数值特征 [node_type_dim:node_type_dim+numeric_dim]\n",
    "                plan_width = float(node.get(\"Plan Width\", 0))\n",
    "                plan_rows = float(node.get(\"Plan Rows\", 0))\n",
    "                \n",
    "                # 对数归一化\n",
    "                node_vector[offset] = np.log1p(plan_width)\n",
    "                node_vector[offset + 1] = np.log1p(plan_rows)\n",
    "                offset += self.numeric_dim\n",
    "                \n",
    "                # 3. BPE文本特征 [node_type_dim+numeric_dim:total_dim]\n",
    "                if self.use_bpe:\n",
    "                    # 关系名特征 (8维)\n",
    "                    relation_name = node.get('Relation Name') or node.get('Alias', '')\n",
    "                    relation_tokens = self.bpe_tokenizers['relation_name'].encode(str(relation_name), 8)\n",
    "                    for i, token_id in enumerate(relation_tokens):\n",
    "                        if offset + i < len(node_vector):\n",
    "                            node_vector[offset + i] = float(token_id) / 1000.0  # 归一化\n",
    "                    offset += 8\n",
    "                    \n",
    "                    # 索引名特征 (8维)\n",
    "                    index_name = node.get('Index Name', '')\n",
    "                    index_tokens = self.bpe_tokenizers['index_name'].encode(str(index_name), 8)\n",
    "                    for i, token_id in enumerate(index_tokens):\n",
    "                        if offset + i < len(node_vector):\n",
    "                            node_vector[offset + i] = float(token_id) / 1000.0\n",
    "                    offset += 8\n",
    "                    \n",
    "                    # Join条件特征 (8维)\n",
    "                    join_cond = ''\n",
    "                    if 'Hash Cond' in node:\n",
    "                        join_cond = str(node['Hash Cond'])\n",
    "                    elif 'Merge Cond' in node:\n",
    "                        join_cond = str(node['Merge Cond'])\n",
    "                    elif 'Join Filter' in node:\n",
    "                        join_cond = str(node['Join Filter'])\n",
    "                    \n",
    "                    join_tokens = self.bpe_tokenizers['join_condition'].encode(join_cond, 8)\n",
    "                    for i, token_id in enumerate(join_tokens):\n",
    "                        if offset + i < len(node_vector):\n",
    "                            node_vector[offset + i] = float(token_id) / 1000.0\n",
    "                    offset += 8\n",
    "                    \n",
    "                    # 过滤条件特征 (8维)\n",
    "                    filter_cond = ''\n",
    "                    if 'Filter' in node:\n",
    "                        filter_cond = str(node['Filter'])\n",
    "                    elif 'Index Cond' in node:\n",
    "                        filter_cond = str(node['Index Cond'])\n",
    "                    \n",
    "                    filter_tokens = self.bpe_tokenizers['filter_condition'].encode(filter_cond, 8)\n",
    "                    for i, token_id in enumerate(filter_tokens):\n",
    "                        if offset + i < len(node_vector):\n",
    "                            node_vector[offset + i] = float(token_id) / 1000.0\n",
    "                \n",
    "                plan_matrix.append(node_vector)\n",
    "            res.append(plan_matrix)\n",
    "        \n",
    "        return res\n",
    "\n",
    "# 使用BPE增强的向量化器\n",
    "print(\"=== 使用BPE增强的节点向量化 ===\")\n",
    "bpe_vectorizer = BPENodeVectorizer(\n",
    "    node_types=NodeTypeVocab, \n",
    "    use_bpe=True, \n",
    "    bpe_vocab_size=500\n",
    ")\n",
    "\n",
    "# 训练BPE模型\n",
    "bpe_vectorizer.fit(matrix_plans)\n",
    "\n",
    "# 转换数据\n",
    "res = bpe_vectorizer.transform(matrix_plans)\n",
    "\n",
    "print(f\"BPE向量化完成:\")\n",
    "print(f\"  特征维度: {bpe_vectorizer.total_dim}\")\n",
    "print(f\"  - 节点类型: {bpe_vectorizer.node_type_dim}\")\n",
    "print(f\"  - 数值特征: {bpe_vectorizer.numeric_dim}\")\n",
    "print(f\"  - BPE文本特征: {bpe_vectorizer.bpe_dim}\")\n",
    "print(f\"  样本形状: {len(res[0])}x{len(res[0][0])}\")\n",
    "\n",
    "# 更新特征维度\n",
    "F_in = bpe_vectorizer.total_dim\n",
    "\n",
    "# 原始NodeVectorizer已被BPE版本替代\n",
    "\n",
    "# 模型搭建\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class NodeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: data.x 形状 [N, F_in]\n",
    "    输出: node_embs [N, d_node]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, d_node: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, d_node),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_node),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "# ---- 组合总模型 ----\n",
    "class PlanCostModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NodeEncoder → GATTreeEncoder → PredictionHead\n",
    "    \"\"\"\n",
    "    def __init__(self, nodecoder: nn.Module, treeencoder: nn.Module, predict_head: nn.Module):\n",
    "        super().__init__()\n",
    "        self.nodecoder = nodecoder\n",
    "        self.treeencoder = treeencoder\n",
    "        self.predict_head = predict_head\n",
    "\n",
    "    def forward(self, data: Data | Batch):\n",
    "        \"\"\"\n",
    "        期望 data 里至少有:\n",
    "        - x: [N, F_in]\n",
    "        - edge_index: [2, E]\n",
    "        - batch: [N]  指示每个节点属于哪张图\n",
    "        \"\"\"\n",
    "        x = self.nodecoder(data.x)                                   # [N, d_node]\n",
    "        g = self.treeencoder(x, data.edge_index, data.batch)         # [B, d_graph]\n",
    "        y = self.predict_head(g)                                     # [B, out_dim]\n",
    "        return y\n",
    "\n",
    "\n",
    "from models.TreeEncoder import GATTreeEncoder\n",
    "from models.PredictionHead import PredictionHead\n",
    "# ---- 使用示例 ----\n",
    "d_node, d_graph = 32, 64\n",
    "nodecoder = NodeEncoder(F_in, d_node)\n",
    "gatTreeEncoder = GATTreeEncoder(\n",
    "    input_dim=d_node,      # 一定用实际特征维度\n",
    "    hidden_dim=64,\n",
    "    output_dim=d_graph,\n",
    "    num_layers=3,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    pooling=\"mean\"\n",
    ")\n",
    "predict_head = PredictionHead(d_graph, out_dim=1)\n",
    "\n",
    "model = PlanCostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "\n",
    "print(type(ExecutionTimes))\n",
    "print(type(res))\n",
    "print(type(edges_list))\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "# 4.构建数据集\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def coerce_x_to_tensor(x_plan, in_dim: int):\n",
    "    \"\"\"\n",
    "    x_plan: 很深的 list（最终行向量长度= in_dim）\n",
    "    变成 [N, in_dim] 的 float32 Tensor\n",
    "    \"\"\"\n",
    "    x = torch.tensor(x_plan, dtype=torch.float32)\n",
    "    assert x.numel() % in_dim == 0, f\"最后一维应为 {in_dim}，拿到形状 {tuple(x.shape)}\"\n",
    "    x = x.view(-1, in_dim)   # 拉平成 [N, in_dim]\n",
    "    return x\n",
    "\n",
    "def coerce_edge_index(ei_like):\n",
    "    \"\"\"\n",
    "    ei_like: list/ndarray/tensor, 形状 [2,E] 或 [E,2]\n",
    "    返回规范 [2,E] 的 long Tensor\n",
    "    \"\"\"\n",
    "    ei = torch.as_tensor(ei_like, dtype=torch.long)\n",
    "    if ei.ndim != 2:\n",
    "        raise ValueError(f\"edge_index 需要二维，拿到 {tuple(ei.shape)}\")\n",
    "    if ei.shape[0] != 2 and ei.shape[1] == 2:\n",
    "        ei = ei.t().contiguous()\n",
    "    elif ei.shape[0] != 2 and ei.shape[1] != 2:\n",
    "        raise ValueError(f\"edge_index 需为 [2,E] 或 [E,2]，拿到 {tuple(ei.shape)}\")\n",
    "    return ei.contiguous()\n",
    "\n",
    "def build_dataset(res, edges_list, execution_times, in_dim=16, bidirectional=False):\n",
    "    assert len(res) == len(edges_list) == len(execution_times), \"长度必须一致\"\n",
    "    data_list = []\n",
    "    for i, (x_plan, ei_like, y) in enumerate(zip(res, edges_list, execution_times)):\n",
    "        x = coerce_x_to_tensor(x_plan, in_dim)      # [N, in_dim]\n",
    "        edge_index = coerce_edge_index(ei_like)     # [2,E]\n",
    "        N = x.size(0)\n",
    "\n",
    "        # 边索引有效性检查\n",
    "        if edge_index.numel() > 0:\n",
    "            if int(edge_index.min()) < 0 or int(edge_index.max()) >= N:\n",
    "                raise ValueError(f\"plan[{i}] 的 edge_index 越界：节点数 N={N}，但 edge_index.max={int(edge_index.max())}\")\n",
    "\n",
    "        # 可选：做成双向图（若你的 edges 只有父->子）\n",
    "        if bidirectional:\n",
    "            edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "        y = torch.tensor([float(y)], dtype=torch.float32)  # 图级回归标签\n",
    "        data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf3815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 100000\n",
      "第一个样本: x.shape=torch.Size([2, 47]), edge_index.shape=torch.Size([2, 6]), y=tensor([224.4540])\n",
      "训练集: 70000, 验证集: 15000, 测试集: 15000\n",
      "训练批次数: 2188, 验证批次数: 469\n",
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== #\n",
    "# 构建数据集\n",
    "dataset = build_dataset(res, edges_list, ExecutionTimes, in_dim=F_in, bidirectional=True)\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "print(f\"第一个样本: x.shape={dataset[0].x.shape}, edge_index.shape={dataset[0].edge_index.shape}, y={dataset[0].y}\")\n",
    "\n",
    "# 5. 训练准备\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 数据集划分\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    range(len(dataset)), test_size=0.3, random_state=42\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = [dataset[i] for i in train_indices]\n",
    "val_dataset = [dataset[i] for i in val_indices]\n",
    "test_dataset = [dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"训练集: {len(train_dataset)}, 验证集: {len(val_dataset)}, 测试集: {len(test_dataset)}\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}, 验证批次数: {len(val_loader)}\")\n",
    "\n",
    "# 6. 训练配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e31e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 训练函数\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.y)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_errors = []\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.y)\n",
    "\n",
    "            q_error = torch.maximum(pred / batch.y, batch.y / pred)\n",
    "            q_error_mean = q_error.mean()\n",
    "            q_errors.append(q_error_mean.item())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    print(f\"Q-error: {np.mean(q_errors):.6f}\")\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# 8. 训练循环\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "                early_stopping, device, num_epochs=100):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"开始训练...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 计算时间\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # 打印进度\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\n早停触发在第 {epoch+1} 轮\")\n",
    "            break\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, '../models/best_model.pth')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"训练完成!\")\n",
    "    print(f\"最佳验证损失: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "------------------------------------------------------------\n",
      "Q-error: 320.422213\n",
      "Epoch   1/100 | Train Loss: 44614010.864145 | Val Loss: 46447202.091818 | LR: 1.00e-03 | Time: 25.92s\n",
      "Q-error: 738.004716\n",
      "Q-error: 581.083236\n",
      "Q-error: 171.822751\n",
      "Q-error: 34.776389\n",
      "Q-error: 413.293125\n",
      "Epoch   6/100 | Train Loss: 36454521.536277 | Val Loss: 29828281.277719 | LR: 1.00e-03 | Time: 29.06s\n",
      "Q-error: 1997.085861\n",
      "Q-error: 1793.630228\n",
      "Q-error: 636.986132\n",
      "Q-error: 1262.282821\n",
      "Q-error: 143.686626\n",
      "Epoch  11/100 | Train Loss: 30705269.481576 | Val Loss: 22259106.845682 | LR: 1.00e-03 | Time: 26.92s\n",
      "Q-error: 106.204749\n",
      "Q-error: 47.841082\n",
      "Q-error: 198.913776\n",
      "Q-error: 204.342360\n",
      "Q-error: 66.556571\n",
      "Epoch  16/100 | Train Loss: 30522661.629770 | Val Loss: 39542781.332190 | LR: 1.00e-03 | Time: 30.28s\n",
      "Q-error: 40.017836\n",
      "Q-error: 50.446628\n",
      "Q-error: 42.156121\n",
      "Q-error: 133.768519\n",
      "Q-error: 387.463919\n",
      "Epoch  21/100 | Train Loss: 28158963.626150 | Val Loss: 19005090.735208 | LR: 1.00e-03 | Time: 26.75s\n",
      "Q-error: 31.505780\n",
      "Q-error: 190.655845\n",
      "Q-error: 256.102948\n",
      "Q-error: 94.327629\n",
      "Q-error: 65.677823\n",
      "Epoch  26/100 | Train Loss: 28283843.842707 | Val Loss: 26254197.451426 | LR: 1.00e-03 | Time: 25.72s\n",
      "Q-error: 49.414222\n",
      "Q-error: 100.437018\n",
      "Q-error: 204.094919\n",
      "Q-error: 182.181912\n",
      "Q-error: 87.947714\n",
      "Epoch  31/100 | Train Loss: 27006274.906886 | Val Loss: 23735959.995736 | LR: 1.00e-03 | Time: 27.02s\n",
      "Q-error: 59.650540\n",
      "Q-error: 33.279876\n",
      "Q-error: 61.568286\n",
      "Q-error: 26.317036\n",
      "Q-error: 11.907277\n",
      "Epoch  36/100 | Train Loss: 21777628.410828 | Val Loss: 17836400.802639 | LR: 5.00e-04 | Time: 26.22s\n",
      "Q-error: 39.506866\n",
      "Q-error: 7.796242\n",
      "Q-error: 550.184431\n",
      "Q-error: 6.523978\n",
      "Q-error: 17.588960\n",
      "Epoch  41/100 | Train Loss: 20941664.945562 | Val Loss: 17262126.624300 | LR: 5.00e-04 | Time: 27.36s\n",
      "Q-error: 34.042634\n",
      "Q-error: 15.832951\n",
      "Q-error: 30.010063\n",
      "Q-error: 25.893814\n",
      "Q-error: 53.946226\n",
      "Epoch  46/100 | Train Loss: 21447733.117859 | Val Loss: 16641211.072961 | LR: 5.00e-04 | Time: 26.75s\n",
      "Q-error: 27.187151\n",
      "Q-error: 9.820826\n",
      "Q-error: 30.064194\n",
      "Q-error: 12.276365\n",
      "Q-error: 26.452028\n",
      "Epoch  51/100 | Train Loss: 20366149.214265 | Val Loss: 14973682.545442 | LR: 5.00e-04 | Time: 27.38s\n",
      "Q-error: 37.515656\n",
      "Q-error: 11.382278\n",
      "Q-error: 19.315333\n",
      "Q-error: 18.462671\n",
      "Q-error: 21.563050\n",
      "Epoch  56/100 | Train Loss: 20682870.700861 | Val Loss: 30196609.570063 | LR: 5.00e-04 | Time: 27.37s\n",
      "Q-error: 32.414657\n",
      "Q-error: 11.143318\n",
      "Q-error: 13.305356\n",
      "Q-error: 64.091647\n",
      "Q-error: 11.250347\n",
      "Epoch  61/100 | Train Loss: 19657930.351363 | Val Loss: 14309266.486407 | LR: 5.00e-04 | Time: 26.56s\n",
      "Q-error: 17.899636\n",
      "Q-error: 17.269729\n",
      "Q-error: 37.562403\n",
      "Q-error: 14.959305\n",
      "Q-error: 28.674918\n",
      "Epoch  66/100 | Train Loss: 18778839.565471 | Val Loss: 13241729.889859 | LR: 5.00e-04 | Time: 29.03s\n",
      "Q-error: 52.538859\n",
      "Q-error: 26.214305\n",
      "Q-error: 22.624097\n",
      "Q-error: 19.869031\n",
      "Q-error: 18.486503\n",
      "Epoch  71/100 | Train Loss: 19044418.146084 | Val Loss: 19044056.078225 | LR: 5.00e-04 | Time: 26.01s\n",
      "Q-error: 11.975009\n",
      "Q-error: 21.076860\n",
      "Q-error: 11.371543\n",
      "Q-error: 52.908469\n",
      "Q-error: 16.397792\n",
      "Epoch  76/100 | Train Loss: 18684808.135587 | Val Loss: 14124968.184402 | LR: 5.00e-04 | Time: 30.62s\n",
      "Q-error: 27.596961\n",
      "Q-error: 32.022574\n",
      "Q-error: 26.575646\n",
      "Q-error: 23.427099\n",
      "Q-error: 20.897628\n",
      "Epoch  81/100 | Train Loss: 18234561.975759 | Val Loss: 20327294.014809 | LR: 5.00e-04 | Time: 31.31s\n",
      "Q-error: 17.839655\n",
      "Q-error: 11.990034\n",
      "Q-error: 35.646562\n",
      "Q-error: 5.814226\n",
      "Q-error: 33.033830\n",
      "Epoch  86/100 | Train Loss: 17922578.843525 | Val Loss: 14014962.524287 | LR: 5.00e-04 | Time: 25.30s\n",
      "Q-error: 12.969268\n",
      "Q-error: 14.380795\n",
      "Q-error: 16.783238\n",
      "Q-error: 20.183200\n",
      "Q-error: 29.730166\n",
      "Epoch  91/100 | Train Loss: 16608745.501860 | Val Loss: 18840820.152685 | LR: 5.00e-04 | Time: 31.59s\n",
      "Q-error: 30.237743\n",
      "Q-error: 21.363229\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, \n",
    "    criterion, early_stopping, device, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0639348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 测试评估\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(batch.y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - targets))\n",
    "    \n",
    "    # 计算相对误差\n",
    "    relative_error = np.mean(np.abs((predictions - targets) / (targets + 1e-8)))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"测试集评估结果:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"MSE:  {mse:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE:  {mae:.6f}\")\n",
    "    print(f\"相对误差: {relative_error:.6f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return predictions, targets, {'mse': mse, 'rmse': rmse, 'mae': mae, 'relative_error': relative_error}\n",
    "\n",
    "# 加载最佳模型进行测试\n",
    "try:\n",
    "    checkpoint = torch.load('../models/best_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"已加载最佳模型进行测试\")\n",
    "except FileNotFoundError:\n",
    "    print(\"未找到保存的模型，使用当前模型进行测试\")\n",
    "\n",
    "predictions, targets, metrics = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# 10. 可视化训练过程和结果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='训练损失')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='验证损失')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('训练和验证损失')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 预测 vs 真实值\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(targets, predictions, alpha=0.5, s=20)\n",
    "    min_val = min(targets.min(), predictions.min())\n",
    "    max_val = max(targets.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.xlabel('真实执行时间')\n",
    "    plt.ylabel('预测执行时间')\n",
    "    plt.title('预测 vs 真实值')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# 绘制结果\n",
    "plot_training_history(train_losses, val_losses)\n",
    "\n",
    "# 11. 模型保存和加载工具函数\n",
    "def save_model_with_metadata(model, filepath, metadata=None):\n",
    "    \"\"\"保存模型和相关元数据\"\"\"\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'F_in': F_in,\n",
    "            'd_node': d_node,\n",
    "            'd_graph': d_graph,\n",
    "        },\n",
    "        'training_metadata': metadata or {}\n",
    "    }\n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"模型已保存到: {filepath}\")\n",
    "\n",
    "def load_model_with_metadata(filepath, model_class, device='cpu'):\n",
    "    \"\"\"加载模型和元数据\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    # 重建模型\n",
    "    config = checkpoint['model_config']\n",
    "    nodecoder = NodeEncoder(config['F_in'], config['d_node'])\n",
    "    gatTreeEncoder = GATTreeEncoder(\n",
    "        input_dim=config['d_node'],\n",
    "        hidden_dim=64,\n",
    "        output_dim=config['d_graph'],\n",
    "        num_layers=3,\n",
    "        num_heads=4,\n",
    "        dropout=0.1,\n",
    "        pooling=\"mean\"\n",
    "    )\n",
    "    predict_head = PredictionHead(config['d_graph'], out_dim=1)\n",
    "    model = PlanCostModel(nodecoder, gatTreeEncoder, predict_head)\n",
    "    \n",
    "    # 加载权重\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint.get('training_metadata', {})\n",
    "\n",
    "# 保存最终模型\n",
    "final_metadata = {\n",
    "    'train_size': len(train_dataset),\n",
    "    'val_size': len(val_dataset),\n",
    "    'test_size': len(test_dataset),\n",
    "    'final_metrics': metrics,\n",
    "    'training_epochs': len(train_losses)\n",
    "}\n",
    "\n",
    "save_model_with_metadata(model, '../models/final_model.pth', final_metadata)\n",
    "\n",
    "print(\"\\n训练完成! 模型已保存，结果已可视化。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
