"""Pure Node-level Encoder for PlanNode objects.

This is the correct NODE ENCODER layer in the architecture:
ğŸ“Š Architecture Position: Step 2 (Node-level Encoding)
- Input: Individual TreeNode with attributes (node_type, extra_info)
- Output: Node-level embedding vector
- Scope: ONLY single node feature extraction

âš ï¸  IMPORTANT: This encoder handles ONLY node-level features.
NO tree structure processing, NO recursive aggregation.
Structure-level encoding is handled by TreeModel.
"""

from __future__ import annotations

from typing import Dict, Iterable, List, Optional, Any, Union, Tuple
import numpy as np
from dataclasses import dataclass
import re
from collections import Counter

import torch
import torch.nn as nn


@dataclass
class NodeFeatures:
    """Container for node features extracted from query plan nodes."""
    
    node_type_vector: np.ndarray  # One-hot encoding of node type
    numerical_features: np.ndarray  # Numerical features like cost, rows, etc.
    categorical_features: np.ndarray  # Encoded categorical features
    predicate_features: np.ndarray  # Encoded predicate/condition features
    relation_features: np.ndarray  # Relation/index information features
    execution_context_features: np.ndarray  # Execution context features
    historical_features: np.ndarray  # Historical/actual feedback features
    combined_features: np.ndarray  # Combined feature vector


class NodeEncoder(nn.Module):
    """Pure node-level encoder for converting individual PlanNodes to feature vectors.
    
    This is the NODE ENCODER layer in the correct architecture:
    ğŸ“Š Architecture Position: Step 2 (Node-level Encoding)
    - Input: Individual TreeNode with attributes (node_type, extra_info)
    - Output: Node-level embedding vector
    - Scope: Single node feature extraction ONLY
    
    âš ï¸  CRITICAL: This encoder handles ONLY node-level features.
    - NO recursive processing of children
    - NO tree structure aggregation
    - NO graph construction
    
    Structure-level encoding (tree/graph aggregation) is handled by TreeModel.
    
    Two encoding strategies:
    1. Simple mode: Only node type (one-hot)
    2. Rich mode: Node type + numerical + categorical features
    """
    
    def __init__(self,
                 rich_features: bool = False,
                 feature_dim: Optional[int] = None,
                 include_numerical: bool = True,
                 include_categorical: bool = True,
                 normalize_features: bool = True) -> None:
        """Initialize the node encoder.
        
        Parameters
        ----------
        rich_features:
            If True, extracts comprehensive features beyond just node type
        feature_dim:
            Target dimension for output vectors (if None, uses dynamic size)
        include_numerical:
            Whether to include numerical features from extra_info
        include_categorical:
            Whether to include categorical features from extra_info
        normalize_features:
            Whether to normalize numerical features
        """
        super().__init__()

        # Core vocabulary for node types
        self.node_index: Dict[str, int] = {}
        
        # Feature extraction configuration
        self.rich_features = rich_features
        self.feature_dim = feature_dim
        self.include_numerical = include_numerical
        self.include_categorical = include_categorical
        self.normalize_features = normalize_features
        
        # Additional vocabularies for rich features
        self.categorical_vocabs: Dict[str, Dict[str, int]] = {}
        
        # Predefined feature keys for query plans
        self.numerical_keys = [
            'Total Cost', 'Startup Cost', 'Plan Rows', 'Plan Width',
            'Actual Total Time', 'Actual Rows', 'Actual Loops'
        ]
        
        self.categorical_keys = [
            'Join Type', 'Scan Direction', 'Strategy', 'Parent Relationship',
            'Relation Name', 'Index Name', 'Sort Method'
        ]
        
        # Additional feature keys for enhanced encoding
        self.predicate_keys = [
            'Filter', 'Index Cond', 'Hash Cond', 'Merge Cond', 'Join Filter'
        ]
        
        self.relation_keys = [
            'Relation Name', 'Alias', 'Index Name', 'Schema'
        ]
        
        self.execution_context_keys = [
            'Workers Planned', 'Workers Launched', 'Parallel Aware'
        ]
        
        self.historical_keys = [
            'Actual Total Time', 'Actual Startup Time', 'Actual Rows', 
            'Actual Loops', 'Shared Hit Blocks', 'Shared Read Blocks'
        ]
        
        # Embedding dimensions for different components
        self.operator_embedding_dim = 32
        self.relation_embedding_dim = 16
        self.predicate_embedding_dim = 24
    
    # ------------------------------------------------------------------ utilities
    def _ensure_index(self, node_type: str) -> int:
        """Ensure a node type exists in vocabulary and return its index."""
        if node_type not in self.node_index:
            self.node_index[node_type] = len(self.node_index)
        return self.node_index[node_type]
    
    def _one_hot(self, idx: int) -> np.ndarray:
        """Create one-hot vector for given index."""
        vec = np.zeros(len(self.node_index), dtype=float)
        vec[idx] = 1.0
        return vec
    
    def _ensure_categorical_vocab(self, vocab_dict: Dict[str, int], key: str) -> int:
        """Ensure a key exists in categorical vocabulary and return its index."""
        if key not in vocab_dict:
            vocab_dict[key] = len(vocab_dict)
        return vocab_dict[key]
    
    # --------------------------------------------------------------------- encoder
    def encode_node(self, node) -> 'torch.Tensor':
        """Encode a SINGLE PlanNode to feature vector and store it in node.node_vector.
        
        âš ï¸  IMPORTANT: This method processes ONLY the given node.
        It does NOT process children or any tree structure.
        
        Parameters
        ----------
        node:
            PlanNode object to encode (children are IGNORED)
            
        Returns
        -------
        np.ndarray:
            Node-level feature vector (also stored in node.node_vector)
        """
        if not self.rich_features:
            # Simple mode: only node type
            vector = self._encode_simple(node)
        else:
            # Rich mode: comprehensive features
            vector = self._encode_rich(node)

        # Convert to torch tensor for downstream models
        tensor = torch.as_tensor(vector, dtype=torch.float32)

        # Store the vector in the node
        node.node_vector = tensor
        return tensor
    
    def _encode_simple(self, node) -> np.ndarray:
        """Simple encoding: only node type (one-hot)."""
        idx = self._ensure_index(getattr(node, "node_type", "Unknown"))
        return self._one_hot(idx)
    
    def _encode_rich(self, node) -> np.ndarray:
        """Rich encoding: node type + numerical + categorical features."""
        # Start with node type encoding
        idx = self._ensure_index(getattr(node, "node_type", "Unknown"))
        node_type_vec = self._one_hot(idx)
        
        # Collect all feature components
        feature_components = [node_type_vec]
        
        # Add numerical features
        if self.include_numerical:
            numerical_features = self._extract_numerical_features(node)
            if len(numerical_features) > 0:
                feature_components.append(numerical_features)
        
        # Add categorical features
        if self.include_categorical:
            categorical_features = self._extract_categorical_features(node)
            if len(categorical_features) > 0:
                feature_components.append(categorical_features)
        
        # Combine all features
        combined_features = np.concatenate(feature_components)
        
        # Apply fixed dimension if specified
        if self.feature_dim is not None:
            combined_features = self._resize_vector(combined_features, self.feature_dim)
        
        return combined_features
    
    def encode_nodes(self, nodes: Iterable) -> List['torch.Tensor']:
        """Encode multiple nodes into vectors and store them in each node.node_vector.
        
        Each node is processed independently and its vector is stored in node.node_vector.
        
        Parameters
        ----------
        nodes:
            Iterable of PlanNode objects to encode
            
        Returns
        -------
        List[np.ndarray]:
            List of node-level feature vectors (also stored in each node.node_vector)
        """
        return [self.encode_node(node) for node in nodes]

    # --------------------------------------------------------------------- nn.Module
    def forward(self, node):
        """Alias for encode_node to comply with nn.Module interface."""
        return self.encode_node(node)
    
    # --------------------------------------------------------- feature extraction ---
    def _extract_numerical_features(self, node) -> np.ndarray:
        """Extract numerical features from node's extra_info."""
        extra_info = getattr(node, 'extra_info', {})
        features = []
        
        for key in self.numerical_keys:
            value = extra_info.get(key, 0.0)
            
            # Handle different value types
            if isinstance(value, (int, float)):
                features.append(float(value))
            elif isinstance(value, str):
                try:
                    features.append(float(value))
                except ValueError:
                    features.append(0.0)
            else:
                features.append(0.0)
        
        if not features:
            return np.array([])
        
        features = np.array(features, dtype=np.float32)
        
        # Normalize if requested
        if self.normalize_features:
            features = self._normalize_numerical(features)
        
        return features
    
    def _extract_categorical_features(self, node) -> np.ndarray:
        """Extract categorical features from node's extra_info."""
        extra_info = getattr(node, 'extra_info', {})
        features = []
        
        for key in self.categorical_keys:
            value = extra_info.get(key, 'Unknown')
            
            # Ensure vocabulary exists for this key
            if key not in self.categorical_vocabs:
                self.categorical_vocabs[key] = {}
            
            # Convert value to string and get index
            str_value = str(value) if value is not None else 'Unknown'
            idx = self._ensure_categorical_vocab(self.categorical_vocabs[key], str_value)
            features.append(idx)
        
        return np.array(features, dtype=np.float32) if features else np.array([])
    
    # ===================================================================
    # è¾“å…¥ç»´åº¦ç¼–ç æ–¹æ³• (Input Dimension Encoding Methods)
    # ===================================================================
    
    def encode_operator_type(self, node, method: str = 'one_hot') -> np.ndarray:
        """ç®—å­ç±»å‹ç¼–ç  (Operator Type Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        method: str
            ç¼–ç æ–¹æ³•: 'one_hot', 'embedding', 'learned_embedding'
            
        Returns
        -------
        np.ndarray
            ç®—å­ç±»å‹ç‰¹å¾å‘é‡
        """
        node_type = getattr(node, "node_type", "Unknown")
        
        if method == 'one_hot':
            idx = self._ensure_index(node_type)
            return self._one_hot(idx)
        
        elif method == 'embedding':
            # ç®€å•çš„å›ºå®šembedding (å¯ä»¥ç”¨é¢„è®­ç»ƒçš„ç®—å­embeddingæ›¿æ¢)
            idx = self._ensure_index(node_type)
            # åˆ›å»ºç®€å•çš„embeddingå‘é‡
            embedding = np.zeros(self.operator_embedding_dim, dtype=np.float32)
            # ä½¿ç”¨hashæ¥åˆ›å»ºä¼ªéšæœºä½†ç¡®å®šæ€§çš„embedding
            hash_val = hash(node_type) % self.operator_embedding_dim
            embedding[hash_val] = 1.0
            # æ·»åŠ ä¸€äº›å˜åŒ–
            for i in range(min(3, self.operator_embedding_dim)):
                embedding[(hash_val + i + 1) % self.operator_embedding_dim] = 0.5
            return embedding / np.linalg.norm(embedding)
        
        elif method == 'learned_embedding':
            # å¯å­¦ä¹ çš„embedding (éœ€è¦è®­ç»ƒ)
            # è¿™é‡Œè¿”å›ç´¢å¼•ï¼Œå®é™…è®­ç»ƒæ—¶ä¼šç”¨embeddingå±‚
            idx = self._ensure_index(node_type)
            return np.array([idx], dtype=np.float32)
        
        else:
            raise ValueError(f"Unknown encoding method: {method}")
    
    def encode_estimated_stats(self, node, normalize: bool = True) -> np.ndarray:
        """æ•°æ®ç»Ÿè®¡ç¼–ç  (Estimated Stats Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        normalize: bool
            æ˜¯å¦æ ‡å‡†åŒ–æ•°å€¼
            
        Returns
        -------
        np.ndarray
            æ•°æ®ç»Ÿè®¡ç‰¹å¾å‘é‡
        """
        extra_info = getattr(node, 'extra_info', {})
        stats_features = []
        
        # æ ¸å¿ƒç»Ÿè®¡ä¿¡æ¯
        core_stats = ['Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']
        
        for stat in core_stats:
            value = extra_info.get(stat, 0.0)
            if isinstance(value, (int, float)):
                stats_features.append(float(value))
            else:
                try:
                    stats_features.append(float(str(value).replace(',', '')))
                except:
                    stats_features.append(0.0)
        
        if not stats_features:
            return np.array([])
        
        features = np.array(stats_features, dtype=np.float32)
        
        if normalize:
            # ä½¿ç”¨log normalization for cost and rows
            features[:2] = np.log1p(features[:2])  # rows, width
            features[2:] = np.log1p(features[2:])  # costs
            
            # ç®€å•çš„min-max normalization
            features = np.clip(features, 0, np.percentile(features, 95) if np.any(features > 0) else 1)
            max_val = np.maximum(features.max(), 1.0)
            features = features / max_val
        
        return features
    
    def encode_predicate_info(self, node, method: str = 'complexity') -> np.ndarray:
        """è°“è¯/æ¡ä»¶ä¿¡æ¯ç¼–ç  (Predicate Info Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        method: str
            ç¼–ç æ–¹æ³•: 'complexity', 'bow', 'token_embedding'
            
        Returns
        -------
        np.ndarray
            è°“è¯ç‰¹å¾å‘é‡
        """
        extra_info = getattr(node, 'extra_info', {})
        
        # æ”¶é›†æ‰€æœ‰è°“è¯ä¿¡æ¯
        predicates = []
        for key in self.predicate_keys:
            if key in extra_info and extra_info[key]:
                predicates.append(str(extra_info[key]))
        
        if not predicates:
            if method == 'complexity':
                return np.zeros(6, dtype=np.float32)  # 6ä¸ªå¤æ‚åº¦ç‰¹å¾
            else:
                return np.zeros(self.predicate_embedding_dim, dtype=np.float32)
        
        if method == 'complexity':
            return self._encode_predicate_complexity(predicates)
        elif method == 'bow':
            return self._encode_predicate_bow(predicates)
        elif method == 'token_embedding':
            return self._encode_predicate_tokens(predicates)
        else:
            raise ValueError(f"Unknown predicate encoding method: {method}")
    
    def encode_relation_info(self, node, method: str = 'embedding') -> np.ndarray:
        """å…³ç³»/ç´¢å¼•ä¿¡æ¯ç¼–ç  (Relation/Index Info Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        method: str
            ç¼–ç æ–¹æ³•: 'embedding', 'one_hot', 'features'
            
        Returns
        -------
        np.ndarray
            å…³ç³»ä¿¡æ¯ç‰¹å¾å‘é‡
        """
        extra_info = getattr(node, 'extra_info', {})
        
        if method == 'features':
            # å¸ƒå°”ç‰¹å¾ç»„åˆ
            features = []
            
            # æ˜¯å¦ä½¿ç”¨ç´¢å¼•
            has_index = bool(extra_info.get('Index Name') or extra_info.get('Index Cond'))
            features.append(float(has_index))
            
            # æ˜¯å¦æœ‰è¡¨å
            has_relation = bool(extra_info.get('Relation Name'))
            features.append(float(has_relation))
            
            # æ˜¯å¦æœ‰åˆ«å
            has_alias = bool(extra_info.get('Alias'))
            features.append(float(has_alias))
            
            # æ‰«æç±»å‹
            scan_types = ['Seq Scan', 'Index Scan', 'Index Only Scan', 'Bitmap Heap Scan']
            node_type = getattr(node, "node_type", "Unknown")
            for scan_type in scan_types:
                features.append(float(scan_type in node_type))
            
            return np.array(features, dtype=np.float32)
        
        elif method == 'embedding':
            # å…³ç³»åembedding
            relation_name = extra_info.get('Relation Name', 'Unknown')
            return self._create_relation_embedding(relation_name)
        
        elif method == 'one_hot':
            # å…³ç³»åone-hot
            relation_name = extra_info.get('Relation Name', 'Unknown')
            if 'relation_vocab' not in self.categorical_vocabs:
                self.categorical_vocabs['relation_vocab'] = {}
            idx = self._ensure_categorical_vocab(self.categorical_vocabs['relation_vocab'], relation_name)
            vec = np.zeros(len(self.categorical_vocabs['relation_vocab']), dtype=float)
            vec[idx] = 1.0
            return vec
        
        else:
            raise ValueError(f"Unknown relation encoding method: {method}")
    
    def encode_execution_context(self, node) -> np.ndarray:
        """æ‰§è¡Œä¸Šä¸‹æ–‡ç¼–ç  (Execution Context Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
            
        Returns
        -------
        np.ndarray
            æ‰§è¡Œä¸Šä¸‹æ–‡ç‰¹å¾å‘é‡
        """
        extra_info = getattr(node, 'extra_info', {})
        context_features = []
        
        # æ˜¯å¦å¹¶è¡Œ
        workers_planned = extra_info.get('Workers Planned', 0)
        is_parallel = float(workers_planned > 0)
        context_features.append(is_parallel)
        
        # å¹¶è¡Œåº¦
        parallel_degree = float(workers_planned) if workers_planned else 0.0
        context_features.append(parallel_degree)
        
        # æ˜¯å¦æ˜¯é˜»å¡ç®—å­
        node_type = getattr(node, "node_type", "Unknown")
        blocking_ops = ['Sort', 'Hash', 'Aggregate', 'WindowAgg', 'Materialize']
        is_blocking = float(any(op in node_type for op in blocking_ops))
        context_features.append(is_blocking)
        
        # æ˜¯å¦æ˜¯è¿æ¥ç®—å­
        join_ops = ['Hash Join', 'Merge Join', 'Nested Loop']
        is_join = float(any(op in node_type for op in join_ops))
        context_features.append(is_join)
        
        # JoinåŸºæ•°æ¯” (å¦‚æœæ˜¯joinç®—å­)
        if is_join:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè®¡ç®—å·¦å³è¾“å…¥åŸºæ•°æ¯”
            # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨è®¡åˆ’è¡Œæ•°ä½œä¸ºä»£ç†
            plan_rows = extra_info.get('Plan Rows', 1.0)
            cardinality_ratio = np.log1p(float(plan_rows))
            context_features.append(cardinality_ratio)
        else:
            context_features.append(0.0)
        
        # Pipelineç‰¹å¾
        pipeline_ops = ['Seq Scan', 'Index Scan', 'Filter']
        is_pipeline = float(any(op in node_type for op in pipeline_ops))
        context_features.append(is_pipeline)
        
        return np.array(context_features, dtype=np.float32)
    
    def encode_historical_feedback(self, node, normalize: bool = True) -> np.ndarray:
        """å†å²/çœŸå®åé¦ˆç¼–ç  (Historical/Actual Feedback Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        normalize: bool
            æ˜¯å¦æ ‡å‡†åŒ–
            
        Returns
        -------
        np.ndarray
            å†å²åé¦ˆç‰¹å¾å‘é‡
        """
        extra_info = getattr(node, 'extra_info', {})
        historical_features = []
        
        # å®é™…æ‰§è¡Œç»Ÿè®¡
        actual_stats = ['Actual Total Time', 'Actual Startup Time', 'Actual Rows', 'Actual Loops']
        
        for stat in actual_stats:
            value = extra_info.get(stat, 0.0)
            if isinstance(value, (int, float)):
                historical_features.append(float(value))
            else:
                try:
                    historical_features.append(float(str(value)))
                except:
                    historical_features.append(0.0)
        
        # I/Oç»Ÿè®¡
        io_stats = ['Shared Hit Blocks', 'Shared Read Blocks', 'Shared Dirtied Blocks', 'Shared Written Blocks']
        for stat in io_stats:
            value = extra_info.get(stat, 0.0)
            historical_features.append(float(value) if value else 0.0)
        
        # è®¡ç®—ä¼°è®¡vså®é™…çš„æ¯”ç‡ç‰¹å¾
        if len(historical_features) >= 4:  # ç¡®ä¿æœ‰å®é™…ç»Ÿè®¡
            # æ—¶é—´æ¯”ç‡
            estimated_cost = extra_info.get('Total Cost', 1.0)
            actual_time = historical_features[0]  # Actual Total Time
            if estimated_cost > 0 and actual_time > 0:
                time_ratio = actual_time / estimated_cost
                historical_features.append(time_ratio)
            else:
                historical_features.append(1.0)
            
            # è¡Œæ•°æ¯”ç‡
            estimated_rows = extra_info.get('Plan Rows', 1.0)
            actual_rows = historical_features[2]  # Actual Rows
            if estimated_rows > 0 and actual_rows > 0:
                rows_ratio = actual_rows / estimated_rows
                historical_features.append(rows_ratio)
            else:
                historical_features.append(1.0)
        
        if not historical_features:
            return np.array([])
        
        features = np.array(historical_features, dtype=np.float32)
        
        if normalize:
            # å¯¹æ—¶é—´å’ŒI/Oç»Ÿè®¡ä½¿ç”¨log normalization
            features[:4] = np.log1p(features[:4])  # actual stats
            features[4:8] = np.log1p(features[4:8])  # io stats
            
            # æ¯”ç‡ç‰¹å¾ä½¿ç”¨clip
            if len(features) > 8:
                features[8:] = np.clip(features[8:], 0.01, 100)  # ratio features
                features[8:] = np.log(features[8:])
        
        return features
    
    # ===================================================================
    # ç¼–ç æ–¹å¼å®ç° (Encoding Implementation Methods)
    # ===================================================================
    
    def encode_concatenation_mlp(self, node, components: List[str] = None) -> np.ndarray:
        """ç®€å•æ‹¼æ¥ + MLP ç¼–ç æ–¹å¼
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        components: List[str]
            è¦åŒ…å«çš„ç‰¹å¾ç»„ä»¶: ['operator', 'stats', 'predicate', 'relation', 'context', 'historical']
            
        Returns
        -------
        np.ndarray
            æ‹¼æ¥åçš„ç‰¹å¾å‘é‡
        """
        if components is None:
            components = ['operator', 'stats', 'context']
        
        feature_vectors = []
        
        for component in components:
            if component == 'operator':
                vec = self.encode_operator_type(node, method='one_hot')
            elif component == 'stats':
                vec = self.encode_estimated_stats(node)
            elif component == 'predicate':
                vec = self.encode_predicate_info(node, method='complexity')
            elif component == 'relation':
                vec = self.encode_relation_info(node, method='features')
            elif component == 'context':
                vec = self.encode_execution_context(node)
            elif component == 'historical':
                vec = self.encode_historical_feedback(node)
            else:
                continue
            
            if len(vec) > 0:
                feature_vectors.append(vec)
        
        if not feature_vectors:
            return np.array([])
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined = np.concatenate(feature_vectors)
        
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ç»´åº¦ï¼Œè°ƒæ•´å¤§å°
        if self.feature_dim is not None:
            combined = self._resize_vector(combined, self.feature_dim)
        
        return combined
    
    def encode_multi_view(self, node, view_methods: Dict[str, str] = None) -> Dict[str, np.ndarray]:
        """åˆ†å—ç¼–ç  (Multi-View Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        view_methods: Dict[str, str]
            å„ä¸ªè§†å›¾çš„ç¼–ç æ–¹æ³•
            
        Returns
        -------
        Dict[str, np.ndarray]
            å„ä¸ªè§†å›¾çš„ç‰¹å¾å‘é‡
        """
        if view_methods is None:
            view_methods = {
                'operator': 'embedding',
                'stats': 'normalize',
                'predicate': 'complexity',
                'relation': 'features'
            }
        
        views = {}
        
        if 'operator' in view_methods:
            views['operator'] = self.encode_operator_type(node, method=view_methods['operator'])
        
        if 'stats' in view_methods:
            views['stats'] = self.encode_estimated_stats(node, normalize=True)
        
        if 'predicate' in view_methods:
            views['predicate'] = self.encode_predicate_info(node, method=view_methods['predicate'])
        
        if 'relation' in view_methods:
            views['relation'] = self.encode_relation_info(node, method=view_methods['relation'])
        
        if 'context' in view_methods:
            views['context'] = self.encode_execution_context(node)
        
        if 'historical' in view_methods:
            views['historical'] = self.encode_historical_feedback(node)
        
        return views
    
    def encode_knowledge_enhanced(self, node, include_cost_model: bool = True) -> np.ndarray:
        """çŸ¥è¯†å¢å¼ºç¼–ç  (Knowledge Enhanced Encoding)
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
        include_cost_model: bool
            æ˜¯å¦åŒ…å«ä¼ ç»Ÿä»£ä»·æ¨¡å‹çš„è¾“å‡º
            
        Returns
        -------
        np.ndarray
            çŸ¥è¯†å¢å¼ºçš„ç‰¹å¾å‘é‡
        """
        # åŸºç¡€ç‰¹å¾
        base_features = self.encode_concatenation_mlp(node, ['operator', 'stats', 'context'])
        
        enhanced_features = [base_features]
        
        if include_cost_model:
            # ä¼ ç»Ÿä»£ä»·æ¨¡å‹ç‰¹å¾
            extra_info = getattr(node, 'extra_info', {})
            cost_features = []
            
            # ä»£ä»·æ¨¡å‹è¾“å‡º
            startup_cost = extra_info.get('Startup Cost', 0.0)
            total_cost = extra_info.get('Total Cost', 0.0)
            
            cost_features.extend([float(startup_cost), float(total_cost)])
            
            # é€‰æ‹©æ€§ä¼°è®¡ (ç®€åŒ–ç‰ˆ)
            plan_rows = extra_info.get('Plan Rows', 1.0)
            if plan_rows > 0:
                selectivity = min(1.0, float(plan_rows) / 1000000)  # å‡è®¾åŸºå‡†è¡¨å¤§å°
            else:
                selectivity = 0.0
            cost_features.append(selectivity)
            
            # ä»£ä»·æ¯”ç‡
            if total_cost > 0 and startup_cost >= 0:
                cost_ratio = startup_cost / total_cost
                cost_features.append(cost_ratio)
            else:
                cost_features.append(0.0)
            
            cost_vec = np.array(cost_features, dtype=np.float32)
            enhanced_features.append(cost_vec)
        
        # æ‹¼æ¥æ‰€æœ‰å¢å¼ºç‰¹å¾
        combined = np.concatenate(enhanced_features)
        
        if self.feature_dim is not None:
            combined = self._resize_vector(combined, self.feature_dim)
        
        return combined
    
    # ===================================================================
    # è°“è¯ç¼–ç è¾…åŠ©æ–¹æ³• (Predicate Encoding Helper Methods)
    # ===================================================================
    
    def _encode_predicate_complexity(self, predicates: List[str]) -> np.ndarray:
        """ç¼–ç è°“è¯å¤æ‚åº¦ç‰¹å¾"""
        complexity_features = []
        
        all_predicates = ' '.join(predicates).lower()
        
        # è°“è¯æ•°é‡
        complexity_features.append(float(len(predicates)))
        
        # æ˜¯å¦æœ‰èŒƒå›´è¿‡æ»¤
        range_patterns = ['>', '<', '>=', '<=', 'between', 'range']
        has_range = float(any(pattern in all_predicates for pattern in range_patterns))
        complexity_features.append(has_range)
        
        # æ˜¯å¦åŒ…å«å­æŸ¥è¯¢
        subquery_patterns = ['exists', 'in (select', 'any', 'all', 'subplan']
        has_subquery = float(any(pattern in all_predicates for pattern in subquery_patterns))
        complexity_features.append(has_subquery)
        
        # æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨
        function_patterns = ['(', 'upper', 'lower', 'substr', 'date', 'cast']
        has_function = float(any(pattern in all_predicates for pattern in function_patterns))
        complexity_features.append(has_function)
        
        # æ˜¯å¦æœ‰LIKEæ¨¡å¼åŒ¹é…
        has_like = float('like' in all_predicates or '%' in all_predicates)
        complexity_features.append(has_like)
        
        # è¿æ¥æ¡ä»¶æ•°é‡ (ç®€åŒ–ä¼°è®¡)
        join_patterns = ['=', 'join', 'on']
        join_count = sum(all_predicates.count(pattern) for pattern in join_patterns[:1])  # åªè®¡ç®—=
        complexity_features.append(float(join_count))
        
        return np.array(complexity_features, dtype=np.float32)
    
    def _encode_predicate_bow(self, predicates: List[str]) -> np.ndarray:
        """Bag-of-Wordsç¼–ç è°“è¯"""
        # ç®€åŒ–çš„BoWå®ç°
        all_text = ' '.join(predicates).lower()
        
        # é¢„å®šä¹‰çš„é‡è¦è¯æ±‡
        vocab = [
            '=', '>', '<', '>=', '<=', 'like', 'in', 'exists', 'and', 'or', 'not',
            'between', 'is', 'null', 'true', 'false', 'cast', 'date', 'time',
            'upper', 'lower', 'substr', 'length', 'count', 'sum', 'avg', 'max', 'min'
        ]
        
        bow_features = []
        for word in vocab:
            count = all_text.count(word)
            bow_features.append(float(count))
        
        # æ ‡å‡†åŒ–
        bow_array = np.array(bow_features, dtype=np.float32)
        if bow_array.sum() > 0:
            bow_array = bow_array / bow_array.sum()
        
        # è°ƒæ•´åˆ°ç›®æ ‡ç»´åº¦
        if len(bow_array) < self.predicate_embedding_dim:
            padded = np.zeros(self.predicate_embedding_dim, dtype=np.float32)
            padded[:len(bow_array)] = bow_array
            return padded
        else:
            return bow_array[:self.predicate_embedding_dim]
    
    def _encode_predicate_tokens(self, predicates: List[str]) -> np.ndarray:
        """Token embeddingç¼–ç è°“è¯"""
        # ç®€åŒ–çš„token embedding
        all_text = ' '.join(predicates).lower()
        
        # æå–tokens
        tokens = re.findall(r'\w+', all_text)
        
        if not tokens:
            return np.zeros(self.predicate_embedding_dim, dtype=np.float32)
        
        # ç®€å•çš„token embedding (ä½¿ç”¨hash)
        token_embeddings = []
        for token in tokens[:10]:  # é™åˆ¶tokenæ•°é‡
            hash_val = hash(token) % self.predicate_embedding_dim
            embedding = np.zeros(self.predicate_embedding_dim, dtype=np.float32)
            embedding[hash_val] = 1.0
            token_embeddings.append(embedding)
        
        if token_embeddings:
            # å¹³å‡æ± åŒ–
            avg_embedding = np.mean(token_embeddings, axis=0)
            return avg_embedding / np.linalg.norm(avg_embedding) if np.linalg.norm(avg_embedding) > 0 else avg_embedding
        else:
            return np.zeros(self.predicate_embedding_dim, dtype=np.float32)
    
    def _create_relation_embedding(self, relation_name: str) -> np.ndarray:
        """åˆ›å»ºå…³ç³»åembedding"""
        # ä½¿ç”¨hashåˆ›å»ºç¡®å®šæ€§embedding
        embedding = np.zeros(self.relation_embedding_dim, dtype=np.float32)
        
        hash_val = hash(relation_name) % self.relation_embedding_dim
        embedding[hash_val] = 1.0
        
        # æ·»åŠ ä¸€äº›å˜åŒ–
        for i in range(min(2, self.relation_embedding_dim)):
            pos = (hash_val + i + 1) % self.relation_embedding_dim
            embedding[pos] = 0.5
        
        return embedding / np.linalg.norm(embedding) if np.linalg.norm(embedding) > 0 else embedding
    
    def extract_node_features(self, node) -> NodeFeatures:
        """Extract comprehensive features from a single node.
        
        Parameters
        ----------
        node:
            PlanNode object to extract features from
            
        Returns
        -------
        NodeFeatures:
            Container with different types of extracted features
        """
        # Extract different types of features
        idx = self._ensure_index(getattr(node, "node_type", "Unknown"))
        node_type_vec = self._one_hot(idx)
        numerical_vec = self._extract_numerical_features(node)
        categorical_vec = self._extract_categorical_features(node)
        
        # Extract new feature types
        predicate_vec = self.encode_predicate_info(node, method='complexity')
        relation_vec = self.encode_relation_info(node, method='features')
        context_vec = self.encode_execution_context(node)
        historical_vec = self.encode_historical_feedback(node)
        
        # Combine features
        feature_components = [node_type_vec]
        if len(numerical_vec) > 0:
            feature_components.append(numerical_vec)
        if len(categorical_vec) > 0:
            feature_components.append(categorical_vec)
        if len(predicate_vec) > 0:
            feature_components.append(predicate_vec)
        if len(relation_vec) > 0:
            feature_components.append(relation_vec)
        if len(context_vec) > 0:
            feature_components.append(context_vec)
        if len(historical_vec) > 0:
            feature_components.append(historical_vec)
        
        combined_vec = np.concatenate(feature_components)
        if self.feature_dim is not None:
            combined_vec = self._resize_vector(combined_vec, self.feature_dim)
        
        return NodeFeatures(
            node_type_vector=node_type_vec,
            numerical_features=numerical_vec,
            categorical_features=categorical_vec,
            predicate_features=predicate_vec,
            relation_features=relation_vec,
            execution_context_features=context_vec,
            historical_features=historical_vec,
            combined_features=combined_vec
        )
    
    # ---------------------------------------------------------------- internal ---
    def _normalize_numerical(self, features: np.ndarray) -> np.ndarray:
        """Normalize numerical features using min-max scaling."""
        if len(features) == 0:
            return features
        
        # Simple normalization - clip outliers and scale
        features = np.clip(features, 0, np.percentile(features, 95) if np.any(features > 0) else 1)
        max_val = np.maximum(features.max(), 1.0)
        return features / max_val
    
    def _resize_vector(self, vector: np.ndarray, target_dim: int) -> np.ndarray:
        """Resize vector to target dimension."""
        if len(vector) == target_dim:
            return vector
        elif len(vector) < target_dim:
            # Pad with zeros
            padded = np.zeros(target_dim, dtype=np.float32)
            padded[:len(vector)] = vector
            return padded
        else:
            # Truncate
            return vector[:target_dim]
    
    # -------------------------------------------------------------- configuration ---
    def enable_rich_features(self, feature_dim: Optional[int] = None):
        """Enable rich feature extraction."""
        self.rich_features = True
        if feature_dim is not None:
            self.feature_dim = feature_dim
    
    def disable_rich_features(self):
        """Disable rich features (only node type)."""
        self.rich_features = False
    
    def get_feature_info(self) -> Dict[str, Any]:
        """Get information about current feature configuration."""
        return {
            'rich_features': self.rich_features,
            'feature_dim': self.feature_dim,
            'include_numerical': self.include_numerical,
            'include_categorical': self.include_categorical,
            'node_types_count': len(self.node_index),
            'categorical_vocabs': {k: len(v) for k, v in self.categorical_vocabs.items()}
        }
    
    def get_vocab_sizes(self) -> Dict[str, int]:
        """Get the sizes of different vocabularies."""
        vocab_sizes = {
            'node_type': len(self.node_index)
        }
        
        for key, vocab in self.categorical_vocabs.items():
            vocab_sizes[f'categorical_{key}'] = len(vocab)
        
        return vocab_sizes


# ===================================================================
# ä¾¿åˆ©å·¥å‚å‡½æ•° (Convenience Factory Functions)
# ===================================================================

def create_simple_node_encoder() -> NodeEncoder:
    """Create a simple node encoder (only node type)."""
    return NodeEncoder(rich_features=False)


def create_rich_node_encoder(feature_dim: int = 64, 
                           include_numerical: bool = True,
                           include_categorical: bool = True,
                           normalize_features: bool = True) -> NodeEncoder:
    """Create a rich node encoder with comprehensive features."""
    return NodeEncoder(
        rich_features=True,
        feature_dim=feature_dim,
        include_numerical=include_numerical,
        include_categorical=include_categorical,
        normalize_features=normalize_features
    )


def create_concatenation_encoder(components: List[str] = None, 
                               feature_dim: int = 128) -> NodeEncoder:
    """åˆ›å»ºç®€å•æ‹¼æ¥ç¼–ç å™¨
    
    Parameters
    ----------
    components: List[str]
        è¦åŒ…å«çš„ç‰¹å¾ç»„ä»¶: ['operator', 'stats', 'predicate', 'relation', 'context', 'historical']
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„æ‹¼æ¥ç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'concatenation'
    encoder._encoding_components = components or ['operator', 'stats', 'context']
    return encoder


def create_multi_view_encoder(view_methods: Dict[str, str] = None,
                            feature_dim: int = 128) -> NodeEncoder:
    """åˆ›å»ºåˆ†å—ç¼–ç å™¨
    
    Parameters
    ----------
    view_methods: Dict[str, str]
        å„ä¸ªè§†å›¾çš„ç¼–ç æ–¹æ³•
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„åˆ†å—ç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'multi_view'
    encoder._view_methods = view_methods or {
        'operator': 'embedding',
        'stats': 'normalize',
        'predicate': 'complexity',
        'relation': 'features'
    }
    return encoder


def create_knowledge_enhanced_encoder(include_cost_model: bool = True,
                                   feature_dim: int = 128) -> NodeEncoder:
    """åˆ›å»ºçŸ¥è¯†å¢å¼ºç¼–ç å™¨
    
    Parameters
    ----------
    include_cost_model: bool
        æ˜¯å¦åŒ…å«ä¼ ç»Ÿä»£ä»·æ¨¡å‹ç‰¹å¾
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„çŸ¥è¯†å¢å¼ºç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'knowledge_enhanced'
    encoder._include_cost_model = include_cost_model
    return encoder


def create_predicate_focused_encoder(predicate_method: str = 'complexity',
                                   feature_dim: int = 96) -> NodeEncoder:
    """åˆ›å»ºè°“è¯é‡ç‚¹ç¼–ç å™¨
    
    Parameters
    ----------
    predicate_method: str
        è°“è¯ç¼–ç æ–¹æ³•: 'complexity', 'bow', 'token_embedding'
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„è°“è¯é‡ç‚¹ç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'predicate_focused'
    encoder._predicate_method = predicate_method
    return encoder


def create_historical_aware_encoder(normalize_historical: bool = True,
                                  feature_dim: int = 128) -> NodeEncoder:
    """åˆ›å»ºå†å²æ„ŸçŸ¥ç¼–ç å™¨
    
    Parameters
    ----------
    normalize_historical: bool
        æ˜¯å¦æ ‡å‡†åŒ–å†å²ç‰¹å¾
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„å†å²æ„ŸçŸ¥ç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'historical_aware'
    encoder._normalize_historical = normalize_historical
    return encoder


def create_custom_encoder(operator_method: str = 'one_hot',
                        stats_normalize: bool = True,
                        predicate_method: str = 'complexity',
                        relation_method: str = 'features',
                        include_context: bool = True,
                        include_historical: bool = False,
                        feature_dim: int = 128) -> NodeEncoder:
    """åˆ›å»ºè‡ªå®šä¹‰ç¼–ç å™¨
    
    Parameters
    ----------
    operator_method: str
        ç®—å­ç¼–ç æ–¹æ³•: 'one_hot', 'embedding', 'learned_embedding'
    stats_normalize: bool
        æ˜¯å¦æ ‡å‡†åŒ–ç»Ÿè®¡ç‰¹å¾
    predicate_method: str
        è°“è¯ç¼–ç æ–¹æ³•: 'complexity', 'bow', 'token_embedding'
    relation_method: str
        å…³ç³»ç¼–ç æ–¹æ³•: 'embedding', 'one_hot', 'features'
    include_context: bool
        æ˜¯å¦åŒ…å«æ‰§è¡Œä¸Šä¸‹æ–‡ç‰¹å¾
    include_historical: bool
        æ˜¯å¦åŒ…å«å†å²ç‰¹å¾
    feature_dim: int
        ç›®æ ‡ç‰¹å¾ç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„è‡ªå®šä¹‰ç¼–ç å™¨
    """
    encoder = NodeEncoder(rich_features=True, feature_dim=feature_dim)
    encoder._encoding_method = 'custom'
    encoder._custom_config = {
        'operator_method': operator_method,
        'stats_normalize': stats_normalize,
        'predicate_method': predicate_method,
        'relation_method': relation_method,
        'include_context': include_context,
        'include_historical': include_historical
    }
    return encoder
