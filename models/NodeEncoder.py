"""Pure Node-level Encoder for PlanNode objects.

This is the correct NODE ENCODER layer in the architecture:
ğŸ“Š Architecture Position: Step 2 (Node-level Encoding)
- Input: Individual TreeNode with attributes (node_type, extra_info)
- Output: Node-level embedding vector
- Scope: ONLY single node feature extraction

âš ï¸  IMPORTANT: This encoder handles ONLY node-level features.
NO tree structure processing, NO recursive aggregation.
Structure-level encoding is handled by TreeModel.

ğŸ› ï¸ ç¼–ç æ–¹å¼ï¼šåˆ†å—ç¼–ç  (Multi-View Encoding)
- ç®—å­ç±»å‹ â†’ Embedding
- æ•°æ®ç»Ÿè®¡ â†’ MLP
- è°“è¯ä¿¡æ¯ â†’ Encoder
- æœ€å Concat æ‰€æœ‰ç‰¹å¾
"""

from __future__ import annotations

from typing import Dict, Iterable, List, Optional, Any
import numpy as np
import torch
import torch.nn as nn
import re


class NodeEncoder(nn.Module):
    """Pure node-level encoder using multi-view encoding strategy.
    
    åˆ†å—ç¼–ç ç­–ç•¥ï¼š
    1. ç®—å­ç±»å‹ â†’ Embedding Layer
    2. æ•°æ®ç»Ÿè®¡ â†’ MLP (æ ‡å‡†åŒ– + å…¨è¿æ¥)
    3. è°“è¯ä¿¡æ¯ â†’ Simple Encoder (å¤æ‚åº¦ç‰¹å¾)
    4. æœ€å Concat æ‰€æœ‰ç‰¹å¾
    
    âš ï¸  CRITICAL: This encoder handles ONLY node-level features.
    - NO recursive processing of children
    - NO tree structure aggregation
    - NO graph construction
    
    Structure-level encoding (tree/graph aggregation) is handled by TreeModel.
    """
    
    def __init__(self, 
                 operator_embedding_dim: int = 32,
                 stats_hidden_dim: int = 16,
                 predicate_dim: int = 8,
                 output_dim: int = 64) -> None:
        """Initialize the multi-view node encoder.
        
        Parameters
        ----------
        operator_embedding_dim: int
            ç®—å­ç±»å‹embeddingç»´åº¦
        stats_hidden_dim: int
            ç»Ÿè®¡ç‰¹å¾MLPéšå±‚ç»´åº¦
        predicate_dim: int
            è°“è¯ç‰¹å¾ç»´åº¦
        output_dim: int
            æœ€ç»ˆè¾“å‡ºç»´åº¦
        """
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.operator_embedding_dim = operator_embedding_dim
        self.stats_hidden_dim = stats_hidden_dim
        self.predicate_dim = predicate_dim
        self.output_dim = output_dim
        
        # ç®—å­ç±»å‹è¯æ±‡è¡¨
        self.node_type_vocab: Dict[str, int] = {}
        
        # æ ¸å¿ƒç»Ÿè®¡ç‰¹å¾é”®
        self.stats_keys = ['Plan Rows', 'Plan Width', 'Startup Cost', 'Total Cost']
        
        # è°“è¯ç‰¹å¾é”®
        self.predicate_keys = ['Filter', 'Index Cond', 'Hash Cond', 'Merge Cond', 'Join Filter']
        
        # å»¶è¿Ÿåˆå§‹åŒ–çš„ç»„ä»¶ (åœ¨ç¬¬ä¸€æ¬¡forwardæ—¶åˆå§‹åŒ–)
        self.operator_embedding: Optional[nn.Embedding] = None
        self.stats_mlp: Optional[nn.Sequential] = None
        self.output_projection: Optional[nn.Linear] = None
        
        self._initialized = False
    
    def _ensure_initialized(self, node):
        """ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½å·²åˆå§‹åŒ–"""
        if self._initialized:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•embeddingå±‚
            self._update_operator_vocab(node)
            current_vocab_size = len(self.node_type_vocab)
            if current_vocab_size > self.operator_embedding.num_embeddings:
                # éœ€è¦æ‰©å±•embeddingå±‚
                old_embedding = self.operator_embedding
                self.operator_embedding = nn.Embedding(current_vocab_size, self.operator_embedding_dim)
                # å¤åˆ¶æ—§çš„æƒé‡
                with torch.no_grad():
                    self.operator_embedding.weight[:old_embedding.num_embeddings] = old_embedding.weight
            return
            
        # 1. åˆå§‹åŒ–ç®—å­embedding
        self._update_operator_vocab(node)
        vocab_size = len(self.node_type_vocab)
        self.operator_embedding = nn.Embedding(vocab_size, self.operator_embedding_dim)
        
        # 2. åˆå§‹åŒ–ç»Ÿè®¡ç‰¹å¾MLP
        stats_input_dim = len(self.stats_keys)
        self.stats_mlp = nn.Sequential(
            nn.Linear(stats_input_dim, self.stats_hidden_dim),
            nn.ReLU(),
            nn.Linear(self.stats_hidden_dim, self.stats_hidden_dim)
        )
        
        # 3. è®¡ç®—concatåçš„æ€»ç»´åº¦
        total_dim = self.operator_embedding_dim + self.stats_hidden_dim + self.predicate_dim
        
        # 4. åˆå§‹åŒ–è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(total_dim, self.output_dim)
        
        self._initialized = True
    
    def _update_operator_vocab(self, node):
        """æ›´æ–°ç®—å­ç±»å‹è¯æ±‡è¡¨"""
        node_type = getattr(node, "node_type", "Unknown")
        if node_type not in self.node_type_vocab:
            self.node_type_vocab[node_type] = len(self.node_type_vocab)
    
    def _encode_operator(self, node) -> torch.Tensor:
        """ç¼–ç ç®—å­ç±»å‹ â†’ Embedding"""
        self._update_operator_vocab(node)
        node_type = getattr(node, "node_type", "Unknown")
        idx = self.node_type_vocab[node_type]
        idx_tensor = torch.tensor([idx], dtype=torch.long)
        return self.operator_embedding(idx_tensor).squeeze(0)  # [embedding_dim]
    
    def _encode_stats(self, node) -> torch.Tensor:
        """ç¼–ç æ•°æ®ç»Ÿè®¡ â†’ MLP"""
        extra_info = getattr(node, 'extra_info', {})
        stats_values = []
        
        for key in self.stats_keys:
            value = extra_info.get(key, 0.0)
            if isinstance(value, (int, float)):
                stats_values.append(float(value))
            else:
                try:
                    stats_values.append(float(str(value).replace(',', '')))
                except:
                    stats_values.append(0.0)
        
        # ç®€å•çš„logæ ‡å‡†åŒ–
        stats_tensor = torch.tensor(stats_values, dtype=torch.float32)
        stats_tensor = torch.log1p(stats_tensor)  # log(1 + x)
        
        # é€šè¿‡MLP
        return self.stats_mlp(stats_tensor)  # [stats_hidden_dim]
    
    def _encode_predicate(self, node) -> torch.Tensor:
        """ç¼–ç è°“è¯ä¿¡æ¯ â†’ ç®€å•ç‰¹å¾"""
        extra_info = getattr(node, 'extra_info', {})
        
        # æ”¶é›†æ‰€æœ‰è°“è¯ä¿¡æ¯
        predicates = []
        for key in self.predicate_keys:
            if key in extra_info and extra_info[key]:
                predicates.append(str(extra_info[key]))
        
        if not predicates:
            return torch.zeros(self.predicate_dim, dtype=torch.float32)
        
        # ç®€å•çš„å¤æ‚åº¦ç‰¹å¾
        all_predicates = ' '.join(predicates).lower()
        
        features = []
        
        # 1. è°“è¯æ•°é‡ (å½’ä¸€åŒ–)
        features.append(min(len(predicates) / 5.0, 1.0))
        
        # 2. æ˜¯å¦æœ‰èŒƒå›´è¿‡æ»¤
        range_patterns = ['>', '<', '>=', '<=', 'between']
        features.append(float(any(pattern in all_predicates for pattern in range_patterns)))
        
        # 3. æ˜¯å¦åŒ…å«å­æŸ¥è¯¢
        subquery_patterns = ['exists', 'in (select', 'subplan']
        features.append(float(any(pattern in all_predicates for pattern in subquery_patterns)))
        
        # 4. æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨
        features.append(float('(' in all_predicates))
        
        # 5. æ˜¯å¦æœ‰LIKEæ¨¡å¼åŒ¹é…
        features.append(float('like' in all_predicates or '%' in all_predicates))
        
        # 6. è¿æ¥æ¡ä»¶æ•°é‡ (å½’ä¸€åŒ–)
        join_count = all_predicates.count('=')
        features.append(min(join_count / 3.0, 1.0))
        
        # 7-8. å¡«å……åˆ°predicate_dimç»´åº¦
        while len(features) < self.predicate_dim:
            features.append(0.0)
        
        return torch.tensor(features[:self.predicate_dim], dtype=torch.float32)
    
    def forward(self, node) -> torch.Tensor:
        """åˆ†å—ç¼–ç  + Concat
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
            
        Returns
        -------
        torch.Tensor
            èŠ‚ç‚¹ç¼–ç å‘é‡ [output_dim]
        """
        # ç¡®ä¿åˆå§‹åŒ–
        self._ensure_initialized(node)
        
        # 1. ç®—å­ç±»å‹ç¼–ç 
        operator_vec = self._encode_operator(node)  # [operator_embedding_dim]
        
        # 2. ç»Ÿè®¡ç‰¹å¾ç¼–ç 
        stats_vec = self._encode_stats(node)  # [stats_hidden_dim]
        
        # 3. è°“è¯ç‰¹å¾ç¼–ç 
        predicate_vec = self._encode_predicate(node)  # [predicate_dim]
        
        # 4. Concatæ‰€æœ‰ç‰¹å¾
        combined = torch.cat([operator_vec, stats_vec, predicate_vec], dim=0)
        
        # 5. è¾“å‡ºæŠ•å½±
        output = self.output_projection(combined)  # [output_dim]
        
        return output
    
    def encode_node(self, node) -> torch.Tensor:
        """ç¼–ç å•ä¸ªèŠ‚ç‚¹å¹¶å­˜å‚¨åˆ°node.node_vector
        
        Parameters
        ----------
        node: PlanNode
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹
            
        Returns
        -------
        torch.Tensor
            èŠ‚ç‚¹ç¼–ç å‘é‡
        """
        vector = self.forward(node)
        node.node_vector = vector
        return vector
    
    def encode_nodes(self, nodes: Iterable) -> List[torch.Tensor]:
        """ç¼–ç å¤šä¸ªèŠ‚ç‚¹
        
        Parameters
        ----------
        nodes: Iterable
            æŸ¥è¯¢è®¡åˆ’èŠ‚ç‚¹åˆ—è¡¨
            
        Returns
        -------
        List[torch.Tensor]
            èŠ‚ç‚¹ç¼–ç å‘é‡åˆ—è¡¨
        """
        return [self.encode_node(node) for node in nodes]
    
    def get_output_dim(self) -> int:
        """è·å–è¾“å‡ºç»´åº¦"""
        return self.output_dim
    
    def get_vocab_size(self) -> int:
        """è·å–ç®—å­è¯æ±‡è¡¨å¤§å°"""
        return len(self.node_type_vocab)
    
    def get_config(self) -> Dict[str, Any]:
        """è·å–ç¼–ç å™¨é…ç½®ä¿¡æ¯"""
        return {
            'operator_embedding_dim': self.operator_embedding_dim,
            'stats_hidden_dim': self.stats_hidden_dim,
            'predicate_dim': self.predicate_dim,
            'output_dim': self.output_dim,
            'vocab_size': len(self.node_type_vocab),
            'initialized': self._initialized
        }


# ä¾¿åˆ©å·¥å‚å‡½æ•°
def create_node_encoder(operator_dim: int = 32,
                       stats_dim: int = 16, 
                       predicate_dim: int = 8,
                       output_dim: int = 64) -> NodeEncoder:
    """åˆ›å»ºNodeEncoderå®ä¾‹
    
    Parameters
    ----------
    operator_dim: int
        ç®—å­embeddingç»´åº¦
    stats_dim: int
        ç»Ÿè®¡ç‰¹å¾MLPéšå±‚ç»´åº¦
    predicate_dim: int
        è°“è¯ç‰¹å¾ç»´åº¦
    output_dim: int
        è¾“å‡ºç»´åº¦
        
    Returns
    -------
    NodeEncoder
        é…ç½®å¥½çš„èŠ‚ç‚¹ç¼–ç å™¨
    """
    return NodeEncoder(
        operator_embedding_dim=operator_dim,
        stats_hidden_dim=stats_dim,
        predicate_dim=predicate_dim,
        output_dim=output_dim
    )


def create_simple_node_encoder() -> NodeEncoder:
    """åˆ›å»ºç®€å•çš„èŠ‚ç‚¹ç¼–ç å™¨ (å°ç»´åº¦)"""
    return NodeEncoder(
        operator_embedding_dim=16,
        stats_hidden_dim=8,
        predicate_dim=4,
        output_dim=32
    )


def create_large_node_encoder() -> NodeEncoder:
    """åˆ›å»ºå¤§å®¹é‡èŠ‚ç‚¹ç¼–ç å™¨ (å¤§ç»´åº¦)"""
    return NodeEncoder(
        operator_embedding_dim=64,
        stats_hidden_dim=32,
        predicate_dim=16,
        output_dim=128
    )